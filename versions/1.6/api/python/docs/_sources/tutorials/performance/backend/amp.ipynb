{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n",
    "<!--- or more contributor license agreements.  See the NOTICE file -->\n",
    "<!--- distributed with this work for additional information -->\n",
    "<!--- regarding copyright ownership.  The ASF licenses this file -->\n",
    "<!--- to you under the Apache License, Version 2.0 (the -->\n",
    "<!--- \"License\"); you may not use this file except in compliance -->\n",
    "<!--- with the License.  You may obtain a copy of the License at -->\n",
    "\n",
    "<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n",
    "\n",
    "<!--- Unless required by applicable law or agreed to in writing, -->\n",
    "<!--- software distributed under the License is distributed on an -->\n",
    "<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n",
    "<!--- KIND, either express or implied.  See the License for the -->\n",
    "<!--- specific language governing permissions and limitations -->\n",
    "<!--- under the License. -->\n",
    "\n",
    "# Using AMP: Automatic Mixed Precision\n",
    "\n",
    "Training Deep Learning networks is a very computationally intensive task. Novel model architectures tend to have increasing number of layers and parameters, which slows down training. Fortunately, new generations of training hardware as well as software optimizations, make it a feasible task.\n",
    "\n",
    "However, where most of the (both hardware and software) optimization opportunities exists is in exploiting lower precision (like FP16) to, for example, utilize Tensor Cores available on new Volta and Turing GPUs. While training in FP16 showed great success in image classification tasks, other more complicated neural networks typically stayed in FP32 due to difficulties in applying the FP16 training guidelines.\n",
    "\n",
    "That is where AMP (Automatic Mixed Precision) comes into play. It automatically applies the guidelines of FP16 training, using FP16 precision where it provides the most benefit, while conservatively keeping in full FP32 precision operations unsafe to do in FP16.\n",
    "\n",
    "This tutorial shows how to get started with mixed precision training using AMP for MXNet. As an example of a network we will use SSD network from GluonCV.\n",
    "\n",
    "## Data loader and helper functions\n",
    "\n",
    "For demonstration purposes we will use synthetic data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.gluon as gluon\n",
    "from mxnet import autograd\n",
    "from mxnet.test_utils import download_model\n",
    "import gluoncv as gcv\n",
    "from gluoncv.model_zoo import get_model\n",
    "\n",
    "data_shape = 512\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "wd = 0.0005\n",
    "momentum = 0.9\n",
    "\n",
    "# training contexts\n",
    "ctx = [mx.gpu(0)]\n",
    "\n",
    "# set up logger\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "ce_metric = mx.metric.Loss('CrossEntropy')\n",
    "smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SyntheticDataLoader(object):\n",
    "    def __init__(self, data_shape, batch_size):\n",
    "        super(SyntheticDataLoader, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.epoch_size = 200\n",
    "        shape = (batch_size, 3, data_shape, data_shape)\n",
    "        cls_targets_shape = (batch_size, 6132)\n",
    "        box_targets_shape = (batch_size, 6132, 4)\n",
    "        self.data = mx.nd.random.uniform(-1, 1, shape=shape, ctx=mx.cpu_pinned())\n",
    "        self.cls_targets = mx.nd.random.uniform(0, 1, shape=cls_targets_shape, ctx=mx.cpu_pinned())\n",
    "        self.box_targets = mx.nd.random.uniform(0, 1, shape=box_targets_shape, ctx=mx.cpu_pinned())\n",
    "    \n",
    "    def next(self):\n",
    "        if self.counter >= self.epoch_size:\n",
    "            self.counter = self.counter % self.epoch_size\n",
    "            raise StopIteration\n",
    "        self.counter += 1\n",
    "        return [self.data, self.cls_targets, self.box_targets]\n",
    "    \n",
    "    __next__ = next\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "train_data = SyntheticDataLoader(data_shape, batch_size)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_network():\n",
    "    # SSD with RN50 backbone\n",
    "    net_name = 'ssd_512_resnet50_v1_coco'\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        net = get_model(net_name, pretrained_base=True, norm_layer=gluon.nn.BatchNorm)\n",
    "        net.initialize()\n",
    "        net.collect_params().reset_ctx(ctx)\n",
    "\n",
    "    return net\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in FP32\n",
    "\n",
    "First, let us create the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "net = get_network()\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a Gluon Trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': lr, 'wd': wd, 'momentum': momentum})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "            cls_preds = []\n",
    "            box_preds = []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                cls_preds, box_preds, cls_targets, box_targets)\n",
    "            autograd.backward(sum_loss)\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        if not (i + 1) % 50:\n",
    "            name1, loss1 = ce_metric.get()\n",
    "            name2, loss2 = smoothl1_metric.get()\n",
    "            logger.info('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "INFO:root:[Epoch 0][Batch 49], Speed: 58.105 samples/sec, CrossEntropy=1.190, SmoothL1=0.688\n",
    "INFO:root:[Epoch 0][Batch 99], Speed: 58.683 samples/sec, CrossEntropy=0.693, SmoothL1=0.536\n",
    "INFO:root:[Epoch 0][Batch 149], Speed: 58.915 samples/sec, CrossEntropy=0.500, SmoothL1=0.453\n",
    "INFO:root:[Epoch 0][Batch 199], Speed: 58.422 samples/sec, CrossEntropy=0.396, SmoothL1=0.399\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with AMP\n",
    "\n",
    "### AMP initialization\n",
    "\n",
    "In order to start using AMP, we need to import and initialize it. This has to happen before we create the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mxnet.contrib import amp\n",
    "\n",
    "amp.init()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "INFO:root:Using AMP\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can create the network exactly the same way we did in FP32 training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "net = get_network()\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some models that may be enough to start training in mixed precision, but the full FP16 recipe recommends using dynamic loss scaling to guard against over- and underflows of FP16 values. Therefore, as a next step, we create a trainer and initialize it with support for AMP's dynamic loss scaling. Currently, support for dynamic loss scaling is limited to trainers created with `update_on_kvstore=False` option, and so we add it to our trainer initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': lr, 'wd': wd, 'momentum': momentum},\n",
    "    update_on_kvstore=False)\n",
    "\n",
    "amp.init_trainer(trainer)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic loss scaling in the training loop\n",
    "\n",
    "The last step is to apply the dynamic loss scaling during the training loop and . We can achieve that using the `amp.scale_loss` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "            cls_preds = []\n",
    "            box_preds = []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                cls_preds, box_preds, cls_targets, box_targets)\n",
    "            with amp.scale_loss(sum_loss, trainer) as scaled_loss:\n",
    "                autograd.backward(scaled_loss)\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        if not (i + 1) % 50:\n",
    "            name1, loss1 = ce_metric.get()\n",
    "            name2, loss2 = smoothl1_metric.get()\n",
    "            logger.info('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "INFO:root:[Epoch 0][Batch 49], Speed: 93.585 samples/sec, CrossEntropy=1.166, SmoothL1=0.684\n",
    "INFO:root:[Epoch 0][Batch 99], Speed: 93.773 samples/sec, CrossEntropy=0.682, SmoothL1=0.533\n",
    "INFO:root:[Epoch 0][Batch 149], Speed: 93.399 samples/sec, CrossEntropy=0.493, SmoothL1=0.451\n",
    "INFO:root:[Epoch 0][Batch 199], Speed: 93.674 samples/sec, CrossEntropy=0.391, SmoothL1=0.397\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 60% speed increase from 3 additional lines of code!\n",
    "\n",
    "## Inference with AMP\n",
    "\n",
    "To do inference with mixed precision for a trained model in FP32, you can use the conversion APIs: `amp.convert_model` for symbolic model and `amp.convert_hybrid_block` for gluon models. The conversion APIs will take the FP32 model as input and will return a mixed precision model, which can be used to run inference.\n",
    "Below, we demonstrate for a gluon model and a symbolic model:\n",
    "- Conversion from FP32 model to mixed precision model.\n",
    "- Run inference on the mixed precision model.\n",
    "- For AMP conversion of bucketing module please refer to [example/rnn/bucketing/README.md](https://github.com/apache/incubator-mxnet/blob/master/example/rnn/bucketing/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with mx.Context(mx.gpu(0)):\n",
    "    # Below is an example of converting a gluon hybrid block to a mixed precision block\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = get_model(\"resnet50_v1\")\n",
    "        model.collect_params().initialize(ctx=mx.current_context())\n",
    "        model.hybridize()\n",
    "        model(mx.nd.zeros((1, 3, 224, 224)))\n",
    "        converted_model = amp.convert_hybrid_block(model)\n",
    "\n",
    "    # Run dummy inference with the converted gluon model\n",
    "    result = converted_model.forward(mx.nd.random.uniform(shape=(1, 3, 224, 224),\n",
    "                                                          dtype=np.float32))\n",
    "\n",
    "    # Below is an example of converting a symbolic model to a mixed precision model\n",
    "    model_path = \"model\"\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    prefix, epoch = mx.test_utils.download_model(\"imagenet1k-resnet-18\", dst_dir=model_path)\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    result_sym, result_arg_params, result_aux_params = amp.convert_model(sym,\n",
    "                                                                         arg_params,\n",
    "                                                                         aux_params)\n",
    "\n",
    "    # Run dummy inference with the converted symbolic model\n",
    "    mod = mx.mod.Module(result_sym, data_names=[\"data\"], label_names=[\"softmax_label\"], context=mx.current_context())\n",
    "    mod.bind(data_shapes=[['data', (1, 3, 224, 224)]], label_shapes=[['softmax_label', (1,)]])\n",
    "    mod.set_params(result_arg_params, result_aux_params)\n",
    "    mod.forward(mx.io.DataBatch(data=[mx.nd.ones((1, 3, 224, 224))],\n",
    "                                label=[mx.nd.ones((1,))]))\n",
    "    mod.get_outputs()[0].wait_to_read()\n",
    "    print(\"Conversion and Inference completed successfully\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also customize the operators to run in FP16 versus the operator to run in FP32 or to conditionally run in FP32.\n",
    "Also, you can force cast the params wherever possible to FP16. Below is an example which demonstrates both these use cases\n",
    "for symbolic model. You can do the same for gluon hybrid block with `amp.convert_hybrid_block` API, `cast_optional_params` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with mx.Context(mx.gpu(0)):\n",
    "    # Below is an example of converting a symbolic model to a mixed precision model\n",
    "    # with only Convolution op being force casted to FP16.\n",
    "    model_path = \"model\"\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    prefix, epoch = mx.test_utils.download_model(\"imagenet1k-resnet-18\", dst_dir=model_path)\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "\n",
    "    # All Convolution ops should run in FP16, SoftmaxOutput and FullyConnected should run in FP32\n",
    "    # cast_optional_params=True: Force cast params to FP16 wherever possible\n",
    "    result_sym, result_arg_params, result_aux_params = amp.convert_model(sym,\n",
    "                                                                         arg_params,\n",
    "                                                                         aux_params,\n",
    "                                                                         target_dtype_ops=[\"Convolution\"],\n",
    "                                                                         fp32_ops=[\"SoftmaxOutput\", \"FullyConnected\"],\n",
    "                                                                         cast_optional_params=True)\n",
    "\n",
    "    # Run dummy inference with the converted symbolic model\n",
    "    mod = mx.mod.Module(result_sym, data_names=[\"data\"], label_names=[\"softmax_label\"], context=mx.current_context())\n",
    "    mod.bind(data_shapes=[['data', (1, 3, 224, 224)]], label_shapes=[['softmax_label', (1,)]])\n",
    "    mod.set_params(result_arg_params, result_aux_params)\n",
    "    mod.forward(mx.io.DataBatch(data=[mx.nd.ones((1, 3, 224, 224))],\n",
    "                                label=[mx.nd.ones((1,))]))\n",
    "    mod.get_outputs()[0].wait_to_read()\n",
    "\n",
    "    # Assert that the params for conv are in FP16, this is because cast_optional_params is set to True\n",
    "    assert mod._arg_params[\"conv0_weight\"].dtype == np.float16\n",
    "    # FullyConnected params stay in FP32\n",
    "    assert mod._arg_params[\"fc1_bias\"].dtype == np.float32\n",
    "\n",
    "    print(\"Conversion and Inference completed successfully\")\n",
    "\n",
    "    # Serialize AMP model and save to disk\n",
    "    mod.save_checkpoint(\"amp_tutorial_model\", 0, remove_amp_cast=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current limitations of AMP\n",
    "\n",
    "- AMP's dynamic loss scaling currently supports only Gluon trainer with `update_on_kvstore=False` option set\n",
    "- Using `SoftmaxOutput`, `LinearRegressionOutput`, `LogisticRegressionOutput`, `MAERegressionOutput` with dynamic loss scaling does not work when training networks with multiple Gluon trainers and so multiple loss scales"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}