<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Gluon Contrib API" property="og:title">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image:secure_url">
<meta content="Gluon Contrib API" property="og:description"/>
<title>Gluon Contrib API — mxnet  documentation</title>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" rel="stylesheet"/>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../../_static/basic.css" rel="stylesheet" type="text/css">
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mxnet.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
<script src="https://code.jquery.com/jquery-1.11.1.min.js" type="text/javascript"></script>
<script src="../../../_static/underscore.js" type="text/javascript"></script>
<script src="../../../_static/searchtools_custom.js" type="text/javascript"></script>
<script src="../../../_static/doctools.js" type="text/javascript"></script>
<script src="../../../_static/selectlang.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript"> jQuery(function() { Search.loadIndex("/versions/1.5.0/searchindex.js"); Search.init();}); </script>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
      Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-96378503-1', 'auto');
      ga('send', 'pageview');

    </script>
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/jquery.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/underscore.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/doctools.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<!-- -->
<link href="../../../genindex.html" rel="index" title="Index">
<link href="../../../search.html" rel="search" title="Search"/>
<link href="gluon.html" rel="up" title="Gluon Package"/>
<link href="../image/image.html" rel="next" title="Image API"/>
<link href="model_zoo.html" rel="prev" title="Gluon Model Zoo"/>
<link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"/>
</link></link></link></meta></meta></meta></head>
<body background="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-background-compressed.jpeg" role="document">
<div class="content-block"><div class="navbar navbar-fixed-top">
<div class="container" id="navContainer">
<div class="innder" id="header-inner">
<h1 id="logo-wrap">
<a href="../../../" id="logo"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo.png"/></a>
</h1>
<nav class="nav-bar" id="main-nav">
<a class="main-nav-link" href="/versions/1.5.0/install/index.html">Install</a>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Gluon <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="https://www.d2l.ai/">Dive into Deep Learning</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">API <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/scala/index.html">Scala</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-docs">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Docs <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-docs">
<li><a class="main-nav-link" href="/versions/1.5.0/faq/index.html">FAQ</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/index.html">Tutorials</a>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.5.0/example">Examples</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/architecture/index.html">Architecture</a></li>
<li><a class="main-nav-link" href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/model_zoo/index.html">Model Zoo</a></li>
<li><a class="main-nav-link" href="https://github.com/onnx/onnx-mxnet">ONNX</a></li>
</li></ul>
</span>
<span id="dropdown-menu-position-anchor-community">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Community <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-community">
<li><a class="main-nav-link" href="http://discuss.mxnet.io">Forum</a></li>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.5.0">Github</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/contribute.html">Contribute</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/ecosystem.html">Ecosystem</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/powered_by.html">Powered By</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-version" style="position: relative"><a href="#" class="main-nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="true">1.5.0<span class="caret"></span></a><ul id="package-dropdown-menu" class="dropdown-menu"><li><a href="/">master</a></li><li><a href=/versions/1.6/>1.6</a></li><li><a href=/versions/1.5.0/>1.5.0</a></li><li><a href=/versions/1.4.1/>1.4.1</a></li><li><a href=/versions/1.3.1/>1.3.1</a></li><li><a href=/versions/1.2.1/>1.2.1</a></li><li><a href=/versions/1.1.0/>1.1.0</a></li><li><a href=/versions/1.0.0/>1.0.0</a></li><li><a href=/versions/0.12.1/>0.12.1</a></li><li><a href=/versions/0.11.0/>0.11.0</a></li></ul></span></nav>
<script> function getRootPath(){ return "../../../" } </script>
<div class="burgerIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button">☰</a>
<ul class="dropdown-menu" id="burgerMenu">
<li><a href="/versions/1.5.0/install/index.html">Install</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/index.html">Tutorials</a></li>
<li class="dropdown-submenu dropdown">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Gluon</a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="http://gluon.mxnet.io">The Straight Dope (Tutorials)</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">API</a>
<ul class="dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/scala/index.html">Scala</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Docs</a>
<ul class="dropdown-menu">
<li><a href="/versions/1.5.0/faq/index.html" tabindex="-1">FAQ</a></li>
<li><a href="/versions/1.5.0/tutorials/index.html" tabindex="-1">Tutorials</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.5.0/example" tabindex="-1">Examples</a></li>
<li><a href="/versions/1.5.0/architecture/index.html" tabindex="-1">Architecture</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home" tabindex="-1">Developer Wiki</a></li>
<li><a href="/versions/1.5.0/model_zoo/index.html" tabindex="-1">Gluon Model Zoo</a></li>
<li><a href="https://github.com/onnx/onnx-mxnet" tabindex="-1">ONNX</a></li>
</ul>
</li>
<li class="dropdown-submenu dropdown">
<a aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" role="button" tabindex="-1">Community</a>
<ul class="dropdown-menu">
<li><a href="http://discuss.mxnet.io" tabindex="-1">Forum</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.5.0" tabindex="-1">Github</a></li>
<li><a href="/versions/1.5.0/community/contribute.html" tabindex="-1">Contribute</a></li>
<li><a href="/versions/1.5.0/community/ecosystem.html" tabindex="-1">Ecosystem</a></li>
<li><a href="/versions/1.5.0/community/powered_by.html" tabindex="-1">Powered By</a></li>
</ul>
</li>
<li id="dropdown-menu-position-anchor-version-mobile" class="dropdown-submenu" style="position: relative"><a href="#" tabindex="-1">1.5.0</a><ul class="dropdown-menu"><li><a tabindex="-1" href=/>master</a></li><li><a tabindex="-1" href=/versions/1.6/>1.6</a></li><li><a tabindex="-1" href=/versions/1.5.0/>1.5.0</a></li><li><a tabindex="-1" href=/versions/1.4.1/>1.4.1</a></li><li><a tabindex="-1" href=/versions/1.3.1/>1.3.1</a></li><li><a tabindex="-1" href=/versions/1.2.1/>1.2.1</a></li><li><a tabindex="-1" href=/versions/1.1.0/>1.1.0</a></li><li><a tabindex="-1" href=/versions/1.0.0/>1.0.0</a></li><li><a tabindex="-1" href=/versions/0.12.1/>0.12.1</a></li><li><a tabindex="-1" href=/versions/0.11.0/>0.11.0</a></li></ul></li></ul>
</div>
<div class="plusIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button"><span aria-hidden="true" class="glyphicon glyphicon-plus"></span></a>
<ul class="dropdown-menu dropdown-menu-right" id="plusMenu"></ul>
</div>
<div id="search-input-wrap">
<form action="../../../search.html" autocomplete="off" class="" method="get" role="search">
<div class="form-group inner-addon left-addon">
<i class="glyphicon glyphicon-search"></i>
<input class="form-control" name="q" placeholder="Search" type="text"/>
</div>
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default"/>
</input></form>
<div id="search-preview"></div>
</div>
<div id="searchIcon">
<span aria-hidden="true" class="glyphicon glyphicon-search"></span>
</div>
<!-- <div id="lang-select-wrap"> -->
<!--   <label id="lang-select-label"> -->
<!--     <\!-- <i class="fa fa-globe"></i> -\-> -->
<!--     <span></span> -->
<!--   </label> -->
<!--   <select id="lang-select"> -->
<!--     <option value="en">Eng</option> -->
<!--     <option value="zh">中文</option> -->
<!--   </select> -->
<!-- </div> -->
<!--     <a id="mobile-nav-toggle">
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
      </a> -->
</div>
</div>
</div>
<script type="text/javascript">
        $('body').css('background', 'white');
    </script>
<div class="container">
<div class="row">
<div aria-label="main navigation" class="sphinxsidebar leftsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">MXNet APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/index.html">MXNet Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/index.html">MXNet Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/index.html">MXNet FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gluon/index.html">About Gluon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Installing MXNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html#nvidia-jetson-tx-family">Nvidia Jetson TX family</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html#source-download">Source Download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo/index.html">MXNet Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Tutorials</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="page-tracker"></div>
<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at --><!---   http://www.apache.org/licenses/LICENSE-2.0 --><!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. --><div class="section" id="gluon-contrib-api">
<span id="gluon-contrib-api"></span><h1>Gluon Contrib API<a class="headerlink" href="#gluon-contrib-api" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<span id="overview"></span><h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This document lists the contrib APIs in Gluon:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#module-mxnet.gluon.contrib" title="mxnet.gluon.contrib"><code class="xref py py-obj docutils literal"><span class="pre">mxnet.gluon.contrib</span></code></a></td>
<td>Contrib neural network module.</td>
</tr>
</tbody>
</table>
<p>The <code class="docutils literal"><span class="pre">Gluon</span> <span class="pre">Contrib</span></code> API, defined in the <code class="docutils literal"><span class="pre">gluon.contrib</span></code> package, provides
many useful experimental APIs for new features.
This is a place for the community to try out the new features,
so that feature contributors can receive feedback.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This package contains experimental APIs and may change in the near future.</p>
</div>
<p>In the rest of this document, we list routines provided by the <code class="docutils literal"><span class="pre">gluon.contrib</span></code> package.</p>
</div>
<div class="section" id="contrib">
<span id="contrib"></span><h2>Contrib<a class="headerlink" href="#contrib" title="Permalink to this headline">¶</a></h2>
<div class="section" id="neural-network">
<span id="neural-network"></span><h3>Neural network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.Concurrent" title="mxnet.gluon.contrib.nn.Concurrent"><code class="xref py py-obj docutils literal"><span class="pre">Concurrent</span></code></a></td>
<td>Lays <cite>Block</cite> s concurrently.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.HybridConcurrent" title="mxnet.gluon.contrib.nn.HybridConcurrent"><code class="xref py py-obj docutils literal"><span class="pre">HybridConcurrent</span></code></a></td>
<td>Lays <cite>HybridBlock</cite> s concurrently.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.Identity" title="mxnet.gluon.contrib.nn.Identity"><code class="xref py py-obj docutils literal"><span class="pre">Identity</span></code></a></td>
<td>Block that passes through the input directly.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.SparseEmbedding" title="mxnet.gluon.contrib.nn.SparseEmbedding"><code class="xref py py-obj docutils literal"><span class="pre">SparseEmbedding</span></code></a></td>
<td>Turns non-negative integers (indexes/tokens) into dense vectors of fixed size.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.SyncBatchNorm" title="mxnet.gluon.contrib.nn.SyncBatchNorm"><code class="xref py py-obj docutils literal"><span class="pre">SyncBatchNorm</span></code></a></td>
<td>Cross-GPU Synchronized Batch normalization (SyncBN)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.PixelShuffle1D" title="mxnet.gluon.contrib.nn.PixelShuffle1D"><code class="xref py py-obj docutils literal"><span class="pre">PixelShuffle1D</span></code></a></td>
<td>Pixel-shuffle layer for upsampling in 1 dimension.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.PixelShuffle2D" title="mxnet.gluon.contrib.nn.PixelShuffle2D"><code class="xref py py-obj docutils literal"><span class="pre">PixelShuffle2D</span></code></a></td>
<td>Pixel-shuffle layer for upsampling in 2 dimensions.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.PixelShuffle3D" title="mxnet.gluon.contrib.nn.PixelShuffle3D"><code class="xref py py-obj docutils literal"><span class="pre">PixelShuffle3D</span></code></a></td>
<td>Pixel-shuffle layer for upsampling in 3 dimensions.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="convolutional-neural-network">
<span id="convolutional-neural-network"></span><h3>Convolutional neural network<a class="headerlink" href="#convolutional-neural-network" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.cnn.DeformableConvolution" title="mxnet.gluon.contrib.cnn.DeformableConvolution"><code class="xref py py-obj docutils literal"><span class="pre">DeformableConvolution</span></code></a></td>
<td>2-D Deformable Convolution v_1 (Dai, 2017).</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="recurrent-neural-network">
<span id="recurrent-neural-network"></span><h3>Recurrent neural network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell" title="mxnet.gluon.contrib.rnn.VariationalDropoutCell"><code class="xref py py-obj docutils literal"><span class="pre">VariationalDropoutCell</span></code></a></td>
<td>Applies Variational Dropout on base cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DRNNCell" title="mxnet.gluon.contrib.rnn.Conv1DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DRNNCell</span></code></a></td>
<td>1D Convolutional RNN cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DRNNCell" title="mxnet.gluon.contrib.rnn.Conv2DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DRNNCell</span></code></a></td>
<td>2D Convolutional RNN cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DRNNCell" title="mxnet.gluon.contrib.rnn.Conv3DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DRNNCell</span></code></a></td>
<td>3D Convolutional RNN cells</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv1DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DLSTMCell</span></code></a></td>
<td>1D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv2DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DLSTMCell</span></code></a></td>
<td>2D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv3DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DLSTMCell</span></code></a></td>
<td>3D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DGRUCell" title="mxnet.gluon.contrib.rnn.Conv1DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DGRUCell</span></code></a></td>
<td>1D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DGRUCell" title="mxnet.gluon.contrib.rnn.Conv2DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DGRUCell</span></code></a></td>
<td>2D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DGRUCell" title="mxnet.gluon.contrib.rnn.Conv3DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DGRUCell</span></code></a></td>
<td>3D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.LSTMPCell" title="mxnet.gluon.contrib.rnn.LSTMPCell"><code class="xref py py-obj docutils literal"><span class="pre">LSTMPCell</span></code></a></td>
<td>Long-Short Term Memory Projected (LSTMP) network cell.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="data">
<span id="data"></span><h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.IntervalSampler" title="mxnet.gluon.contrib.data.IntervalSampler"><code class="xref py py-obj docutils literal"><span class="pre">IntervalSampler</span></code></a></td>
<td>Samples elements from [0, length) at fixed intervals.</td>
</tr>
</tbody>
</table>
<div class="section" id="text-dataset">
<span id="text-dataset"></span><h4>Text dataset<a class="headerlink" href="#text-dataset" title="Permalink to this headline">¶</a></h4>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.text.WikiText2" title="mxnet.gluon.contrib.data.text.WikiText2"><code class="xref py py-obj docutils literal"><span class="pre">WikiText2</span></code></a></td>
<td>WikiText-2 word-level dataset for language modeling, from Salesforce research.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.text.WikiText103" title="mxnet.gluon.contrib.data.text.WikiText103"><code class="xref py py-obj docutils literal"><span class="pre">WikiText103</span></code></a></td>
<td>WikiText-103 word-level dataset for language modeling, from Salesforce research.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="api-reference">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#api-reference" title="Permalink to this headline">¶</a></h2>
<script src="../../../_static/js/auto_module_index.js" type="text/javascript"></script><span class="target" id="module-mxnet.gluon.contrib"></span><p>Contrib neural network module.</p>
<span class="target" id="module-mxnet.gluon.contrib.nn"></span><p>Contributed neural network modules.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.Concurrent">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">Concurrent</code><span class="sig-paren">(</span><em>axis=-1</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#Concurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.Concurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Lays <cite>Block</cite> s concurrently.</p>
<p>This block feeds its input to all children blocks, and
produce the output by concatenating all the children blocks’ outputs
on the specified axis.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Concurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give children blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>axis</strong> (<em>int</em><em>, </em><em>default -1</em>) – The axis on which to concatenate the outputs.</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.HybridConcurrent">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">HybridConcurrent</code><span class="sig-paren">(</span><em>axis=-1</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#HybridConcurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.HybridConcurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Lays <cite>HybridBlock</cite> s concurrently.</p>
<p>This block feeds its input to all children blocks, and
produce the output by concatenating all the children blocks’ outputs
on the specified axis.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">HybridConcurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give children blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>axis</strong> (<em>int</em><em>, </em><em>default -1</em>) – The axis on which to concatenate the outputs.</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.Identity">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">Identity</code><span class="sig-paren">(</span><em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#Identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>Block that passes through the input directly.</p>
<p>This block can be used in conjunction with HybridConcurrent
block for residual connection.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">HybridConcurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give child Blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.SparseEmbedding">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">SparseEmbedding</code><span class="sig-paren">(</span><em>input_dim</em>, <em>output_dim</em>, <em>dtype='float32'</em>, <em>weight_initializer=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#SparseEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.SparseEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Turns non-negative integers (indexes/tokens) into dense vectors
of fixed size. eg. [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]</p>
<p>This SparseBlock is designed for distributed training with extremely large
input dimension. Both weight and gradient w.r.t. weight are <cite>RowSparseNDArray</cite>.</p>
<p>Note: if <cite>sparse_grad</cite> is set to True, the gradient w.r.t weight will be
sparse. Only a subset of optimizers support sparse gradients, including SGD, AdaGrad
and Adam. By default lazy updates is turned on, which may perform differently
from standard updates. For more details, please check the Optimization API at:
<a class="reference external" href="/api/python/optimization/optimization.html">/api/python/optimization/optimization.html</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_dim</strong> (<em>int</em>) – Size of the vocabulary, i.e. maximum integer index + 1.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Dimension of the dense embedding.</li>
<li><strong>dtype</strong> (<em>str</em><em> or </em><em>np.dtype</em><em>, </em><em>default 'float32'</em>) – Data type of output embeddings.</li>
<li><strong>weight_initializer</strong> (<a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the <cite>embeddings</cite> matrix.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: (N-1)-D tensor with shape: <cite>(x1, x2, ..., xN-1)</cite>.</li>
</ul>
</li>
<li><strong>Output</strong> – <ul>
<li><strong>out</strong>: N-D tensor with shape: <cite>(x1, x2, ..., xN-1, output_dim)</cite>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.SyncBatchNorm">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">SyncBatchNorm</code><span class="sig-paren">(</span><em>in_channels=0</em>, <em>num_devices=None</em>, <em>momentum=0.9</em>, <em>epsilon=1e-05</em>, <em>center=True</em>, <em>scale=True</em>, <em>use_global_stats=False</em>, <em>beta_initializer='zeros'</em>, <em>gamma_initializer='ones'</em>, <em>running_mean_initializer='zeros'</em>, <em>running_variance_initializer='ones'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#SyncBatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.SyncBatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-GPU Synchronized Batch normalization (SyncBN)</p>
<p>Standard BN <a class="footnote-reference" href="#id3" id="id1">[1]</a> implementation only normalize the data within each device.
SyncBN normalizes the input within the whole mini-batch.
We follow the implementation described in the paper <a class="footnote-reference" href="#id4" id="id2">[2]</a>.</p>
<p>Note: Current implementation of SyncBN does not support FP16 training.
For FP16 inference, use standard nn.BatchNorm instead of SyncBN.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<em>int</em><em>, </em><em>default 0</em>) – Number of channels (feature maps) in input data. If not specified,
initialization will be deferred to the first time <cite>forward</cite> is called
and <cite>in_channels</cite> will be inferred from the shape of input data.</li>
<li><strong>num_devices</strong> (<em>int</em><em>, </em><em>default number of visible GPUs</em>) – </li>
<li><strong>momentum</strong> (<em>float</em><em>, </em><em>default 0.9</em>) – Momentum for the moving average.</li>
<li><strong>epsilon</strong> (<em>float</em><em>, </em><em>default 1e-5</em>) – Small float added to variance to avoid dividing by zero.</li>
<li><strong>center</strong> (<em>bool</em><em>, </em><em>default True</em>) – If True, add offset of <cite>beta</cite> to normalized tensor.
If False, <cite>beta</cite> is ignored.</li>
<li><strong>scale</strong> (<em>bool</em><em>, </em><em>default True</em>) – If True, multiply by <cite>gamma</cite>. If False, <cite>gamma</cite> is not used.
When the next layer is linear (also e.g. <cite>nn.relu</cite>),
this can be disabled since the scaling
will be done by the next layer.</li>
<li><strong>use_global_stats</strong> (<em>bool</em><em>, </em><em>default False</em>) – If True, use global moving statistics instead of local batch-norm. This will force
change batch-norm into a scale shift operator.
If False, use local batch-norm.</li>
<li><strong>beta_initializer</strong> (str or <cite>Initializer</cite>, default ‘zeros’) – Initializer for the beta weight.</li>
<li><strong>gamma_initializer</strong> (str or <cite>Initializer</cite>, default ‘ones’) – Initializer for the gamma weight.</li>
<li><strong>running_mean_initializer</strong> (str or <cite>Initializer</cite>, default ‘zeros’) – Initializer for the running mean.</li>
<li><strong>running_variance_initializer</strong> (str or <cite>Initializer</cite>, default ‘ones’) – Initializer for the running variance.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><strong>data</strong>: input tensor with arbitrary shape.</li>
</ul>
</dd>
<dt>Outputs:</dt>
<dd><ul class="first last simple">
<li><strong>out</strong>: output tensor with the same shape as <cite>data</cite>.</li>
</ul>
</dd>
<dt>Reference:</dt>
<dd><table class="first docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating           deep network training by reducing internal covariate shift.” <em>ICML 2015</em></td></tr>
</tbody>
</table>
<table class="last docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang,           Ambrish Tyagi, and Amit Agrawal. “Context Encoding for Semantic Segmentation.” <em>CVPR 2018</em></td></tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle1D">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">PixelShuffle1D</code><span class="sig-paren">(</span><em>factor</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle1D" title="Permalink to this definition">¶</a></dt>
<dd><p>Pixel-shuffle layer for upsampling in 1 dimension.</p>
<p>Pixel-shuffling is the operation of taking groups of values along
the <em>channel</em> dimension and regrouping them into blocks of pixels
along the <code class="docutils literal"><span class="pre">W</span></code> dimension, thereby effectively multiplying that dimension
by a constant factor in size.</p>
<p>For example, a feature map of shape <span class="math">\((fC, W)\)</span> is reshaped
into <span class="math">\((C, fW)\)</span> by forming little value groups of size <span class="math">\(f\)</span>
and arranging them in a grid of size <span class="math">\(W\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>factor</strong> (<em>int</em><em> or </em><em>1-tuple of int</em>) – Upsampling factor, applied to the <code class="docutils literal"><span class="pre">W</span></code> dimension.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">f*C,</span> <span class="pre">W)</span></code>.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">C,</span> <span class="pre">W*f)</span></code>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">>>> </span><span class="n">pxshuf</span> <span class="o">=</span> <span class="n">PixelShuffle1D</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">>>> </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">>>> </span><span class="n">pxshuf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 4, 6)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle1D.hybrid_forward">
<code class="descname">hybrid_forward</code><span class="sig-paren">(</span><em>F</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle1D.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle1D.hybrid_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform pixel-shuffling on the input.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle2D">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">PixelShuffle2D</code><span class="sig-paren">(</span><em>factor</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Pixel-shuffle layer for upsampling in 2 dimensions.</p>
<p>Pixel-shuffling is the operation of taking groups of values along
the <em>channel</em> dimension and regrouping them into blocks of pixels
along the <code class="docutils literal"><span class="pre">H</span></code> and <code class="docutils literal"><span class="pre">W</span></code> dimensions, thereby effectively multiplying
those dimensions by a constant factor in size.</p>
<p>For example, a feature map of shape <span class="math">\((f^2 C, H, W)\)</span> is reshaped
into <span class="math">\((C, fH, fW)\)</span> by forming little <span class="math">\(f \times f\)</span> blocks
of pixels and arranging them in an <span class="math">\(H \times W\)</span> grid.</p>
<p>Pixel-shuffling together with regular convolution is an alternative,
learnable way of upsampling an image by arbitrary factors. It is reported
to help overcome checkerboard artifacts that are common in upsampling with
transposed convolutions (also called deconvolutions). See the paper
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient
Sub-Pixel Convolutional Neural Network</a>
for further details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>factor</strong> (<em>int</em><em> or </em><em>2-tuple of int</em>) – Upsampling factors, applied to the <code class="docutils literal"><span class="pre">H</span></code> and <code class="docutils literal"><span class="pre">W</span></code> dimensions,
in that order.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">f1*f2*C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">C,</span> <span class="pre">H*f1,</span> <span class="pre">W*f2)</span></code>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">>>> </span><span class="n">pxshuf</span> <span class="o">=</span> <span class="n">PixelShuffle2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">>>> </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">>>> </span><span class="n">pxshuf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 2, 6, 15)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle2D.hybrid_forward">
<code class="descname">hybrid_forward</code><span class="sig-paren">(</span><em>F</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle2D.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle2D.hybrid_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform pixel-shuffling on the input.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle3D">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">PixelShuffle3D</code><span class="sig-paren">(</span><em>factor</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle3D" title="Permalink to this definition">¶</a></dt>
<dd><p>Pixel-shuffle layer for upsampling in 3 dimensions.</p>
<p>Pixel-shuffling (or voxel-shuffling in 3D) is the operation of taking
groups of values along the <em>channel</em> dimension and regrouping them into
blocks of voxels along the <code class="docutils literal"><span class="pre">D</span></code>, <code class="docutils literal"><span class="pre">H</span></code> and <code class="docutils literal"><span class="pre">W</span></code> dimensions, thereby
effectively multiplying those dimensions by a constant factor in size.</p>
<p>For example, a feature map of shape <span class="math">\((f^3 C, D, H, W)\)</span> is reshaped
into <span class="math">\((C, fD, fH, fW)\)</span> by forming little <span class="math">\(f \times f \times f\)</span>
blocks of voxels and arranging them in a <span class="math">\(D \times H \times W\)</span> grid.</p>
<p>Pixel-shuffling together with regular convolution is an alternative,
learnable way of upsampling an image by arbitrary factors. It is reported
to help overcome checkerboard artifacts that are common in upsampling with
transposed convolutions (also called deconvolutions). See the paper
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient
Sub-Pixel Convolutional Neural Network</a>
for further details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>factor</strong> (<em>int</em><em> or </em><em>3-tuple of int</em>) – Upsampling factors, applied to the <code class="docutils literal"><span class="pre">D</span></code>, <code class="docutils literal"><span class="pre">H</span></code> and <code class="docutils literal"><span class="pre">W</span></code>
dimensions, in that order.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">f1*f2*f3*C,</span> <span class="pre">D,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: Tensor of shape <code class="docutils literal"><span class="pre">(N,</span> <span class="pre">C,</span> <span class="pre">D*f1,</span> <span class="pre">H*f2,</span> <span class="pre">W*f3)</span></code>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">>>> </span><span class="n">pxshuf</span> <span class="o">=</span> <span class="n">PixelShuffle3D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">>>> </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">>>> </span><span class="n">pxshuf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 2, 6, 15, 28)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mxnet.gluon.contrib.nn.PixelShuffle3D.hybrid_forward">
<code class="descname">hybrid_forward</code><span class="sig-paren">(</span><em>F</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#PixelShuffle3D.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.PixelShuffle3D.hybrid_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform pixel-shuffling on the input.</p>
</dd></dl>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.cnn"></span><p>Contrib convolutional neural network module.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.cnn.DeformableConvolution">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.cnn.</code><code class="descname">DeformableConvolution</code><span class="sig-paren">(</span><em>channels</em>, <em>kernel_size=(1</em>, <em>1)</em>, <em>strides=(1</em>, <em>1)</em>, <em>padding=(0</em>, <em>0)</em>, <em>dilation=(1</em>, <em>1)</em>, <em>groups=1</em>, <em>num_deformable_group=1</em>, <em>layout='NCHW'</em>, <em>use_bias=True</em>, <em>in_channels=0</em>, <em>activation=None</em>, <em>weight_initializer=None</em>, <em>bias_initializer='zeros'</em>, <em>offset_weight_initializer='zeros'</em>, <em>offset_bias_initializer='zeros'</em>, <em>offset_use_bias=True</em>, <em>op_name='DeformableConvolution'</em>, <em>adj=None</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/cnn/conv_layers.html#DeformableConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.cnn.DeformableConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D Deformable Convolution v_1 (Dai, 2017).
Normal Convolution uses sampling points in a regular grid, while the sampling
points of Deformablem Convolution can be offset. The offset is learned with a
separate convolution layer during the training. Both the convolution layer for
generating the output features and the offsets are included in this gluon layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>channels</strong> (<em>int</em><em>,</em>) – The dimensionality of the output space
i.e. the number of output channels in the convolution.</li>
<li><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple/list of 2 ints</em><em>, </em><em>(</em><em>Default value =</em><em> (</em><em>1</em><em>,</em><em>1</em><em>)</em><em>)</em>) – Specifies the dimensions of the convolution window.</li>
<li><strong>strides</strong> (<em>int</em><em> or </em><em>tuple/list of 2 ints</em><em>, </em><em>(</em><em>Default value =</em><em> (</em><em>1</em><em>,</em><em>1</em><em>)</em><em>)</em>) – Specifies the strides of the convolution.</li>
<li><strong>padding</strong> (<em>int</em><em> or </em><em>tuple/list of 2 ints</em><em>, </em><em>(</em><em>Default value =</em><em> (</em><em>0</em><em>,</em><em>0</em><em>)</em><em>)</em>) – If padding is non-zero, then the input is implicitly zero-padded
on both sides for padding number of points.</li>
<li><strong>dilation</strong> (<em>int</em><em> or </em><em>tuple/list of 2 ints</em><em>, </em><em>(</em><em>Default value =</em><em> (</em><em>1</em><em>,</em><em>1</em><em>)</em><em>)</em>) – Specifies the dilation rate to use for dilated convolution.</li>
<li><strong>groups</strong> (<em>int</em><em>, </em><em>(</em><em>Default value = 1</em><em>)</em>) – Controls the connections between inputs and outputs.
At groups=1, all inputs are convolved to all outputs.
At groups=2, the operation becomes equivalent to having two convolution
layers side by side, each seeing half the input channels, and producing
half the output channels, and both subsequently concatenated.</li>
<li><strong>num_deformable_group</strong> (<em>int</em><em>, </em><em>(</em><em>Default value = 1</em><em>)</em>) – Number of deformable group partitions.</li>
<li><strong>layout</strong> (<em>str</em><em>, </em><em>(</em><em>Default value = NCHW</em><em>)</em>) – Dimension ordering of data and weight. Can be ‘NCW’, ‘NWC’, ‘NCHW’,
‘NHWC’, ‘NCDHW’, ‘NDHWC’, etc. ‘N’, ‘C’, ‘H’, ‘W’, ‘D’ stands for
batch, channel, height, width and depth dimensions respectively.
Convolution is performed over ‘D’, ‘H’, and ‘W’ dimensions.</li>
<li><strong>use_bias</strong> (<em>bool</em><em>, </em><em>(</em><em>Default value = True</em><em>)</em>) – Whether the layer for generating the output features uses a bias vector.</li>
<li><strong>in_channels</strong> (<em>int</em><em>, </em><em>(</em><em>Default value = 0</em><em>)</em>) – The number of input channels to this layer. If not specified,
initialization will be deferred to the first time <cite>forward</cite> is called
and input channels will be inferred from the shape of input data.</li>
<li><strong>activation</strong> (<em>str</em><em>, </em><em>(</em><em>Default value = None</em><em>)</em>) – Activation function to use. See <a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a>.
If you don’t specify anything, no activation is applied
(ie. “linear” activation: <cite>a(x) = x</cite>).</li>
<li><strong>weight_initializer</strong> (str or <cite>Initializer</cite>, (Default value = None)) – Initializer for the <cite>weight</cite> weights matrix for the convolution layer
for generating the output features.</li>
<li><strong>bias_initializer</strong> (str or <cite>Initializer</cite>, (Default value = zeros)) – Initializer for the bias vector for the convolution layer
for generating the output features.</li>
<li><strong>offset_weight_initializer</strong> (str or <cite>Initializer</cite>, (Default value = zeros)) – Initializer for the <cite>weight</cite> weights matrix for the convolution layer
for generating the offset.</li>
<li><strong>offset_bias_initializer</strong> (str or <cite>Initializer</cite>, (Default value = zeros),) – Initializer for the bias vector for the convolution layer
for generating the offset.</li>
<li><strong>offset_use_bias</strong> (<em>bool</em><em>, </em><em>(</em><em>Default value = True</em><em>)</em>) – Whether the layer for generating the offset uses a bias vector.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: 4D input tensor with shape
<cite>(batch_size, in_channels, height, width)</cite> when <cite>layout</cite> is <cite>NCHW</cite>.
For other layouts shape is permuted accordingly.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: 4D output tensor with shape
<cite>(batch_size, channels, out_height, out_width)</cite> when <cite>layout</cite> is <cite>NCHW</cite>.
out_height and out_width are calculated as:<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">out_height</span> <span class="o">=</span> <span class="n">floor</span><span class="p">((</span><span class="n">height</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="mi">1</span>
<span class="n">out_width</span> <span class="o">=</span> <span class="n">floor</span><span class="p">((</span><span class="n">width</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="mi">1</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.rnn"></span><p>Contrib recurrent neural network module.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional RNN cell.</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_rnn_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional RNN cell.</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_rnn_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional RNN cells</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_rnn_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_lstm_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_lstm_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_lstm_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_gru_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_gru_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'conv_gru_</span></code>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.VariationalDropoutCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">VariationalDropoutCell</code><span class="sig-paren">(</span><em>base_cell</em>, <em>drop_inputs=0.0</em>, <em>drop_states=0.0</em>, <em>drop_outputs=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#VariationalDropoutCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Variational Dropout on base cell.
<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a></p>
<p>Variational dropout uses the same dropout mask across time-steps. It can be applied to RNN
inputs, outputs, and states. The masks for them are not shared.</p>
<p>The dropout mask is initialized when stepping forward for the first time and will remain
the same until .reset() is called. Thus, if using the cell and stepping manually without calling
.unroll(), the .reset() should be called after each sequence.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_cell</strong> (<a class="reference internal" href="rnn.html#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>) – The cell on which to perform variational dropout.</li>
<li><strong>drop_inputs</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for inputs. Won’t apply dropout if it equals 0.</li>
<li><strong>drop_states</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for state inputs on the first state channel.
Won’t apply dropout if it equals 0.</li>
<li><strong>drop_outputs</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for outputs. Won’t apply dropout if it equals 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.contrib.rnn.VariationalDropoutCell.unroll">
<code class="descname">unroll</code><span class="sig-paren">(</span><em>length</em>, <em>inputs</em>, <em>begin_state=None</em>, <em>layout='NTC'</em>, <em>merge_outputs=None</em>, <em>valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#VariationalDropoutCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell.unroll" title="Permalink to this definition">¶</a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>length</strong> (<em>int</em>) – Number of steps to unroll.</li>
<li><strong>inputs</strong> (<a class="reference internal" href="../symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>) – <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ...) if <cite>layout</cite> is ‘NTC’,
or (length, batch_size, ...) if <cite>layout</cite> is ‘TNC’.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ...).</p>
</li>
<li><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>) – Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</li>
<li><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>) – <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</li>
<li><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ...) if layout is ‘NTC’,
or (length, batch_size, ...) if layout is ‘TNC’.
If <cite>None</cite>, output whatever is faster.</li>
<li><strong>valid_length</strong> (<a class="reference internal" href="../symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>) – <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>outputs</strong> (<em>list of Symbol or Symbol</em>) – Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</li>
<li><strong>states</strong> (<em>list of Symbol</em>) – The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.LSTMPCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">LSTMPCell</code><span class="sig-paren">(</span><em>hidden_size</em>, <em>projection_size</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>h2r_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>input_size=0</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#LSTMPCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.LSTMPCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Long-Short Term Memory Projected (LSTMP) network cell.
(<a class="reference external" href="https://arxiv.org/abs/1402.1128">https://arxiv.org/abs/1402.1128</a>)</p>
<p>Each call computes the following function:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{ri} r_{(t-1)} + b_{ri}) \\
f_t = sigmoid(W_{if} x_t + b_{if} + W_{rf} r_{(t-1)} + b_{rf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{rc} r_{(t-1)} + b_{rg}) \\
o_t = sigmoid(W_{io} x_t + b_{io} + W_{ro} r_{(t-1)} + b_{ro}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t) \\
r_t = W_{hr} h_t
\end{array}\end{split}\]</div>
<p>where <span class="math">\(r_t\)</span> is the projected recurrent activation at time <cite>t</cite>,
<span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the
cell state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input at time <cite>t</cite>, and <span class="math">\(i_t\)</span>,
<span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>, <span class="math">\(o_t\)</span> are the input, forget, cell, and
out gates, respectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>hidden_size</strong> (<em>int</em>) – Number of units in cell state symbol.</li>
<li><strong>projection_size</strong> (<em>int</em>) – Number of units in output symbol.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the linear
transformation of the inputs.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the linear
transformation of the hidden state.</li>
<li><strong>h2r_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the projection weights matrix, used for the linear
transformation of the recurrent state.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'lstmbias'</em>) – Initializer for the bias vector. By default, bias for the forget
gate is initialized to 1 while all other biases are initialized
to zero.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the bias vector.</li>
<li><strong>prefix</strong> (str, default <code class="docutils literal"><span class="pre">'lstmp_</span></code>‘) – Prefix for name of <cite>Block`s
(and name of weight if params is `None</cite>).</li>
<li><strong>params</strong> (<a class="reference internal" href="gluon.html#mxnet.gluon.Parameter" title="mxnet.gluon.Parameter"><em>Parameter</em></a><em> or </em><em>None</em>) – Container for weight sharing between cells.
Created if <cite>None</cite>.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</li>
<li><strong>states</strong>: a list of two initial recurrent state tensors, with shape
<cite>(batch_size, projection_size)</cite> and <cite>(batch_size, hidden_size)</cite> respectively.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</li>
<li><strong>next_states</strong>: a list of two output recurrent state tensors. Each has
the same shape as <cite>states</cite>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.data"></span><p>Contrib datasets.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.IntervalSampler">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.</code><code class="descname">IntervalSampler</code><span class="sig-paren">(</span><em>length</em>, <em>interval</em>, <em>rollover=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/sampler.html#IntervalSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.IntervalSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements from [0, length) at fixed intervals.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>length</strong> (<em>int</em>) – Length of the sequence.</li>
<li><strong>interval</strong> (<em>int</em>) – The number of items to skip between two samples.</li>
<li><strong>rollover</strong> (<em>bool</em><em>, </em><em>default True</em>) – Whether to start again from the first skipped item after reaching the end.
If true, this sampler would start again from the first skipped item until all items
are visited.
Otherwise, iteration stops when end is reached and skipped items are ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">>>> </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">contrib</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IntervalSampler</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">>>> </span><span class="nb">list</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
<span class="go">[0, 3, 6, 9, 12, 1, 4, 7, 10, 2, 5, 8, 11]</span>
<span class="gp">>>> </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">contrib</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IntervalSampler</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rollover</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">>>> </span><span class="nb">list</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
<span class="go">[0, 3, 6, 9, 12]</span>
</pre></div>
</div>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.data.text"></span><p>Text datasets.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.text.WikiText2">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.text.</code><code class="descname">WikiText2</code><span class="sig-paren">(</span><em>root=u'/work/mxnet/docs/build_version_doc/apache-mxnet/v1.5.x/datasets/wikitext-2'</em>, <em>segment='train'</em>, <em>vocab=None</em>, <em>seq_len=35</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/text.html#WikiText2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.text.WikiText2" title="Permalink to this definition">¶</a></dt>
<dd><p>WikiText-2 word-level dataset for language modeling, from Salesforce research.</p>
<p>From
<a class="reference external" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset</a></p>
<p>License: Creative Commons Attribution-ShareAlike</p>
<p>Each sample is a vector of length equal to the specified sequence length.
At the end of each sentence, an end-of-sentence token ‘<eos>’ is added.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>str</em><em>, </em><em>default $MXNET_HOME/datasets/wikitext-2</em>) – Path to temp folder for storing data.</li>
<li><strong>segment</strong> (<em>str</em><em>, </em><em>default 'train'</em>) – Dataset segment. Options are ‘train’, ‘validation’, ‘test’.</li>
<li><strong>vocab</strong> (<a class="reference internal" href="../contrib/text.html#mxnet.contrib.text.vocab.Vocabulary" title="mxnet.contrib.text.vocab.Vocabulary"><code class="xref py py-class docutils literal"><span class="pre">Vocabulary</span></code></a>, default None) – The vocabulary to use for indexing the text dataset.
If None, a default vocabulary is created.</li>
<li><strong>seq_len</strong> (<em>int</em><em>, </em><em>default 35</em>) – The sequence length of each sample, regardless of the sentence boundary.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.text.WikiText103">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.text.</code><code class="descname">WikiText103</code><span class="sig-paren">(</span><em>root=u'/work/mxnet/docs/build_version_doc/apache-mxnet/v1.5.x/datasets/wikitext-103'</em>, <em>segment='train'</em>, <em>vocab=None</em>, <em>seq_len=35</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/text.html#WikiText103"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.text.WikiText103" title="Permalink to this definition">¶</a></dt>
<dd><p>WikiText-103 word-level dataset for language modeling, from Salesforce research.</p>
<p>From
<a class="reference external" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset</a></p>
<p>License: Creative Commons Attribution-ShareAlike</p>
<p>Each sample is a vector of length equal to the specified sequence length.
At the end of each sentence, an end-of-sentence token ‘<eos>’ is added.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>str</em><em>, </em><em>default $MXNET_HOME/datasets/wikitext-103</em>) – Path to temp folder for storing data.</li>
<li><strong>segment</strong> (<em>str</em><em>, </em><em>default 'train'</em>) – Dataset segment. Options are ‘train’, ‘validation’, ‘test’.</li>
<li><strong>vocab</strong> (<a class="reference internal" href="../contrib/text.html#mxnet.contrib.text.vocab.Vocabulary" title="mxnet.contrib.text.vocab.Vocabulary"><code class="xref py py-class docutils literal"><span class="pre">Vocabulary</span></code></a>, default None) – The vocabulary to use for indexing the text dataset.
If None, a default vocabulary is created.</li>
<li><strong>seq_len</strong> (<em>int</em><em>, </em><em>default 35</em>) – The sequence length of each sample, regardless of the sentence boundary.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<script>auto_index("api-reference");</script></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar rightsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h3><a href="../../../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">Gluon Contrib API</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#contrib">Contrib</a><ul>
<li><a class="reference internal" href="#neural-network">Neural network</a></li>
<li><a class="reference internal" href="#convolutional-neural-network">Convolutional neural network</a></li>
<li><a class="reference internal" href="#recurrent-neural-network">Recurrent neural network</a></li>
<li><a class="reference internal" href="#data">Data</a><ul>
<li><a class="reference internal" href="#text-dataset">Text dataset</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api-reference">API Reference</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div><div class="footer">
<div class="section-disclaimer">
<div class="container">
<div>
<img height="60" src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/apache_incubator_logo.png"/>
<p>
            Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <strong>sponsored by the <i>Apache Incubator</i></strong>. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
        </p>
<p>
            "Copyright © 2017-2018, The Apache Software Foundation
            Apache MXNet, MXNet, Apache, the Apache feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the Apache Software Foundation."
        </p>
</div>
</div>
</div>
</div> <!-- pagename != index -->
</div>
<script crossorigin="anonymous" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script src="../../../_static/js/sidebar.js" type="text/javascript"></script>
<script src="../../../_static/js/search.js" type="text/javascript"></script>
<script src="../../../_static/js/navbar.js" type="text/javascript"></script>
<script src="../../../_static/js/clipboard.min.js" type="text/javascript"></script>
<script src="../../../_static/js/copycode.js" type="text/javascript"></script>
<script src="../../../_static/js/page.js" type="text/javascript"></script>
<script src="../../../_static/js/docversion.js" type="text/javascript"></script>
<script type="text/javascript">
        $('body').ready(function () {
            $('body').css('visibility', 'visible');
        });
    </script>
</body>
</html>