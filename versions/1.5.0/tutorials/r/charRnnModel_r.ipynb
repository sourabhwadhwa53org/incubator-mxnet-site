{"nbformat": 4, "cells": [{"source": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Character-level Language Model using RNN\n\nThis tutorial will demonstrate creating a language model using a character level RNN model using MXNet-R package. You will need the following R packages to run this tutorial -\n - readr\n - stringr\n - stringi\n - mxnet\n\nWe will use the [tinyshakespeare](https://github.com/dmlc/web-data/tree/master/mxnet/tinyshakespeare) dataset to build this model.", "cell_type": "markdown", "metadata": {}}, {"source": "library(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"stringi\")\nlibrary(\"mxnet\")", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Preprocess and prepare the data\n\nDownload the data:", "cell_type": "markdown", "metadata": {}}, {"source": "download.data <- function(data_dir) {\n    dir.create(data_dir, showWarnings = FALSE)\n    if (!file.exists(paste0(data_dir,'input.txt'))) {\n        download.file(url='https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt',\n                      destfile=paste0(data_dir,'input.txt'), method='wget')\n    }\n}", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Next we transform the test into feature vectors that is fed into the RNN model. The `make_data` function reads the dataset, cleans it of any non-alphanumeric characters, splits it into individual characters and groups it into sequences of length `seq.len`.", "cell_type": "markdown", "metadata": {}}, {"source": "make_data <- function(path, seq.len = 32, dic=NULL) {\n  \n  text_vec <- read_file(file = path)\n  text_vec <- stri_enc_toascii(str = text_vec)\n  text_vec <- str_replace_all(string = text_vec, pattern = \"[^[:print:]]\", replacement = \"\")\n  text_vec <- strsplit(text_vec, '') %>% unlist\n  \n  if (is.null(dic)) {\n    char_keep <- sort(unique(text_vec))\n  } else char_keep <- names(dic)[!dic == 0]\n  \n  # Remove terms not part of dictionary\n  text_vec <- text_vec[text_vec %in% char_keep]\n  \n  # Build dictionary\n  dic <- 1:length(char_keep)\n  names(dic) <- char_keep\n  \n  # reverse dictionary\n  rev_dic <- names(dic)\n  names(rev_dic) <- dic\n  \n  # Adjust by -1 to have a 1-lag for labels\n  num.seq <- (length(text_vec) - 1) %/% seq.len\n  \n  features <- dic[text_vec[1:(seq.len * num.seq)]] \n  labels <- dic[text_vec[1:(seq.len*num.seq) + 1]]\n  \n  features_array <- array(features, dim = c(seq.len, num.seq))\n  labels_array <- array(labels, dim = c(seq.len, num.seq))\n  \n  return (list(features_array = features_array, labels_array = labels_array, dic = dic, rev_dic = rev_dic))\n}\n\n\nseq.len <- 100\ndata_prep <- make_data(path = \"input.txt\", seq.len = seq.len, dic=NULL)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Fetch the features and labels for training the model, and split the data into training and evaluation in 9:1 ratio.", "cell_type": "markdown", "metadata": {}}, {"source": "X <- data_prep$features_array\nY <- data_prep$labels_array\ndic <- data_prep$dic\nrev_dic <- data_prep$rev_dic\nvocab <- length(dic)\n\nsamples <- tail(dim(X), 1)\ntrain.val.fraction <- 0.9\n\nX.train.data <- X[, 1:as.integer(samples * train.val.fraction)]\nX.val.data <- X[, -(1:as.integer(samples * train.val.fraction))]\n\nX.train.label <- Y[, 1:as.integer(samples * train.val.fraction)]\nX.val.label <- Y[, -(1:as.integer(samples * train.val.fraction))]\n\ntrain_buckets <- list(\"100\" = list(data = X.train.data, label = X.train.label))\neval_buckets <- list(\"100\" = list(data = X.val.data, label = X.val.label))\n\ntrain_buckets <- list(buckets = train_buckets, dic = dic, rev_dic = rev_dic)\neval_buckets <- list(buckets = eval_buckets, dic = dic, rev_dic = rev_dic)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Create iterators for training and evaluation datasets.", "cell_type": "markdown", "metadata": {}}, {"source": "vocab <- length(eval_buckets$dic)\n\nbatch.size <- 32\n\ntrain.data <- mx.io.bucket.iter(buckets = train_buckets$buckets, batch.size = batch.size, \n                                data.mask.element = 0, shuffle = TRUE)\n\neval.data <- mx.io.bucket.iter(buckets = eval_buckets$buckets, batch.size = batch.size,\n                               data.mask.element = 0, shuffle = FALSE)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Train the Model\n\n\nThis model is a multi-layer RNN for sampling from character-level language models. It has a one-to-one model configuration since for each character, we want to predict the next one. For a sequence of length 100, there are also 100 labels, corresponding the same sequence of characters but offset by a position of +1. The parameters output_last_state is set to TRUE in order to access the state of the RNN cells when performing inference.", "cell_type": "markdown", "metadata": {}}, {"source": "rnn_graph_one_one <- rnn.graph(num_rnn_layer = 3, \n                               num_hidden = 96,\n                               input_size = vocab,\n                               num_embed = 64, \n                               num_decode = vocab,\n                               dropout = 0.2, \n                               ignore_label = 0,\n                               cell_type = \"lstm\",\n                               masking = F,\n                               output_last_state = T,\n                               loss_output = \"softmax\",\n                               config = \"one-to-one\")\n\ngraph.viz(rnn_graph_one_one, type = \"graph\", direction = \"LR\", \n          graph.height.px = 180, shape=c(100, 64))\n\ndevices <- mx.cpu()\n\ninitializer <- mx.init.Xavier(rnd_type = \"gaussian\", factor_type = \"avg\", magnitude = 3)\n\noptimizer <- mx.opt.create(\"adadelta\", rho = 0.9, eps = 1e-5, wd = 1e-8,\n                           clip_gradient = 5, rescale.grad = 1/batch.size)\n\nlogger <- mx.metric.logger()\nepoch.end.callback <- mx.callback.log.train.metric(period = 1, logger = logger)\nbatch.end.callback <- mx.callback.log.train.metric(period = 50)\n\nmx.metric.custom_nd <- function(name, feval) {\n  init <- function() {\n    c(0, 0)\n  }\n  update <- function(label, pred, state) {\n    m <- feval(label, pred)\n    state <- c(state[[1]] + 1, state[[2]] + m)\n    return(state)\n  }\n  get <- function(state) {\n    list(name=name, value = (state[[2]] / state[[1]]))\n  }\n  ret <- (list(init = init, update = update, get = get))\n  class(ret) <- \"mx.metric\"\n  return(ret)\n}\n\nmx.metric.Perplexity <- mx.metric.custom_nd(\"Perplexity\", function(label, pred) {\n  label <- mx.nd.reshape(label, shape = -1)\n  label_probs <- as.array(mx.nd.choose.element.0index(pred, label))\n  batch <- length(label_probs)\n  NLL <- -sum(log(pmax(1e-15, as.array(label_probs)))) / batch\n  Perplexity <- exp(NLL)\n  return(Perplexity)\n})\n\nmodel <- mx.model.buckets(symbol = rnn_graph_one_one,\n                          train.data = train.data, eval.data = eval.data, \n                          num.round = 20, ctx = devices, verbose = TRUE,\n                          metric = mx.metric.Perplexity, \n                          initializer = initializer, optimizer = optimizer, \n                          batch.end.callback = NULL, \n                          epoch.end.callback = epoch.end.callback)\n\nmx.model.save(model, prefix = \"one_to_one_seq_model\", iteration = 20)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "    Start training with 1 devices\n    [1] Train-Perplexity=13.7040474322178\n    [1] Validation-Perplexity=7.94617194460922\n    [2] Train-Perplexity=6.57039815554525\n    [2] Validation-Perplexity=6.60806110658011\n    [3] Train-Perplexity=5.65360504501481\n    [3] Validation-Perplexity=6.18932770630876\n    [4] Train-Perplexity=5.32547285727298\n    [4] Validation-Perplexity=6.02198756798859\n    [5] Train-Perplexity=5.14373631472579\n    [5] Validation-Perplexity=5.8095658243407\n    [6] Train-Perplexity=5.03077673487379\n    [6] Validation-Perplexity=5.72582993567431\n    [7] Train-Perplexity=4.94453383291536\n    [7] Validation-Perplexity=5.6445258528126\n    [8] Train-Perplexity=4.88635290100261\n    [8] Validation-Perplexity=5.6730024536433\n    [9] Train-Perplexity=4.84205646230548\n    [9] Validation-Perplexity=5.50960780230982\n    [10] Train-Perplexity=4.80441673535513\n    [10] Validation-Perplexity=5.57002263750006\n    [11] Train-Perplexity=4.77763413242626\n    [11] Validation-Perplexity=5.55152143269169\n    [12] Train-Perplexity=4.74937775290777\n    [12] Validation-Perplexity=5.44968305351486\n    [13] Train-Perplexity=4.72824849541467\n    [13] Validation-Perplexity=5.50889348298234\n    [14] Train-Perplexity=4.70980846981694\n    [14] Validation-Perplexity=5.51473225859859\n    [15] Train-Perplexity=4.69685776886122\n    [15] Validation-Perplexity=5.45391985233811\n    [16] Train-Perplexity=4.67837107034824\n    [16] Validation-Perplexity=5.46636764997829\n    [17] Train-Perplexity=4.66866961934873\n    [17] Validation-Perplexity=5.44267086113492\n    [18] Train-Perplexity=4.65611469144194\n    [18] Validation-Perplexity=5.4290169469462\n    [19] Train-Perplexity=4.64614689879405\n    [19] Validation-Perplexity=5.44221549833917\n    [20] Train-Perplexity=4.63764001963654\n    [20] Validation-Perplexity=5.42114250842862\n\n\n## Inference on the Model\n\nWe now use the saved model to do inference and sample text character by character that will look like the original training data.", "cell_type": "markdown", "metadata": {}}, {"source": "set.seed(0)\nmodel <- mx.model.load(prefix = \"one_to_one_seq_model\", iteration = 20)\n\ninternals <- model$symbol$get.internals()\nsym_state <- internals$get.output(which(internals$outputs %in% \"RNN_state\"))\nsym_state_cell <- internals$get.output(which(internals$outputs %in% \"RNN_state_cell\"))\nsym_output <- internals$get.output(which(internals$outputs %in% \"loss_output\"))\nsymbol <- mx.symbol.Group(sym_output, sym_state, sym_state_cell)\n\ninfer_raw <- c(\"Thou \")\ninfer_split <- dic[strsplit(infer_raw, '') %>% unlist]\ninfer_length <- length(infer_split)\n\ninfer.data <- mx.io.arrayiter(data = matrix(infer_split), label = matrix(infer_split),  \n                              batch.size = 1, shuffle = FALSE)\n\ninfer <- mx.infer.rnn.one(infer.data = infer.data, \n                          symbol = symbol,\n                          arg.params = model$arg.params,\n                          aux.params = model$aux.params,\n                          input.params = NULL, \n                          ctx = devices)\n\npred_prob <- as.numeric(as.array(mx.nd.slice.axis(\n    infer$loss_output, axis = 0, begin = infer_length-1, end = infer_length)))\npred <- sample(length(pred_prob), prob = pred_prob, size = 1) - 1\npredict <- c(predict, pred)\n\nfor (i in 1:200) {\n  \n  infer.data <- mx.io.arrayiter(data = as.matrix(pred), label = as.matrix(pred),  \n                                batch.size = 1, shuffle = FALSE)\n  \n  infer <- mx.infer.rnn.one(infer.data = infer.data, \n                            symbol = symbol,\n                            arg.params = model$arg.params,\n                            aux.params = model$aux.params,\n                            input.params = list(rnn.state = infer[[2]], \n                                                rnn.state.cell = infer[[3]]), \n                            ctx = devices)\n  \n  pred_prob <- as.numeric(as.array(infer$loss_output))\n  pred <- sample(length(pred_prob), prob = pred_prob, size = 1, replace = T) - 1\n  predict <- c(predict, pred)\n}\n\npredict_txt <- paste0(rev_dic[as.character(predict)], collapse = \"\")\npredict_txt_tot <- paste0(infer_raw, predict_txt, collapse = \"\")\nprint(predict_txt_tot)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\n    [1] \"Thou NAknowledge thee my Comfort and his late she.FRIAR LAURENCE:Nothing a groats waterd forth. The lend he thank that;When she I am brother draw London: and not hear that know.BENVOLIO:How along, makes your \"\n\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n\n", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "r"}, "nbformat_minor": 2}