{"nbformat": 4, "cells": [{"source": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n# Text Classification Using a Convolutional Neural Network on MXNet\n\nThis tutorial is based of Yoon Kim's [paper](https://arxiv.org/abs/1408.5882) on using convolutional neural networks for sentence sentiment classification. The tutorial has been tested on MXNet 1.0 running under Python 2.7 and Python 3.6.\n\nFor this tutorial, we will train a convolutional deep network model on movie review sentences from Rotten Tomatoes labeled with their sentiment. The result will be a model that can classify a sentence based on its sentiment (with 1 being a purely positive sentiment, 0 being a purely negative sentiment and 0.5 being neutral).\n\nOur first step will be to fetch the labeled training data of positive and negative sentiment sentences and process it into sets of vectors that are then randomly split into train and test sets.", "cell_type": "markdown", "metadata": {}}, {"source": "from __future__ import print_function\n\nfrom collections import Counter\nimport itertools\nimport numpy as np\nimport re\n\ntry:\n    # For Python 3.0 and later\n    from urllib.request import urlopen\nexcept ImportError:\n    # Fall back to Python 2's urllib2\n    from urllib2 import urlopen\n    \ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning.\n    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    \n    return string.strip().lower()\n\ndef download_sentences(url):\n    \"\"\"\n    Download sentences from specified URL. \n    \n    Strip trailing newline, convert to Unicode.\n    \"\"\"\n    \n    remote_file = urlopen(url)\n    return [line.decode('Latin1').strip() for line in remote_file.readlines()]\n    \ndef load_data_and_labels():\n    \"\"\"\n    Loads polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    \"\"\"\n\n    positive_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos')\n    negative_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg')\n    \n    # Tokenize\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent).split(\" \") for sent in x_text]\n\n    # Generate labels\n    positive_labels = [1 for _ in positive_examples]\n    negative_labels = [0 for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return x_text, y\n\n\ndef pad_sentences(sentences, padding_word=\"</s>\"):\n    \"\"\"\n    Pads all sentences to be the length of the longest sentence.\n    Returns padded sentences.\n    \"\"\"\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n        \n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    \"\"\"\n    Builds a vocabulary mapping from token to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    \"\"\"\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    \n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    \n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    \n    return vocabulary, vocabulary_inv\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    \"\"\"\n    Maps sentences and labels to vectors based on a vocabulary.\n    \"\"\"\n    x = np.array([\n            [vocabulary[word] for word in sentence]\n            for sentence in sentences])\n    y = np.array(labels)\n    \n    return x, y\n\n\"\"\"\nLoads and preprocesses data for the MR dataset.\nReturns input vectors, labels, vocabulary, and inverse vocabulary.\n\"\"\"\n# Load and preprocess data\nsentences, labels = load_data_and_labels()\nsentences_padded = pad_sentences(sentences)\nvocabulary, vocabulary_inv = build_vocab(sentences_padded)\nx, y = build_input_data(sentences_padded, labels, vocabulary)\n\nvocab_size = len(vocabulary)\n\n# randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(y)))\nx_shuffled = x[shuffle_indices]\ny_shuffled = y[shuffle_indices]\n\n# split train/dev set\n# there are a total of 10662 labeled examples to train on\nx_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\ny_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]\n\nsentence_size = x_train.shape[1]\n\nprint('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))\nprint('train shape:', x_train.shape)\nprint('dev shape:', x_dev.shape)\nprint('vocab_size', vocab_size)\nprint('sentence max words', sentence_size)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Now that we prepared the training and test data by loading, vectorizing and shuffling it we can go on to defining the network architecture we want to train with the data.\n\nWe will first set up some placeholders for the input and output of the network then define the first layer, an embedding layer, which learns to map word vectors into a lower dimensional vector space where distances between words correspond to how related they are (with respect to sentiment they convey).", "cell_type": "markdown", "metadata": {}}, {"source": "import mxnet as mx\nimport sys,os\n\n'''\nDefine batch size and the place holders for network inputs and outputs\n'''\n\nbatch_size = 50\nprint('batch size', batch_size)\n\ninput_x = mx.sym.Variable('data') # placeholder for input data\ninput_y = mx.sym.Variable('softmax_label') # placeholder for output label\n\n\n'''\nDefine the first network layer (embedding)\n'''\n\n# create embedding layer to learn representation of words in a lower dimensional subspace (much like word2vec)\nnum_embed = 300 # dimensions to embed words into\nprint('embedding dimensions', num_embed)\n\nembed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n\n# reshape embedded data for next layer\nconv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "The next layer in the network performs convolutions over the ordered embedded word vectors in a sentence using multiple filter sizes, sliding over 3, 4 or 5 words at a time. This is the equivalent of looking at all 3-grams, 4-grams and 5-grams in a sentence and will allow us to understand how words contribute to sentiment in the context of those around them.\n\nAfter each convolution, we add a max-pool layer to extract the most significant elements in each convolution and turn them into a feature vector.\n\nBecause each convolution+pool filter produces tensors of different shapes we need to create a layer for each of them, and then concatenate the results of these layers into one big feature vector.", "cell_type": "markdown", "metadata": {}}, {"source": "# create convolution + (max) pooling layer for each filter operation\nfilter_list=[3, 4, 5] # the size of filters to use\nprint('convolution filters', filter_list)\n\nnum_filter=100\npooled_outputs = []\nfor filter_size in filter_list:\n    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n    relui = mx.sym.Activation(data=convi, act_type='relu')\n    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n    pooled_outputs.append(pooli)\n\n# combine all pooled outputs\ntotal_filters = num_filter * len(filter_list)\nconcat = mx.sym.Concat(*pooled_outputs, dim=1)\n\n# reshape for next layer\nh_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Next, we add dropout regularization, which will randomly disable a fraction of neurons in the layer (set to 50% here) to ensure that that model does not overfit. This prevents neurons from co-adapting and forces them to learn individually useful features.\n\nThis is necessary for our model because the dataset has a vocabulary of size around 20k and only around 10k examples so since this data set is pretty small we're likely to overfit with a powerful model (like this neural net).", "cell_type": "markdown", "metadata": {}}, {"source": "# dropout layer\ndropout = 0.5\nprint('dropout probability', dropout)\n\nif dropout > 0.0:\n    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\nelse:\n    h_drop = h_pool", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Finally, we add a fully connected layer to add non-linearity to the model. We then classify the resulting output of this layer using a softmax function, yielding a result between 0 (negative sentiment) and 1 (positive).", "cell_type": "markdown", "metadata": {}}, {"source": "# fully connected layer\nnum_label = 2\n\ncls_weight = mx.sym.Variable('cls_weight')\ncls_bias = mx.sym.Variable('cls_bias')\n\nfc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n\n# softmax output\nsm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n\n# set CNN pointer to the \"back\" of the network\ncnn = sm", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Now that we have defined our CNN model we will define the device on our machine that we will train and execute this model on, as well as the datasets to train and test this model with.\n\n*If you are running this code be sure that you have a GPU on your machine if your ctx is set to mx.gpu(0) otherwise you can set your ctx to mx.cpu(0) which will run the training much slower*", "cell_type": "markdown", "metadata": {}}, {"source": "from collections import namedtuple\nimport math\nimport time\n\n# Define the structure of our CNN Model (as a named tuple)\nCNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n\n# Define what device to train/test on, use GPU if available\nctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n\narg_names = cnn.list_arguments()\n\ninput_shapes = {}\ninput_shapes['data'] = (batch_size, sentence_size)\n\narg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\narg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\nargs_grad = {}\nfor shape, name in zip(arg_shape, arg_names):\n    if name in ['softmax_label', 'data']: # input, output\n        continue\n    args_grad[name] = mx.nd.zeros(shape, ctx)\n\ncnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n\nparam_blocks = []\narg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\ninitializer = mx.initializer.Uniform(0.1)\nfor i, name in enumerate(arg_names):\n    if name in ['softmax_label', 'data']: # input, output\n        continue\n    initializer(mx.init.InitDesc(name), arg_dict[name])\n\n    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n\ndata = cnn_exec.arg_dict['data']\nlabel = cnn_exec.arg_dict['softmax_label']\n\ncnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We can now execute the training and testing of our network, which in-part mxnet automatically does for us with its forward and backward propagation methods, along with its automatic gradient calculations.", "cell_type": "markdown", "metadata": {}}, {"source": "'''\nTrain the cnn_model using back prop\n'''\n\noptimizer = 'rmsprop'\nmax_grad_norm = 5.0\nlearning_rate = 0.0005\nepoch = 50\n\nprint('optimizer', optimizer)\nprint('maximum gradient', max_grad_norm)\nprint('learning rate (step size)', learning_rate)\nprint('epochs to train for', epoch)\n\n# create optimizer\nopt = mx.optimizer.create(optimizer)\nopt.lr = learning_rate\n\nupdater = mx.optimizer.get_updater(opt)\n\n# For each training epoch\nfor iteration in range(epoch):\n    tic = time.time()\n    num_correct = 0\n    num_total = 0\n\n    # Over each batch of training data\n    for begin in range(0, x_train.shape[0], batch_size):\n        batchX = x_train[begin:begin+batch_size]\n        batchY = y_train[begin:begin+batch_size]\n        if batchX.shape[0] != batch_size:\n            continue\n\n        cnn_model.data[:] = batchX\n        cnn_model.label[:] = batchY\n\n        # forward\n        cnn_model.cnn_exec.forward(is_train=True)\n\n        # backward\n        cnn_model.cnn_exec.backward()\n\n        # eval on training data\n        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n        num_total += len(batchY)\n\n        # update weights\n        norm = 0\n        for idx, weight, grad, name in cnn_model.param_blocks:\n            grad /= batch_size\n            l2_norm = mx.nd.norm(grad).asscalar()\n            norm += l2_norm * l2_norm\n\n        norm = math.sqrt(norm)\n        for idx, weight, grad, name in cnn_model.param_blocks:\n            if norm > max_grad_norm:\n                grad *= (max_grad_norm / norm)\n\n            updater(idx, grad, weight)\n\n            # reset gradient to zero\n            grad[:] = 0.0\n\n    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n    if iteration % 50 == 0 and iteration > 0:\n        opt.lr *= 0.5\n        print('reset learning rate to %g' % opt.lr)\n\n    # End of training loop for this epoch\n    toc = time.time()\n    train_time = toc - tic\n    train_acc = num_correct * 100 / float(num_total)\n\n    # Saving checkpoint to disk\n    if (iteration + 1) % 10 == 0:\n        prefix = 'cnn'\n        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n        param_name = './%s-%04d.params' % (prefix, iteration)\n        mx.nd.save(param_name, save_dict)\n        print('Saved checkpoint to %s' % param_name)\n\n\n    # Evaluate model after this epoch on dev (test) set\n    num_correct = 0\n    num_total = 0\n\n    # For each test batch\n    for begin in range(0, x_dev.shape[0], batch_size):\n        batchX = x_dev[begin:begin+batch_size]\n        batchY = y_dev[begin:begin+batch_size]\n\n        if batchX.shape[0] != batch_size:\n            continue\n\n        cnn_model.data[:] = batchX\n        cnn_model.cnn_exec.forward(is_train=False)\n\n        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n        num_total += len(batchY)\n\n    dev_acc = num_correct * 100 / float(num_total)\n    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\n\n    optimizer rmsprop\n    maximum gradient 5.0\n    learning rate (step size) 0.0005\n    epochs to train for 50\n    Iter [0] Train: Time: 3.903s, Training Accuracy: 56.290             --- Dev Accuracy thus far: 63.300\n    ...\n    Iter [8] Train: Time: 3.144s, Training Accuracy: 98.425             --- Dev Accuracy thus far: 78.000\n    Saved checkpoint to ./cnn-0009.params\n    ...\n\nNow that we have gone through the trouble of training the model, we have stored the learned parameters in the .params file in our local directory. We can now load this file whenever we want and predict the sentiment of new sentences by running them through a forward pass of the trained model.\n\n## References\n- [\"Implementing a CNN for Text Classification in TensorFlow\" blog post](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n- [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n\n## Next Steps\n* [MXNet tutorials index](http://mxnet.io/tutorials/index.html)\n\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n\n", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "python"}, "nbformat_minor": 2}