{"nbformat": 4, "cells": [{"source": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n# Converting Module API code to the Gluon API\n\nSometimes you find yourself in the situation where the model you want to use has been written using the symbolic Module API rather than the simpler, easier-to-debug, more flexible, imperative Gluon API. In this tutorial, we will give you a comprehensive guide for transforming Module code to Gluon code.\n\nThe different steps to take into consideration are:\n\nI) Data loading\n\nII) Model definition\n\nIII) Loss\n\nIV) Training Loop\n\nV) Exporting Models\n\nVI) Loading Models for Inference\n\nIn the following section we will look at 1:1 mappings between the Module and the Gluon ways of training a neural network.\n\n## I - Data Loading\n\nIn this section we will be looking at the difference in loading data between Module and Gluon.\nLet's first import a few Python modules.", "cell_type": "markdown", "metadata": {}}, {"source": "from collections import namedtuple\nimport logging\nlogging.basicConfig(level=logging.INFO)\nimport random\n\nimport numpy as np\nimport mxnet as mx\nfrom mxnet.gluon.data import ArrayDataset, DataLoader\nfrom mxnet.gluon import nn\nfrom mxnet import gluon\n\n# parameters\nbatch_size = 5\ndataset_length = 50\n\n# random seeds\nrandom.seed(1)\nnp.random.seed(1)\nmx.random.seed(1)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Module\n\nWhen using the Module API we use a [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter), in addition to the data itself, the [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter) contains information about the name of the input symbols. \n\nIn the Module API, `DataIter`s are responsible for both holding the data and iterating through it. Some `DataIter`s support multi-threading like the [`ImageRecordIter`](https://mxnet.incubator.apache.org/api/python/io/io.html#mxnet.io.ImageRecordIter), while other don't, such as the [`NDArrayIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=ndarrayiter#mxnet.io.NDArrayIter).\n\nLet's create some random data, following the same format as grayscale 28x28 images.", "cell_type": "markdown", "metadata": {}}, {"source": "train_data = np.random.rand(dataset_length, 28,28).astype('float32')\ntrain_label = np.random.randint(0, 10, (dataset_length,)).astype('float32')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We can now wraps this data into an ArrayIterator that will create batches of data using the first dimension of the provided array as the batch dimension. ", "cell_type": "markdown", "metadata": {}}, {"source": "data_iter = mx.io.NDArrayIter(data=train_data, label=train_label, batch_size=batch_size, shuffle=False, data_name='data', label_name='softmax_label')\nfor batch in data_iter:\n    print(batch.data[0].shape, batch.label[0])\n    break", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "    (5, 28, 28) \n    [5. 0. 3. 4. 9.]\n    <NDArray 5 @cpu(0)>\n\n\n#### Gluon\n\nWith Gluon, the preferred method is to use a [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) that makes use of a [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) to asynchronously prefetch the data. \n\nThe Gluon API offers you the ability to efficiently fetch data and separate the concerns of loading versus holding data. The DataLoader role is to request certain indices of the dataset. The Dataset has ownership of the data.\nThe `Dataset` data can be in or out of memory, and the `DataLoader` role is to request certain indices of the dataset, in the main thread or through multi-processing (or multi-threaded) workers and batch the data together. ", "cell_type": "markdown", "metadata": {}}, {"source": "dataset = ArrayDataset(train_data, train_label)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\nfor data, label in dataloader:\n    print(data.shape, label)\n    break", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "    (5, 28, 28) \n    [5. 0. 3. 4. 9.]\n    <NDArray 5 @cpu(0)>\n\nYou can check the [`Dataset` and `DataLoader` tutorials](https://mxnet.incubator.apache.org/tutorials/gluon/datasets.html) out. You can either rewrite your code in order to use one of the provided [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) class, like the [`ArrayDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=arraydataset#mxnet.gluon.data.ArrayDataset) or the [`ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset)\n\n\n## II - Model Definition\n\nLet's look at the model definition from the [MNIST Module Tutorial](https://mxnet.incubator.apache.org/tutorials/python/mnist.html):\n\n#### Module\n\nFor the Module API, you define the data flow by setting `data` keyword argument of one layer to the next.\nYou then bind the symbolic model to a specific compute context and specify the symbol names for the data and the label.", "cell_type": "markdown", "metadata": {}}, {"source": "# context\nctx = mx.cpu()\n\ndef get_module_network():\n    data = mx.sym.var('data')\n    data = mx.sym.flatten(data=data)\n    fc1  = mx.sym.FullyConnected(data=data, num_hidden=128)\n    act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")\n    fc2  = mx.sym.FullyConnected(data=act1, num_hidden = 64)\n    act2 = mx.sym.Activation(data=fc2, act_type=\"relu\")\n    fc3  = mx.sym.FullyConnected(data=act2, num_hidden=10)\n    mlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')\n    return mlp\n\nmlp = get_module_network()\n# Bind model to Module\nmlp_model = mx.mod.Module(symbol=mlp, context=ctx, data_names=['data'], label_names=['softmax_label'])", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon\n\nIn Gluon, for the equivalent model, you would create a `Sequential` block, in that case a `HybridSequential` block to allow for future hybridization since we are only using [hybridizable blocks](https://mxnet.incubator.apache.org/tutorials/gluon/hybrid.html). The flow of the data will be automatically set from one layer to the next, since they are held in a `Sequential` block.\nNote that we don't need named symbols for the input, and we show how the loss is handled in Gluon in the next section.", "cell_type": "markdown", "metadata": {}}, {"source": "def get_gluon_network():\n    net = nn.HybridSequential()\n    with net.name_scope():\n        net.add(\n            nn.Flatten(),\n            nn.Dense(units=128, activation=\"relu\"),\n            nn.Dense(units=64, activation=\"relu\"),\n            nn.Dense(units=10)\n        )\n    return net\n\nnet = get_gluon_network()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## III - Loss\n\nThe loss, that you are trying to minimize using an optimization algorithm like SGD, is defined differently in the Module API than in Gluon.\n\n\n#### Module\n\n\nIn the module API, the loss is part of the network. It has usually a forward pass result, that is the inference value, and a backward pass that is the gradient of the output with respect to that particular loss.\n\nFor example, the [sym.SoftmaxOutput](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html?highlight=softmaxout#mxnet.symbol.SoftmaxOutput) is a softmax output in the forward pass and the gradient with respect to the cross-entropy loss in the backward pass.", "cell_type": "markdown", "metadata": {}}, {"source": "# Softmax with cross entropy loss, directly part of the network\nout = mx.sym.SoftmaxOutput(data=mlp, name='softmax')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon\n\n\nIn Gluon, it is a lot more transparent. Losses, like the [SoftmaxCrossEntropyLoss](https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=softmaxcross#mxnet.gluon.loss.SoftmaxCrossEntropyLoss), are only computing the actual value of the loss. You then call `.backward()` on the loss value to compute the gradient of the parameters with respect to that loss. At inference time, you simply call `.softmax()` on your output to get the output of your network normalized according to the softmax function.", "cell_type": "markdown", "metadata": {}}, {"source": "# We simply create a loss function we will use in our training loop\nloss_fn = gluon.loss.SoftmaxCrossEntropyLoss()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "In the next section we will show how you use this loss function in Gluon to generate the loss value in the main training loop.\n\n## IV - Training Loop\n\n\n#### Module\n\nThe Module API provides a [`.fit()`](https://mxnet.incubator.apache.org/api/python/module/module.html?highlight=.fit#mxnet.module.BaseModule.fit) function that takes care of fitting training data to your symbolic model. With Gluon, your execution flow controls the data flow, so you need to write your own loop. It might seems like it is more verbose, but you have a lot more control as to what is happening during the training. \nWith the [`.fit()`](https://mxnet.incubator.apache.org/api/python/module/module.html?highlight=.fit#mxnet.module.BaseModule.fit) function, you control the metric reporting, checkpointing or weights initialization through a lot of different keyword arguments (check the [docs](https://mxnet.incubator.apache.org/api/python/module/module.html?highlight=.fit#mxnet.module.BaseModule.fit)). That is where you define the optimizer for example.", "cell_type": "markdown", "metadata": {}}, {"source": "mlp_model.fit(data_iter,  # train data\n              eval_data=data_iter,  # validation data\n              optimizer='sgd',  # use SGD to train\n              force_init=True,\n              force_rebind=True,\n              optimizer_params={'learning_rate':0.1},  # use fixed learning rate\n              eval_metric='acc',  # report accuracy during training\n              num_epoch=5)  # train for 5 full dataset passes", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon\n\n\nWith Gluon, you do these operations directly in the training loop, and the optimizer is part of the [`Trainer`](https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer) object that handles the weight updates of your parameters.\n\nNotice the `loss.backward()` we call before updating the weight as mentionned in the previous section", "cell_type": "markdown", "metadata": {}}, {"source": "net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) # Initialize network and trainer\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n\nmetric = mx.metric.Accuracy() # Pick a metric\n\nfor e in range(5): # start of epoch\n    \n    for data, label in dataloader: # start of mini-batch\n        data = data.as_in_context(ctx)\n        label = label.as_in_context(ctx)\n        \n        with mx.autograd.record():\n            output = net(data) # forward pass\n            loss = loss_fn(output, label) # get loss\n            \n        loss.backward() # compute gradients\n        trainer.step(data.shape[0]) # update weights with SGD\n        metric.update(label, output) # update the metrics # end of mini-batch\n\n    name, acc = metric.get()\n    print('training metrics at epoch %d: %s=%f'%(e, name, acc))\n    metric.reset() # end of epoch", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "The Gluon training code is more verbose than the simple `.fit` from Module. However that is also the main advantage, there is no black magic going on here, you have full control of your training loop. You can for example easily set breakpoints, modify a learning rate or print data during the training flow. This flexibility also makes easy to implement more complex use-case like gradient accumulation across batches.\n\n## V - Exporting Model\n\nThe ultimate purpose of training a model is to be able to export it and share it, whether it is for deployment or simply reproducibility purposes.\n\n#### Module\n\n\nWith the Module API, you can save model using the [`.save_checkpoint()`](https://mxnet.incubator.apache.org/api/python/module/module.html?highlight=save_chec#mxnet.module.Module.save_checkpoint) and get a `-symbol.json` and a `.params` file that represent your network. ", "cell_type": "markdown", "metadata": {}}, {"source": "mlp_model.save_checkpoint('module-model', epoch=5)\n# module-model-0005.params module-model-symbol.json", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon\n\n\n\nWith Gluon, network parameters are associated with a `Block`, but the execution flow is controlled in python through the code in `.forward()` function. Hence only [hybridized networks]() can be exported with a `-symbol.json` and `.params` file using [`.export()`](https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=export#mxnet.gluon.HybridBlock.export), non-hybridized models can only have their parameters exported using [`.save_parameters()`](https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=save_pa#mxnet.gluon.Block.save_parameters). Check this great tutorial to learn more: [Saving and Loading Gluon Models](https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html).\n\n\nAny models:", "cell_type": "markdown", "metadata": {}}, {"source": "# save only the parameters\nnet.save_parameters('gluon-model.params')\n# gluon-model.params", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Hybridized models:", "cell_type": "markdown", "metadata": {}}, {"source": "# save the parameters and the symbolic representation\nnet.hybridize()\nnet(mx.nd.ones((1,1,28,28), ctx))\n\nnet.export('gluon-model-hybrid', epoch=5)\n# gluon-model-hybrid-symbol.json gluon-model-hybrid-0005.params", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## VI - Loading Model for Inference\n\n\n#### Module\n\n\nFor inference, in the Module API, you need to first load the parameters and symbol, bind the symbol to a module and load the corresponding parameters. You can then pass a batch of data through that module and request the output of the network.", "cell_type": "markdown", "metadata": {}}, {"source": "# Load the symbol and parameters\nsym, arg_params, aux_params = mx.model.load_checkpoint('module-model', 5)\n\n# Bind them in a module\nmod = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\nmod.bind(for_training=False, data_shapes=[('data', (1,1,28,28))], \n         label_shapes=mod._label_shapes)\n\n# Set the parameters\nmod.set_params(arg_params, aux_params, allow_missing=True)\n\n# Run the inference\nBatch = namedtuple('Batch', ['data'])\nmod.forward(Batch([mx.nd.ones((1,28,28))]))\nprob = mod.get_outputs()[0].asnumpy()\nprint(\"Output probabilities: {}\".format(prob))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon (Symbolic Model)\n\nFor the Gluon API, it is a lot simpler. You can just load a serialized model in a [`SymbolBlock`](https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=symbolblo#mxnet.gluon.SymbolBlock) and run inference directly.", "cell_type": "markdown", "metadata": {}}, {"source": "import warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    net = gluon.SymbolBlock.imports('module-model-symbol.json', ['data', 'softmax_label'], 'module-model-0005.params')\nprob = net(mx.nd.ones((1,1,28,28)), mx.nd.ones(1)) # note the second argument here to account for the softmax_label symbol\nprint(\"Output probabilities: {}\".format(prob.asnumpy()))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "#### Gluon (Imperative Model)", "cell_type": "markdown", "metadata": {}}, {"source": "net = get_gluon_network()\nnet.load_parameters('gluon-model.params')\nprob = net(mx.nd.ones((1,1,28,28))).softmax()\nprint(\"Output probabilities: {}\".format(prob.asnumpy()))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\n\n## Conclusion\n\nThis tutorial lead you through the steps necessary to train a deep learning model and showed you the differences between the symbolic approach of the Module API and the imperative one of the Gluon API. If you need more help converting your Module API code to the Gluon API, reach out to the community on the [discuss forum](https://discuss.mxnet.io)!\nYou can also compare the scripts for training MNIST in [Gluon](https://mxnet.incubator.apache.org/tutorials/gluon/mnist.html) and [Module](https://mxnet.incubator.apache.org/tutorials/python/mnist.html).\n\n\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n\n", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "python"}, "nbformat_minor": 2}