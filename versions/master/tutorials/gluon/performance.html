<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Gluon Performance Tips &amp; Tricks" property="og:title">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image:secure_url">
<meta content="Gluon Performance Tips &amp; Tricks" property="og:description"/>
<title>Gluon Performance Tips &amp; Tricks — mxnet  documentation</title>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" rel="stylesheet"/>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../_static/basic.css" rel="stylesheet" type="text/css">
<link href="../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../_static/mxnet.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
<script src="https://code.jquery.com/jquery-1.11.1.min.js" type="text/javascript"></script>
<script src="../../_static/underscore.js" type="text/javascript"></script>
<script src="../../_static/searchtools_custom.js" type="text/javascript"></script>
<script src="../../_static/doctools.js" type="text/javascript"></script>
<script src="../../_static/selectlang.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript"> jQuery(function() { Search.loadIndex("/searchindex.js"); Search.init();}); </script>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
      Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-96378503-1', 'auto');
      ga('send', 'pageview');

    </script>
<!-- -->
<!-- <script type="text/javascript" src="../../_static/jquery.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/underscore.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/doctools.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<!-- -->
<link href="../../genindex.html" rel="index" title="Index">
<link href="../../search.html" rel="search" title="Search"/>
<link href="index.html" rel="up" title="Tutorials"/>
<link href="pretrained_models.html" rel="next" title="Using pre-trained models in MXNet"/>
<link href="ndarray.html" rel="prev" title="NDArray - Scientific computing on CPU and GPU"/>
<link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"/>
</link></link></link></meta></meta></meta></head>
<body background="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-background-compressed.jpeg" role="document">
<div class="content-block"><div class="navbar navbar-fixed-top">
<div class="container" id="navContainer">
<div class="innder" id="header-inner">
<h1 id="logo-wrap">
<a href="../../" id="logo"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo.png"/></a>
</h1>
<nav class="nav-bar" id="main-nav">
<a class="main-nav-link" href="../../install/index.html">Install</a>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Gluon <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="../../gluon/index.html">About</a></li>
<li><a class="main-nav-link" href="https://www.d2l.ai/">Dive into Deep Learning</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">API <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="../../api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="../../api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="../../api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="../../api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="../../api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="../../api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="../../api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="../../api/scala/index.html">Scala</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-docs">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Docs <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-docs">
<li><a class="main-nav-link" href="../../faq/index.html">FAQ</a></li>
<li><a class="main-nav-link" href="../../tutorials/index.html">Tutorials</a>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/master/example">Examples</a></li>
<li><a class="main-nav-link" href="../../architecture/index.html">Architecture</a></li>
<li><a class="main-nav-link" href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
<li><a class="main-nav-link" href="../../api/python/gluon/model_zoo.html">Model Zoo</a></li>
<li><a class="main-nav-link" href="../../api/python/contrib/onnx.html">ONNX</a></li>
</li></ul>
</span>
<span id="dropdown-menu-position-anchor-community">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Community <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-community">
<li><a class="main-nav-link" href="http://discuss.mxnet.io">Forum</a></li>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet">Github</a></li>
<li><a class="main-nav-link" href="../../community/contribute.html">Contribute</a></li>
<li><a class="main-nav-link" href="../../community/ecosystem.html">Ecosystem</a></li>
<li><a class="main-nav-link" href="../../community/powered_by.html">Powered By</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-version" style="position: relative"><a href="#" class="main-nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="true">Versions(master)<span class="caret"></span></a><ul id="package-dropdown-menu" class="dropdown-menu"><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.5.0/index.html>1.5.0</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.4.1/index.html>1.4.1</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.3.1/index.html>1.3.1</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.2.1/index.html>1.2.1</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.1.0/index.html>1.1.0</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/1.0.0/index.html>1.0.0</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/0.12.1/index.html>0.12.1</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/0.11.0/index.html>0.11.0</a></li><li><a class="main-nav-link" href=http://mxnet.incubator.apache.org/versions/master/index.html>master</a></li></ul></span></nav>
<script> function getRootPath(){ return "../../" } </script>
<div class="burgerIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button">☰</a>
<ul class="dropdown-menu" id="burgerMenu">
<li><a href="../../install/index.html">Install</a></li>
<li><a class="main-nav-link" href="../../tutorials/index.html">Tutorials</a></li>
<li class="dropdown-submenu dropdown">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Gluon</a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="../../gluon/index.html">About</a></li>
<li><a class="main-nav-link" href="http://gluon.mxnet.io">The Straight Dope (Tutorials)</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">API</a>
<ul class="dropdown-menu">
<li><a class="main-nav-link" href="../../api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="../../api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="../../api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="../../api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="../../api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="../../api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="../../api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="../../api/scala/index.html">Scala</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Docs</a>
<ul class="dropdown-menu">
<li><a href="../../faq/index.html" tabindex="-1">FAQ</a></li>
<li><a href="../../tutorials/index.html" tabindex="-1">Tutorials</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/master/example" tabindex="-1">Examples</a></li>
<li><a href="../../architecture/index.html" tabindex="-1">Architecture</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home" tabindex="-1">Developer Wiki</a></li>
<li><a href="../../api/python/gluon/model_zoo.html" tabindex="-1">Gluon Model Zoo</a></li>
<li><a href="../../api/python/contrib/onnx.html" tabindex="-1">ONNX</a></li>
</ul>
</li>
<li class="dropdown-submenu dropdown">
<a aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" role="button" tabindex="-1">Community</a>
<ul class="dropdown-menu">
<li><a href="http://discuss.mxnet.io" tabindex="-1">Forum</a></li>
<li><a href="https://github.com/apache/incubator-mxnet" tabindex="-1">Github</a></li>
<li><a href="../../community/contribute.html" tabindex="-1">Contribute</a></li>
<li><a href="../../community/ecosystem.html" tabindex="-1">Ecosystem</a></li>
<li><a href="../../community/powered_by.html" tabindex="-1">Powered By</a></li>
</ul>
</li>
<li id="dropdown-menu-position-anchor-version-mobile" class="dropdown-submenu" style="position: relative"><a href="#" tabindex="-1">Versions(master)</a><ul class="dropdown-menu"><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.5.0/index.html>1.5.0</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.4.1/index.html>1.4.1</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.3.1/index.html>1.3.1</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.2.1/index.html>1.2.1</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.1.0/index.html>1.1.0</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/1.0.0/index.html>1.0.0</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/0.12.1/index.html>0.12.1</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/0.11.0/index.html>0.11.0</a></li><li><a tabindex="-1" href=http://mxnet.incubator.apache.org/versions/master/index.html>master</a></li></ul></li></ul>
</div>
<div class="plusIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button"><span aria-hidden="true" class="glyphicon glyphicon-plus"></span></a>
<ul class="dropdown-menu dropdown-menu-right" id="plusMenu"></ul>
</div>
<div id="search-input-wrap">
<form action="../../search.html" autocomplete="off" class="" method="get" role="search">
<div class="form-group inner-addon left-addon">
<i class="glyphicon glyphicon-search"></i>
<input class="form-control" name="q" placeholder="Search" type="text"/>
</div>
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default"/>
</input></form>
<div id="search-preview"></div>
</div>
<div id="searchIcon">
<span aria-hidden="true" class="glyphicon glyphicon-search"></span>
</div>
<!-- <div id="lang-select-wrap"> -->
<!--   <label id="lang-select-label"> -->
<!--     <\!-- <i class="fa fa-globe"></i> -\-> -->
<!--     <span></span> -->
<!--   </label> -->
<!--   <select id="lang-select"> -->
<!--     <option value="en">Eng</option> -->
<!--     <option value="zh">中文</option> -->
<!--   </select> -->
<!-- </div> -->
<!--     <a id="mobile-nav-toggle">
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
      </a> -->
</div>
</div>
</div>
<script type="text/javascript">
        $('body').css('background', 'white');
    </script>
<div class="container">
<div class="row">
<div aria-label="main navigation" class="sphinxsidebar leftsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">MXNet APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/index.html">MXNet Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">MXNet Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">MXNet FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gluon/index.html">About Gluon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing MXNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#nvidia-jetson-devices">NVIDIA Jetson Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#source-download">Source Download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo/index.html">MXNet Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Tutorials</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="page-tracker"></div>
<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at --><!---   http://www.apache.org/licenses/LICENSE-2.0 --><!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. --><div class="section" id="gluon-performance-tips-tricks">
<span id="gluon-performance-tips-tricks"></span><h1>Gluon Performance Tips &amp; Tricks<a class="headerlink" href="#gluon-performance-tips-tricks" title="Permalink to this headline">¶</a></h1>
<p>Compared to traditional machine learning methods, the field of deep-learning has increased model accuracy across a wide range of tasks, but it has also increased the amount of computation required for model training and inference. Specialised hardware chips, such as GPUs and FPGAs, can speed up the execution of networks, but it can sometimes be hard to write code that uses the hardware to its full potential. We will be looking at a few simple tips and trick in this tutorial that you can use to speed up training and ultimately save on training costs. You’ll find most of these tips and tricks useful for inference too.</p>
<p>We’ll start by writing some code to train an image classification network for the CIFAR-10 dataset, and then benchmark the throughput of the network in terms of samples processed per second. After some performance analysis, we’ll identify the bottlenecks (i.e. the components limiting throughput) and improve the training speed step-by-step. We’ll bring together all the tips and tricks at the end and evaluate our performance gains.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>An Amazon EC2 p3.2xlarge instance was used to benchmark the code in this tutorial. You are likely to get different results and find different bottlenecks on other hardware, but these tips and tricks should still help improve training speed for bottleneck components. A GPU is recommended for this tutorial.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ctx</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">mx</span><span class="o">.</span><span class="n">test_utils</span><span class="o">.</span><span class="n">list_gpus</span><span class="p">()</span> <span class="k">else</span> <span class="n">mx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Using {} context."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="n">context</span><span class="o">.</span>
</pre></div>
</div>
<p>We’ll use the <code class="docutils literal"><span class="pre">CIFAR10</span></code> dataset provided out-of-the-box with Gluon.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'{} samples'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">50000</span> <span class="n">samples</span>
</pre></div>
</div>
<p>So we can learn how to identify training bottlenecks, let’s intentionally introduce a bottleneck by adding a short <code class="docutils literal"><span class="pre">sleep</span></code> into the data loading pipeline. We transform each 32x32 CIFAR-10 image to 224x224 so we can use it with the ResNet-50 network designed for ImageNet. <a class="reference external" href="https://gluon-cv.mxnet.io/api/model_zoo.html#gluoncv.model_zoo.get_cifar_resnet">CIFAR-10 specific ResNet networks</a> exist but we use the more standard ImageNet variants in this example.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transform_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># artificial slow-down</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">imresize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">transform_first</span><span class="p">(</span><span class="n">transform_fn</span><span class="p">)</span>
</pre></div>
</div>
<p>Setting our batch size to 16, we can create the <code class="docutils literal"><span class="pre">DataLoader</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                      <span class="n">last_batch</span><span class="o">=</span><span class="s2">"discard"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'{} batches'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">3125</span> <span class="n">batches</span>
</pre></div>
</div>
<p>Up next, we create all of the other components required for training, such as the network, the loss function, the evaluation metric and parameter trainer.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">resnet50_v2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">magnitude</span><span class="o">=</span><span class="mf">2.3</span><span class="p">),</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">'sgd'</span><span class="p">,</span> <span class="p">{</span><span class="s1">'learning_rate'</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">})</span>
</pre></div>
</div>
<div class="section" id="initial-benchmark">
<span id="initial-benchmark"></span><h2>Initial Benchmark<a class="headerlink" href="#initial-benchmark" title="Permalink to this headline">¶</a></h2>
<p>As a starting point, let’s benchmark the throughput of our training loop: calculating the average samples per second across 25 iterations, where each iteration is a batch of 16 samples. We’ll run a single forward pass through the network before starting our benchmark timer to avoid including shape inference and lazy initialization in the throughput calculations.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">single_forward</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">):</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">wait_to_read</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">single_forward</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">iter_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_iters</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">mx</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'.'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_iters</span> <span class="o">>=</span> <span class="n">iters</span><span class="p">:</span>
        <span class="k">break</span>
<span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">waitall</span><span class="p">()</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'average iterations/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'average samples/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_samples</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">.........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">4.4936</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">71.8975</span>
</pre></div>
</div>
<p>Although ~70 samples per second might sound respectable, let’s see if we can do any better by identifying the bottleneck in the training loop and optimizing that component. A significant amount of time can be wasted by optimizing components that aren’t bottlenecks.</p>
</div>
<div class="section" id="identifying-the-bottleneck">
<span id="identifying-the-bottleneck"></span><h2>Identifying the bottleneck<a class="headerlink" href="#identifying-the-bottleneck" title="Permalink to this headline">¶</a></h2>
<p>Monitoring the CPU (with <code class="docutils literal"><span class="pre">top</span></code>) and GPU utilization (with <code class="docutils literal"><span class="pre">nvidia-smi</span></code>) provide clues as to where potential bottlenecks lie. With the example above, when simultaneously running these monitoring tool, you might spot a single process on the CPU fixed at ~100% utilization while the GPU utilization behaves erratically and often falls to ~0%. Seeing behaviour like can indicate the CPU is struggling to process data and the GPU is being starved of data.</p>
<p>MXNet’s Profiler is another highly recommended tool for identifying bottlenecks, since it gives timing data for individual MXNet operations. Check out <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/python/profiler.html">this comprehensive tutorial</a> for more details. As a simpler form of analysis, we will split our training loop into two common components:</p>
<ol class="simple">
<li>Data Loading</li>
<li>Network Execution (forward and backward passes)</li>
</ol>
<p>We define two function to independently benchmark these components: <code class="docutils literal"><span class="pre">benchmark_dataloader</span></code> and <code class="docutils literal"><span class="pre">benchmark_network</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">startup_time</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">iter_idx</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">startup_time</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">startup_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_iters</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">num_iters</span> <span class="o">>=</span> <span class="n">iters</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'.'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">total_startup_time</span> <span class="o">=</span> <span class="n">startup_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">total_iter_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">startup_time</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'total startup time: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">total_startup_time</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'average iterations/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="n">total_iter_time</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'average samples/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_samples</span><span class="o">/</span><span class="n">total_iter_time</span><span class="p">))</span>
    
    
<span class="k">def</span> <span class="nf">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">waitall</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">iter_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_iters</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">with</span> <span class="n">mx</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">waitall</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num_iters</span> <span class="o">>=</span> <span class="n">iters</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'.'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'average iterations/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'average samples/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_samples</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
</pre></div>
</div>
<p>Our <code class="docutils literal"><span class="pre">benchmark_dataloader</span></code> function just loops through the <code class="docutils literal"><span class="pre">DataLoader</span></code> for a given number of iterations: it doesn’t transfer the data to the correct context or pass it to the network. Our <code class="docutils literal"><span class="pre">benchmark_network</span></code> function just performs a forward and backward pass on an identical (and pre-transferred) batch of data: it doesn’t require new data to be loaded. We’ll run both of these functions now.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_dataloader'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_network'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span> <span class="c1">### benchmark_dataloader </span>

<span class="o">........................</span>

<span class="n">total</span> <span class="n">startup</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.1697</span>
<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">6.2201</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">99.5217</span>

 <span class="c1">### benchmark_network </span>

<span class="o">........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">15.1908</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">243.0525</span>
</pre></div>
</div>
<p>Our data loading pipeline appears to be the bottleneck for training: ~100 samples/second compared with ~250 samples/second for network execution. One limiting factor could be disk throughput when reading samples (using a SSD instead of HDD can help with this), but in this case we intentionally added a delay in data transformation. Augmentation can often be a bottleneck in training if the following trick isn’t applied.</p>
</div>
<div class="section" id="tips-tricks-1-use-multiple-workers-on-dataloader">
<span id="tips-tricks-1-use-multiple-workers-on-dataloader"></span><h2>Tips &amp; Tricks #1: Use multiple workers on <code class="docutils literal"><span class="pre">DataLoader</span></code><a class="headerlink" href="#tips-tricks-1-use-multiple-workers-on-dataloader" title="Permalink to this headline">¶</a></h2>
<p>In the previous section, we established that the data loading component of the training loop was the bottleneck. Instead of simply removing the artificial delay, let’s assume it was some pre-processing or augmentation step that couldn’t be removed. We found that the CPU utilization was fixed at 100%, but this was just for a single core. Usually machines have multiple cores and with one easy trick we can leverage more CPU cores to pre-process the data. Setting <code class="docutils literal"><span class="pre">num_workers</span></code> on the <code class="docutils literal"><span class="pre">DataLoader</span></code> will result in multiple workers being used to preprocess the data. We can use <code class="docutils literal"><span class="pre">multiprocessing.cpu_count()</span></code> to find the number of CPU cores available on the machine, and we save 1 core for the main thread.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                      <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                      <span class="n">last_batch</span><span class="o">=</span><span class="s2">"discard"</span><span class="p">,</span>
                                      <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Using {} workers for DataLoader.'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_workers</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="mi">7</span> <span class="n">workers</span> <span class="k">for</span> <span class="n">DataLoader</span><span class="o">.</span>
</pre></div>
</div>
<p>We benchmark the two main components once again:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_dataloader'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_network'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span> <span class="c1">### benchmark_dataloader </span>

<span class="o">........................</span>

<span class="n">total</span> <span class="n">startup</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.1935</span>
<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">46.2375</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">739.7994</span>

 <span class="c1">### benchmark_network </span>

<span class="o">.........</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">14.8614</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">237.7819</span>
</pre></div>
</div>
<p>Our data loading pipeline is no longer the bottleneck for training throughput: ~700 samples per second versus ~250 samples per second for network execution as before. Check out <a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/examples/mxnet/gluon.html">NVIDIA DALI</a> if you need to futher optimize your data pipeline. We can now focus our attention on improving the network throughput.</p>
</div>
<div class="section" id="tips-tricks-2-hybridize-the-network">
<span id="tips-tricks-2-hybridize-the-network"></span><h2>Tips &amp; Tricks #2: Hybridize the network<a class="headerlink" href="#tips-tricks-2-hybridize-the-network" title="Permalink to this headline">¶</a></h2>
<p>Gluon networks run in imperative mode by default, executing <code class="docutils literal"><span class="pre">NDArray</span></code> operations as the lines of code are stepped through one by one. While in imperative mode, debugging is often simplified and more flexible networks can be defined (using Python control flow). But this comes at a slight cost in terms of throughput. Since the network doesn’t know what line of code will be run next, the network operations cannot be optimized and additional memory allocations are required (which all takes time). Most networks though can be written as <code class="docutils literal"><span class="pre">HybridBlocks</span></code> and, with the <code class="docutils literal"><span class="pre">hybridize</span></code> method, can be converted to symbolic mode execution. We can expect throughput to increase slightly in this mode. Watch out though: debugging can get more complicated. Setting <code class="docutils literal"><span class="pre">static_alloc=True</span></code> and <code class="docutils literal"><span class="pre">static_shape=True</span></code> reduce the number of memory allocations required while training. Once again, we run <code class="docutils literal"><span class="pre">single_forward</span></code> to force the hybridization process to occur before benchmarking.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">hybridize</span><span class="p">(</span><span class="n">static_alloc</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">static_shape</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">single_forward</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_dataloader'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_network'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span> <span class="c1">### benchmark_dataloader </span>

<span class="o">........................</span>

<span class="n">total</span> <span class="n">startup</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.1847</span>
<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">46.3004</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">740.8072</span>

 <span class="c1">### benchmark_network </span>

<span class="o">........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">16.5738</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">265.1812</span>
</pre></div>
</div>
<p>We can see quite a modest ~10% increase in throughput after hybridization. Gains can depend on a number of factors including the network architecture and the batch size used (a larger increase expected for smaller batch size). Our network execution is still the bottleneck in training so let’s focus on that again.</p>
</div>
<div class="section" id="tips-tricks-3-increase-the-batch-size">
<span id="tips-tricks-3-increase-the-batch-size"></span><h2>Tips &amp; Tricks #3: Increase the batch size<a class="headerlink" href="#tips-tricks-3-increase-the-batch-size" title="Permalink to this headline">¶</a></h2>
<p>GPUs are optimized for high throughput and they do this by performing many operations in parallel. Our NVIDIA Tesla V100 GPU utilization peaks at ~85% while running the last example. Although this is already quite high, there’s still room improvement given this metric shows the percentage of time <em>at least one</em> kernel is running (over the last 1 second by default). Given we have enough memory available, the throughput of the network can be improved by increasing the batch size since more samples are processed in parallel. At this stage we’re using approximately 1/4 of the available GPU memory, so let’s increase out batch size by a factor of 4, from 16 to 64. Changing the batch size does have some side effects though. Using the same optimizer with same hyperparameters often leads to slower convergence. More gradients, from more samples, are averaged which leads to a smaller variance in the batch gradient overall. One simple trick to mitigate this is to increase the learning rate by the same factor: so in this case, from 0.001 to 0.004.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="mi">4</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'batch_size: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="mi">4</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'learning_rate: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                            <span class="n">last_batch</span><span class="o">=</span><span class="s2">"discard"</span><span class="p">,</span>
                                            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">'sgd'</span><span class="p">,</span> <span class="p">{</span><span class="s1">'learning_rate'</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">})</span>
<span class="n">single_forward</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">batch_size</span><span class="p">:</span> <span class="mi">64</span>
<span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.004</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_dataloader'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_network'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span> <span class="c1">### benchmark_dataloader </span>

<span class="o">........................</span>

<span class="n">total</span> <span class="n">startup</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.7195</span>
<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">11.7149</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">749.7521</span>

 <span class="c1">### benchmark_network </span>

<span class="o">........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">5.5370</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">354.3710</span>
</pre></div>
</div>
<p>Once again, we see improvements in throughput. ~30% higher this time. Checking GPU memory usage, we still have room to increase the batch size higher than 64 (on NVIDIA Tesla V100). When the batch size starts to reach very large numbers (>512), simple tricks such as linear scaling of the learning rate might be insufficient for maintaining good convergence. Consider using a <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/learning_rate_schedules_advanced.html">warm-up learning rate schedule</a> and changing to specialized optimizers such as <a class="reference external" href="https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.LBSGD">LBSGD</a>.</p>
</div>
<div class="section" id="tips-tricks-4-using-mixed-precision-float32-and-float16">
<span id="tips-tricks-4-using-mixed-precision-float32-and-float16"></span><h2>Tips &amp; Tricks #4: Using Mixed-Precision (<code class="docutils literal"><span class="pre">float32</span></code> and <code class="docutils literal"><span class="pre">float16</span></code>)<a class="headerlink" href="#tips-tricks-4-using-mixed-precision-float32-and-float16" title="Permalink to this headline">¶</a></h2>
<p>Model execution is still our bottleneck, so let’s try out a new trick called <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/faq/float16.html">mixed precision</a> training. Some recent GPUs have cores that are optimized for ‘half-precision’ (i.e. <code class="docutils literal"><span class="pre">float16</span></code>) operations and they can be much faster than their ‘full-precision’ (i.e. <code class="docutils literal"><span class="pre">float32</span></code>) counterparts. Given all of the randomness already in neural network training, this reduction in precision doesn’t significantly impact the model accuracy in many cases. Convergence is slightly better when you keep the network parameters at full-precision but forward and backward passes can be performed at half-precision: hence the term ‘mixed-precision’. Also check out <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/amp/amp_tutorial.html">Automatic Mixed Precision</a> (AMP) for a more automated way of optimizing your network.</p>
<p>We need to <code class="docutils literal"><span class="pre">cast</span></code> the network to <code class="docutils literal"><span class="pre">'float16'</span></code>, configure our optimizer to use <code class="docutils literal"><span class="pre">multi_precision</span></code> and convert our input data types to <code class="docutils literal"><span class="pre">'float16'</span></code> too.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">'float16'</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="s1">'sgd'</span><span class="p">,</span> <span class="p">{</span><span class="s1">'learning_rate'</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
                                                         <span class="s1">'multi_precision'</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
<span class="n">single_forward</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float16'</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_dataloader'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">benchmark_dataloader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'### benchmark_network'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float16'</span><span class="p">)</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float16'</span><span class="p">)</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">benchmark_network</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span> <span class="c1">### benchmark_dataloader </span>

<span class="o">........................</span>

<span class="n">total</span> <span class="n">startup</span> <span class="n">time</span><span class="p">:</span> <span class="mf">0.7006</span>
<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">11.6537</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">745.8338</span>

 <span class="c1">### benchmark_network </span>

<span class="o">........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">11.3614</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">727.1298</span>
</pre></div>
</div>
<p>Overall we see a substantial increase in training throughput: double compared to full-precision training.</p>
</div>
<div class="section" id="tips-tricks-5-others">
<span id="tips-tricks-5-others"></span><h2>Tips &amp; Tricks #5: Others<a class="headerlink" href="#tips-tricks-5-others" title="Permalink to this headline">¶</a></h2>
<p>Many other tips and tricks exist for optimizing the throughput of training.</p>
<p>One area we didn’t explicitly benchmark in this tutorial is data transfer from CPU to GPU memory. Usually this isn’t an issue, but for very large arrays this can become a bottleneck too. You might be able to compress your data significantly before transferring if your data is sparse (i.e. mostly zero values). Check out the <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/index.html">sparse array tutorial</a> for more details and an example of how this can impact training speed.</p>
<p>Another useful trick if data pre-processing or data transfer is the bottleneck is pre-fetching batches. You can write your training loop to transfer the next batch of data to GPU before processing the current batch. Once again, this trick is memory permitting.</p>
<p>And finally, if you are an advanced user, check out the various <a class="reference external" href="https://mxnet.incubator.apache.org/faq/env_var.html">environment variables</a> that can be configured to change the behaviour of the MXNet backend.</p>
</div>
<div class="section" id="final-benchmark">
<span id="final-benchmark"></span><h2>Final Benchmark<a class="headerlink" href="#final-benchmark" title="Permalink to this headline">¶</a></h2>
<p>We will now combine all of the above tricks and tips in the complete training loop and compare to the initial benchmark.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">iters</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">iter_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">num_samples</span> <span class="o">+=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_iters</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float16'</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">as_in_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float16'</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">mx</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'.'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_iters</span> <span class="o">>=</span> <span class="n">iters</span><span class="p">:</span>
        <span class="k">break</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">total_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'average iterations/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'average samples/sec: {:.4f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_samples</span><span class="o">/</span><span class="n">total_time</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">.........................</span>

<span class="n">average</span> <span class="n">iterations</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">7.9435</span>
<span class="n">average</span> <span class="n">samples</span><span class="o">/</span><span class="n">sec</span><span class="p">:</span> <span class="mf">508.3821</span>
</pre></div>
</div>
<p>Using the above tips and tricks we managed to increase the throughput of training by ~600% from the initial benchmark! Our training throughput is less than the throughput of the individual components we tested, but there are additional overheads that we didn’t previously measure (such as data transfer to GPU).</p>
</div>
<div class="section" id="conclusion">
<span id="conclusion"></span><h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We learned a number of tips and tricks to optimize the throughput of training, and they lead to a considerable increase compared to our initial baseline. As general rules, set <code class="docutils literal"><span class="pre">num_workers</span></code> on the <code class="docutils literal"><span class="pre">DataLoader</span></code> to >0, and hybridize your network if you’re not debugging. You should increase <code class="docutils literal"><span class="pre">batch_size</span></code> where possible, but do this with care because of its potential effects on convergence. And finally, consider mixed precision training for substantial speed-ups.</p>
</div>
<div class="section" id="recommended-next-steps">
<span id="recommended-next-steps"></span><h2>Recommended Next Steps<a class="headerlink" href="#recommended-next-steps" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Use the <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/python/profiler.html">MXNet Profiler</a> to identify additional bottlenecks and other areas for optimization.</li>
<li>Check out the <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/hybrid.html">hybridization tutorial</a> for more details on how to write custom <code class="docutils literal"><span class="pre">HybridBlock</span></code>s.</li>
<li>Consider using <a class="reference external" href="https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/multi_gpu.html">multi-GPU</a> and <a class="reference external" href="https://github.com/apache/incubator-mxnet/tree/master/example/distributed_training">multi-host</a> training if you reach the limits of single GPU training.</li>
</ul>
<div class="btn-group" role="group">
<div class="download-btn"><a download="performance.ipynb" href="performance.ipynb"><span class="glyphicon glyphicon-download-alt"></span> performance.ipynb</a></div></div></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar rightsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h3><a href="../../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">Gluon Performance Tips &amp; Tricks</a><ul>
<li><a class="reference internal" href="#initial-benchmark">Initial Benchmark</a></li>
<li><a class="reference internal" href="#identifying-the-bottleneck">Identifying the bottleneck</a></li>
<li><a class="reference internal" href="#tips-tricks-1-use-multiple-workers-on-dataloader">Tips &amp; Tricks #1: Use multiple workers on <code class="docutils literal"><span class="pre">DataLoader</span></code></a></li>
<li><a class="reference internal" href="#tips-tricks-2-hybridize-the-network">Tips &amp; Tricks #2: Hybridize the network</a></li>
<li><a class="reference internal" href="#tips-tricks-3-increase-the-batch-size">Tips &amp; Tricks #3: Increase the batch size</a></li>
<li><a class="reference internal" href="#tips-tricks-4-using-mixed-precision-float32-and-float16">Tips &amp; Tricks #4: Using Mixed-Precision (<code class="docutils literal"><span class="pre">float32</span></code> and <code class="docutils literal"><span class="pre">float16</span></code>)</a></li>
<li><a class="reference internal" href="#tips-tricks-5-others">Tips &amp; Tricks #5: Others</a></li>
<li><a class="reference internal" href="#final-benchmark">Final Benchmark</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#recommended-next-steps">Recommended Next Steps</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div><div class="footer">
<div class="section-disclaimer">
<div class="container">
<div>
<img height="60" src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/apache_incubator_logo.png"/>
<p>
            Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <strong>sponsored by the <i>Apache Incubator</i></strong>. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
        </p>
<p>
            "Copyright © 2017-2018, The Apache Software Foundation
            Apache MXNet, MXNet, Apache, the Apache feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the Apache Software Foundation."
        </p>
</div>
</div>
</div>
</div> <!-- pagename != index -->
</div>
<script crossorigin="anonymous" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script src="../../_static/js/sidebar.js" type="text/javascript"></script>
<script src="../../_static/js/search.js" type="text/javascript"></script>
<script src="../../_static/js/navbar.js" type="text/javascript"></script>
<script src="../../_static/js/clipboard.min.js" type="text/javascript"></script>
<script src="../../_static/js/copycode.js" type="text/javascript"></script>
<script src="../../_static/js/page.js" type="text/javascript"></script>
<script src="../../_static/js/docversion.js" type="text/javascript"></script>
<script type="text/javascript">
        $('body').ready(function () {
            $('body').css('visibility', 'visible');
        });
    </script>
</body>
</html>