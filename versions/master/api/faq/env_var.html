<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->
---
layout: page
---
<div class="row">
    <div class="col-3 docs-side-bar">
        <h3 style="text-transform: capitalize; padding-left:10px">faq</h3>
        <ul>
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/add_op_in_backend">A Beginner's Guide to Implementing Operators in MXNet Backend</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/cloud">MXNet on the Cloud</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/distributed_training">Distributed Training in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/env_var">Environment Variables</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/float16">Float16</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/large_tensor_support">Using MXNet with Large Tensor Support</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/model_parallel_lstm">Model Parallel</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/new_op">Create New Operators</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/perf">Some Tips for Improving MXNet Performance</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/recordio">Create a Dataset Using RecordIO</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/s3_integration">Use data from S3 for training</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/security">MXNet Security Best Practices</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/tensor_inspector_tutorial">Use TensorInspector to Help Debug Operators</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/using_rtc">Using runtime compilation (RTC) to write CUDA kernels in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/why_mxnet">Why MXNet came to be?</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
               <!-- resource-p -->
        </ul>
    </div>
    <div class="col-9">
        <!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="environment-variables">Environment Variables</h1>
<p>MXNet has several settings that you can change with environment variables.
Typically, you wouldn’t need to change these settings, but they are listed here for reference.</p>

<p>For example, you can set these environment variables in Linux or macOS as follows:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export MXNET_GPU_WORKER_NTHREADS=3
</code></pre></div></div>

<p>Or in powershell:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$env:MXNET_STORAGE_FALLBACK_LOG_VERBOSE=0
</code></pre></div></div>

<h2 id="variables-controlling-the-execution-environment">Variables controlling the execution environment</h2>

<ul>
  <li>MXNET_LIBRARY_PATH
  Absolute path indicating where the mxnet dynamic library is to be located, this would be the absolute
  path to <code class="highlighter-rouge">libmxnet.so</code> or <code class="highlighter-rouge">libmxnet.dll</code> depending on the platform. The logic for loading the
  library is in <code class="highlighter-rouge">python/mxnet/libinfo.py</code></li>
</ul>

<h2 id="set-the-number-of-threads">Set the Number of Threads</h2>

<ul>
  <li>MXNET_GPU_WORKER_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=2)</code></li>
      <li>The maximum number of threads to use on each GPU. This parameter is used to parallelize the computation within a single GPU card.</li>
    </ul>
  </li>
  <li>MXNET_GPU_COPY_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=2)</code></li>
      <li>The maximum number of concurrent threads that do the memory copy job on each GPU.</li>
    </ul>
  </li>
  <li>MXNET_CPU_WORKER_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1)</code></li>
      <li>The maximum number of scheduling threads on CPU. It specifies how many operators can be run in parallel. Note that most CPU operators are parallelized by OpenMP. To change the number of threads used by individual operators, please set <code class="highlighter-rouge">OMP_NUM_THREADS</code> instead.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PRIORITY_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>The number of threads given to prioritized CPU jobs.</li>
    </ul>
  </li>
  <li>MXNET_MP_WORKER_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1)</code></li>
      <li>The number of scheduling threads on CPU given to multiprocess workers. Enlarge this number allows more operators to run in parallel in individual workers but please consider reducing the overall <code class="highlighter-rouge">num_workers</code> to avoid thread contention (not available on Windows).</li>
    </ul>
  </li>
  <li>MXNET_MP_OPENCV_NUM_THREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=0)</code></li>
      <li>The number of OpenCV execution threads given to multiprocess workers. OpenCV multithreading is disabled if <code class="highlighter-rouge">MXNET_MP_OPENCV_NUM_THREADS</code> &lt; 1 (default). Enlarge this number may boost the performance of individual workers when executing underlying OpenCV functions but please consider reducing the overall <code class="highlighter-rouge">num_workers</code> to avoid thread contention (not available on Windows).</li>
    </ul>
  </li>
</ul>

<h2 id="memory-options">Memory Options</h2>

<ul>
  <li>MXNET_EXEC_ENABLE_INPLACE
    <ul>
      <li>Values: true or false <code class="highlighter-rouge">(default=true)</code>
        <ul>
          <li>Whether to enable in-place optimization in symbolic execution. Checkout <a href="/versions/master/api/architecture/note_memory#in-place-operations">in-place optimization</a> to know more about it.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>NNVM_EXEC_MATCH_RANGE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=16)</code></li>
      <li>The approximate matching scale in the symbolic execution memory allocator.</li>
      <li>Set this to 0 if you don’t want to enable memory sharing between graph nodes(for debugging purposes).</li>
      <li>This variable has impact on the result of memory planning. So, MXNet sweep between [1, NNVM_EXEC_MATCH_RANGE], and selects the best value.</li>
    </ul>
  </li>
  <li>MXNET_EXEC_NUM_TEMP
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1)</code></li>
      <li>The maximum number of temporary workspaces to allocate to each device. This controls space replicas and in turn reduces the memory usage.</li>
      <li>Setting this to a small number can save GPU memory. It will also likely decrease the level of parallelism, which is usually acceptable.
        <ul>
          <li>MXNet internally uses graph coloring algorithm to <a href="/versions/master/api/architecture/note_memory">optimize memory consumption</a>.</li>
        </ul>
      </li>
      <li>This parameter is also used to get number of matching colors in graph and in turn how much parallelism one can get in each GPU. Color based match usually costs more memory but also enables more parallelism.</li>
    </ul>
  </li>
  <li>MXNET_GPU_MEM_POOL_TYPE
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default=Naive)</code></li>
      <li>The type of GPU memory pool.</li>
      <li>Choices:
        <ul>
          <li><em>Naive</em>: A simple memory pool that allocates memory for the requested size and cache memory buffers, when this memory is released. The size of memory chunk is defined by rounding the requested memory size to the nearest bigger multiple of MXNET_GPU_MEM_POOL_PAGE_SIZE (or MXNET_GPU_MEM_LARGE_ALLOC_ROUND_SIZE, when the result of rounding for MXNET_GPU_MEM_POOL_PAGE_SIZE is bigger than MXNET_GPU_MEM_LARGE_ALLOC_ROUND_SIZE) and allocates memory of the rounded size.</li>
          <li><em>Round</em>: A memory pool that try to rounds the requested memory size to the nearest bigger power of 2. When this rounded number is bigger that 2<em>*MXNET_GPU_MEM_POOL_ROUND_LINEAR_CUTOFF, the *Naive</em> rounding algorithm is used. Caching and allocating buffered memory works in the same way as the naive memory pool.</li>
          <li><em>Unpooled</em>: No memory pool is used.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>MXNET_GPU_MEM_POOL_RESERVE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=5)</code></li>
      <li>The percentage of GPU memory to reserve for things other than the GPU array, such as kernel launch or cudnn handle space.</li>
      <li>The value is used only by the GPU memory pool. If it is not possible to allocate new memory AND still save this reserve, the memory pool will free the cached memory.</li>
      <li>If you see a strange out-of-memory error from the kernel launch, after multiple iterations, try setting this to a larger value.</li>
    </ul>
  </li>
  <li>MXNET_GPU_MEM_LARGE_ALLOC_ROUND_SIZE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=2097152)</code></li>
      <li>When the rounded size of memory allocations calculated by the pool of <em>Naive</em> type is larger than this threshold, it will be rounded up to a multiple of this value.</li>
      <li>The default was chosen to minimize global memory fragmentation within the GPU driver. Set this to 1 to disable.</li>
    </ul>
  </li>
  <li>MXNET_GPU_MEM_POOL_ROUND_LINEAR_CUTOFF
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=24)</code></li>
      <li>The cutoff threshold used by <em>Round</em> strategy. Let’s denote the threshold as T. If the memory size is smaller than <code class="highlighter-rouge">2 ** T</code> (by default, it’s 2 ** 24 = 16MB), it rounds to the smallest <code class="highlighter-rouge">2 ** n</code> that is larger than the requested memory size; if the memory size is larger than <code class="highlighter-rouge">2 ** T</code>, it rounds to the next k * 2 ** T.</li>
    </ul>
  </li>
  <li>MXNET_CPU_MEM_POOL_TYPE
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default=Naive)</code></li>
      <li>The type of CPU memory pool.</li>
      <li>Choices:
        <ul>
          <li><em>Naive</em>: A simple memory pool that allocates memory for the requested size and cache memory buffers, when this memory is released. The size of memory chunk is defined by rounding the requested memory size to the nearest bigger multiple of MXNET_CPU_MEM_POOL_PAGE_SIZE (or MXNET_CPU_MEM_LARGE_ALLOC_ROUND_SIZE, when the result of rounding for MXNET_CPU_MEM_POOL_PAGE_SIZE is bigger than MXNET_CPU_MEM_LARGE_ALLOC_ROUND_SIZE) and allocates memory of the rounded size.</li>
          <li><em>Round</em>: A memory pool that try to rounds the requested memory size to the nearest bigger power of 2. When this rounded number is bigger that 2<em>*MXNET_CPU_MEM_POOL_ROUND_LINEAR_CUTOFF, the the *Naive</em> rounding algorithm is used. Caching and allocating buffered memory works in the same way as the naive memory pool.</li>
          <li><em>Unpooled</em>: No memory pool is used.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>MXNET_CPU_MEM_POOL_RESERVE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=5)</code></li>
      <li>The percentage of CPU memory to reserve for things other than the CPU array.</li>
      <li>The value is used only by the CPU memory pool. If it is not possible to allocate new memory AND still save this reserve, the memory pool will free the cached memory.</li>
      <li>If you see a strange out-of-memory error from the kernel launch, after multiple iterations, try setting this to a larger value.</li>
    </ul>
  </li>
  <li>MXNET_CPU_MEM_LARGE_ALLOC_ROUND_SIZE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=2097152)</code></li>
      <li>When the rounded size of memory allocations calculated by the pool of <em>Naive</em> type is larger than this threshold, it will be rounded up to a multiple of this value.</li>
      <li>Set this to 1 to disable.</li>
    </ul>
  </li>
  <li>MXNET_CPU_MEM_POOL_ROUND_LINEAR_CUTOFF
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=24)</code></li>
      <li>The cutoff threshold used by <em>Round</em> strategy. Let’s denote the threshold as T. If the memory size is smaller than <code class="highlighter-rouge">2 ** T</code> (by default, it’s 2 ** 24 = 16MB), it rounds to the smallest <code class="highlighter-rouge">2 ** n</code> that is larger than the requested memory size; if the memory size is larger than <code class="highlighter-rouge">2 ** T</code>, it rounds to the next k * 2 ** T.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PINNED_MEM_POOL_TYPE
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default=Naive)</code></li>
      <li>The type of CPU_PINNED memory pool.</li>
      <li>Choices:
        <ul>
          <li><em>Naive</em>: A simple memory pool that allocates memory for the requested size and cache memory buffers, when this memory is released. The size of memory chunk is defined by rounding the requested memory size to the nearest bigger multiple of MXNET_CPU_PINNED_MEM_POOL_PAGE_SIZE (or MXNET_CPU_PINNED_MEM_LARGE_ALLOC_ROUND_SIZE, when the result of rounding for MXNET_CPU_PINNED_MEM_POOL_PAGE_SIZE is bigger than MXNET_CPU_PINNED_MEM_LARGE_ALLOC_ROUND_SIZE) and allocates memory of the rounded size.</li>
          <li><em>Round</em>: A memory pool that try to rounds the requested memory size to the nearest bigger power of 2. When this rounded number is bigger that 2<em>*MXNET_CPU_PINNED_MEM_POOL_ROUND_LINEAR_CUTOFF, the the *Naive</em> rounding algorithm is used. Caching and allocating buffered memory works in the same way as the naive memory pool.</li>
          <li><em>Unpooled</em>: No memory pool is used.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>MXNET_CPU_PINNED_MEM_POOL_RESERVE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=5)</code></li>
      <li>The percentage of GPU memory to reserve for things other than the GPU array.</li>
      <li>The value is used only by the CPU memory pool. If it is not possible to allocate new memory AND still save this reserve, the memory pool will free the cached memory.</li>
      <li>If you see a strange out-of-memory error from the kernel launch, after multiple iterations, try setting this to a larger value.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PINNED_MEM_LARGE_ALLOC_ROUND_SIZE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=2097152)</code></li>
      <li>When the rounded size of memory allocations calculated by the pool of <em>Naive</em> type is larger than this threshold, it will be rounded up to a multiple of this value.</li>
      <li>Set this to 1 to disable.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PINNED_MEM_POOL_ROUND_LINEAR_CUTOFF
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=24)</code></li>
      <li>The cutoff threshold used by <em>Round</em> strategy. Let’s denote the threshold as T. If the memory size is smaller than <code class="highlighter-rouge">2 ** T</code> (by default, it’s 2 ** 24 = 16MB), it rounds to the smallest <code class="highlighter-rouge">2 ** n</code> that is larger than the requested memory size; if the memory size is larger than <code class="highlighter-rouge">2 ** T</code>, it rounds to the next k * 2 ** T.</li>
    </ul>
  </li>
  <li>MXNET_USE_NAIVE_STORAGE_MANAGERS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=0)</code></li>
      <li>When value is not 0, no memory pools will be used for any of the following three types of memory: GPU, CPU, CPU_PINNED.</li>
    </ul>
  </li>
</ul>

<h2 id="engine-type">Engine Type</h2>

<ul>
  <li>MXNET_ENGINE_TYPE
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default=ThreadedEnginePerDevice)</code></li>
      <li>The type of underlying execution engine of MXNet.</li>
      <li>Choices:
        <ul>
          <li>NaiveEngine: A very simple engine that uses the master thread to do the computation synchronously. Setting this engine disables multi-threading. You can use this type for debugging in case of any error. Backtrace will give you the series of calls that lead to the error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.</li>
          <li>ThreadedEngine: A threaded engine that uses a global thread pool to schedule jobs.</li>
          <li>ThreadedEnginePerDevice: A threaded engine that allocates thread per GPU and executes jobs asynchronously.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="execution-options">Execution Options</h2>

<ul>
  <li>MXNET_EXEC_BULK_EXEC_INFERENCE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to <code class="highlighter-rouge">1</code>, during inference MXNet executes the entire computation graph in bulk mode, which reduces kernel launch gaps in between symbolic operators.</li>
    </ul>
  </li>
  <li>MXNET_EXEC_BULK_EXEC_TRAIN
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to <code class="highlighter-rouge">1</code>, during training MXNet executes the computation graph as several subgraphs in bulk mode.</li>
    </ul>
  </li>
  <li>MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=15)</code></li>
      <li>The maximum number of nodes in the subgraph executed in bulk during training (not inference). Setting this to a larger number may reduce the degree of parallelism for multi-GPU training.</li>
    </ul>
  </li>
  <li>MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_FWD
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=&lt;value of MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN&gt;)</code></li>
      <li>The maximum number of nodes in the subgraph executed in bulk during training (not inference) in the forward pass.</li>
    </ul>
  </li>
  <li>MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_BWD
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=&lt;value of MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN&gt;)</code></li>
      <li>The maximum number of nodes in the subgraph executed in bulk during training (not inference) in the backward pass.</li>
    </ul>
  </li>
</ul>

<h2 id="control-the-data-communication">Control the Data Communication</h2>

<ul>
  <li>MXNET_KVSTORE_REDUCTION_NTHREADS
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>The number of CPU threads used for summing up big arrays on a single machine</li>
      <li>This will also be used for <code class="highlighter-rouge">dist_sync</code> kvstore to sum up arrays from different contexts on a single machine.</li>
      <li>This does not affect summing up of arrays from different machines on servers.</li>
      <li>Summing up of arrays for <code class="highlighter-rouge">dist_sync_device</code> kvstore is also unaffected as that happens on GPUs.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_BIGARRAY_BOUND
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1000000)</code></li>
      <li>The minimum size of a “big array”.</li>
      <li>When the array size is bigger than this threshold, MXNET_KVSTORE_REDUCTION_NTHREADS threads are used for reduction.</li>
      <li>This parameter is also used as a load balancer in kvstore. It controls when to partition a single weight to all the servers. If the size of a single weight is less than MXNET_KVSTORE_BIGARRAY_BOUND then, it is sent to a single randomly picked server otherwise it is partitioned to all the servers.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_USETREE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If true, MXNet tries to use tree reduction for Push and Pull communication.</li>
      <li>Otherwise, MXNet uses the default Push and Pull implementation.</li>
      <li>Tree reduction technology has been shown to be faster than the standard <code class="highlighter-rouge">--kv-store device</code> Push/Pull and <code class="highlighter-rouge">--kv-store nccl</code> Push/Pull for small batch sizes.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_LOGTREE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If true and MXNET_KVSTORE_USETREE is set to 1, MXNet will log the reduction trees that have been generated.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_TREE_ARRAY_BOUND
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=10000000)</code></li>
      <li>The minimum size of a “big array”.</li>
      <li>When the array size is bigger than this threshold and MXNET_KVSTORE_USETREE is set to 1, multiple trees are used to load balance the big gradient being communicated in order to better saturate link bandwidth.</li>
      <li>Note: This environmental variable only takes effect if Tree KVStore is being used (MXNET_KVSTORE_USETREE=1).</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_TREE_BACKTRACK
    <ul>
      <li>Values: 0(false) or 1(true) ```(default=0)</li>
      <li>If true and MXNET_KVSTORE_USETREE is set to 1, MXNet tries to use backtracking to generate the trees required for tree reduction.</li>
      <li>If false and MXNET_KVSTORE_USETREE is set to 1, MXNet tries to use Kernighan-Lin heuristic to generate the trees required for tree reduction.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_TREE_LINK_USAGE_PENALTY
    <ul>
      <li>Values: Float <code class="highlighter-rouge">(default=0.7)</code></li>
      <li>The multiplicative penalty term to a link being used once.</li>
    </ul>
  </li>
  <li>MXNET_ENABLE_GPU_P2P
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If true, MXNet tries to use GPU peer-to-peer communication, if available on your device,
when kvstore’s type is <code class="highlighter-rouge">device</code>.</li>
    </ul>
  </li>
  <li>MXNET_UPDATE_ON_KVSTORE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If true, weight updates are performed during the communication step, if possible.</li>
    </ul>
  </li>
  <li>MXNET_KVSTORE_SLICE_THRESHOLD
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=40000)</code></li>
      <li>The maximum size of an NDArray slice in terms of number of parameters.</li>
      <li>This parameter is used to slice an NDArray before synchronizing through P3Store (dist_p3).</li>
    </ul>
  </li>
</ul>

<h2 id="memory-optimizations">Memory Optimizations</h2>

<ul>
  <li>MXNET_BACKWARD_DO_MIRROR
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>MXNet uses mirroring concept to save memory. Normally backward pass needs some forward input and it is stored in memory but you can choose to release this saved input and recalculate it in backward pass when needed. This basically trades off the computation for memory consumption.</li>
      <li>This parameter decides whether to do <code class="highlighter-rouge">mirror</code> during training for saving device memory.</li>
      <li>When set to <code class="highlighter-rouge">1</code>, during forward propagation, graph executor will <code class="highlighter-rouge">mirror</code> some layer’s feature map and drop others, but it will re-compute this dropped feature maps when needed.</li>
      <li><code class="highlighter-rouge">MXNET_BACKWARD_DO_MIRROR=1</code> will save 30%~50% of device memory, but retains about 95% of running speed.</li>
      <li>One extension of <code class="highlighter-rouge">mirror</code> in MXNet is called <a href="https://arxiv.org/abs/1604.06174">memonger technology</a>, it will only use O(sqrt(N)) memory at 75% running speed. Checkout the code <a href="https://github.com/dmlc/mxnet-memonger">here</a>.</li>
    </ul>
  </li>
  <li>MXNET_MEMORY_OPT
    <ul>
      <li>Values: 0(no optimizations) or 1(highest optimization level) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If set to ‘1’, various optimizations on memory consumption will be enabled.</li>
    </ul>
  </li>
</ul>

<h2 id="control-the-profiler">Control the profiler</h2>

<p>The following environments can be used to profile the application without changing code. Execution options may affect the granularity of profiling result. If you need profiling result of every operator, please set <code class="highlighter-rouge">MXNET_EXEC_BULK_EXEC_INFERENCE</code>, <code class="highlighter-rouge">MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN</code> and <code class="highlighter-rouge">MXNET_EXEC_BULK_EXEC_TRAIN</code> to 0.</p>

<ul>
  <li>MXNET_PROFILER_AUTOSTART
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>Set to 1, MXNet starts the profiler automatically. The profiling result is stored into profile.json in the working directory.</li>
    </ul>
  </li>
  <li>MXNET_PROFILER_MODE
    <ul>
      <li>Values: 0(false) to 15(profile everything) <code class="highlighter-rouge">(default=13)</code></li>
      <li>If set to ‘0’, turns off all profiling.</li>
      <li>If set to ‘1’, profiler records the events of symbolic operators.</li>
      <li>If set to ‘2’, profiler records the events of imperative operators.</li>
      <li>If set to ‘4’, profiler records the C API events.</li>
      <li>If set to ‘8’, profiler records the events of memory (i.e. storage alloc and free calls).</li>
      <li>You need to sum the values above for a custom combination. For example, for symbolic and imperative operators, set <code class="highlighter-rouge">MXNET_PROFILER_MODE=3</code>(2 + 1).</li>
      <li>If set to ‘15’, profiler records all the above listed events (API, Memory, Symbolic, Imperative).</li>
    </ul>
  </li>
</ul>

<h2 id="interface-between-python-and-the-c-api">Interface between Python and the C API</h2>

<ul>
  <li>MXNET_ENABLE_CYTHON
    <ul>
      <li>Values: 0(false), 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to 0, MXNet uses the ctypes to interface with the C API.</li>
      <li>If set to 1, MXNet tries to use the cython modules for the ndarray and symbol. If it fails, the ctypes is used or an error occurs depending on MXNET_ENFORCE_CYTHON.</li>
    </ul>
  </li>
  <li>MXNET_ENFORCE_CYTHON
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>This has an effect only if MXNET_ENABLE_CYTHON is 1.</li>
      <li>If set to 0, MXNet fallbacks to the ctypes if importing the cython modules fails.</li>
      <li>If set to 1, MXNet raises an error if importing the cython modules fails.</li>
    </ul>
  </li>
</ul>

<p>If cython modules are used, <code class="highlighter-rouge">mx.nd._internal.NDArrayBase</code> must be <code class="highlighter-rouge">mxnet._cy3.ndarray.NDArrayBase</code> for python 3 or <code class="highlighter-rouge">mxnet._cy2.ndarray.NDArrayBase</code> for python 2.
If ctypes is used, it must be <code class="highlighter-rouge">mxnet._ctypes.ndarray.NDArrayBase</code>.</p>

<h2 id="logging">Logging</h2>

<ul>
  <li>DMLC_LOG_STACK_TRACE_DEPTH
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=0)</code></li>
      <li>The depth of stack trace information to log when exception happens.</li>
    </ul>
  </li>
</ul>

<h2 id="other-environment-variables">Other Environment Variables</h2>

<ul>
  <li>MXNET_GPU_WORKER_NSTREAMS
    <ul>
      <li>Values: 1, or 2 <code class="highlighter-rouge">(default=1)</code></li>
      <li>Determines the number of GPU streams available to operators for their functions.</li>
      <li>Setting this to 2 may yield a modest performance increase, since ops like the cuDNN convolution op can then calculate their data- and weight-gradients in parallel.</li>
      <li>Setting this to 2 may also increase a model’s demand for GPU global memory.</li>
    </ul>
  </li>
  <li>MXNET_CUDNN_AUTOTUNE_DEFAULT
    <ul>
      <li>Values: 0, 1, or 2 <code class="highlighter-rouge">(default=1)</code></li>
      <li>The default value of cudnn auto tuning for convolution layers.</li>
      <li>Value of 0 means there is no auto tuning to pick the convolution algo</li>
      <li>Performance tests are run to pick the convolution algo when value is 1 or 2</li>
      <li>Value of 1 chooses the best algo in a limited workspace</li>
      <li>Value of 2 chooses the fastest algo whose memory requirements may be larger than the default workspace threshold</li>
    </ul>
  </li>
  <li>MXNET_CUDA_ALLOW_TENSOR_CORE
    <ul>
      <li>0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to ‘0’, disallows Tensor Core use in CUDA ops.</li>
      <li>If set to ‘1’, allows Tensor Core use in CUDA ops.</li>
      <li>This variable can only be set once in a session.</li>
    </ul>
  </li>
  <li>MXNET_CUDA_TENSOR_OP_MATH_ALLOW_CONVERSION
    <ul>
      <li>0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If set to ‘0’, disallows implicit type conversions to Float16 to use Tensor Cores</li>
      <li>If set to ‘1’, allows CUDA ops like RNN and Convolution to use TensorCores even with Float32 input data by using implicit type casting to Float16. Only has an effect if <code class="highlighter-rouge">MXNET_CUDA_ALLOW_TENSOR_CORE</code> is <code class="highlighter-rouge">1</code>.</li>
    </ul>
  </li>
  <li>MXNET_CUDA_LIB_CHECKING
    <ul>
      <li>0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to ‘0’, disallows various runtime checks of the cuda library version and associated warning messages.</li>
      <li>If set to ‘1’, permits these checks (e.g. compile vs. link mismatch, old version no longer CI-tested)</li>
    </ul>
  </li>
  <li>MXNET_CUDNN_LIB_CHECKING
    <ul>
      <li>0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If set to ‘0’, disallows various runtime checks of the cuDNN library version and associated warning messages.</li>
      <li>If set to ‘1’, permits these checks (e.g. compile vs. link mismatch, old version no longer CI-tested)</li>
    </ul>
  </li>
  <li>MXNET_GLUON_REPO
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default='https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/'</code></li>
      <li>The repository url to be used for Gluon datasets and pre-trained models.</li>
    </ul>
  </li>
  <li>MXNET_HOME
    <ul>
      <li>Data directory in the filesystem for storage, for example when downloading gluon models.</li>
      <li>Default in *nix is .mxnet APPDATA/mxnet in windows.</li>
    </ul>
  </li>
  <li>MXNET_MKLDNN_ENABLED
    <ul>
      <li>Values: 0, 1 <code class="highlighter-rouge">(default=1)</code></li>
      <li>Flag to enable or disable MKLDNN accelerator. On by default.</li>
      <li>Only applies to mxnet that has been compiled with MKLDNN (<code class="highlighter-rouge">pip install mxnet-mkl</code> or built from source with <code class="highlighter-rouge">USE_MKLDNN=1</code>)</li>
    </ul>
  </li>
  <li>MXNET_MKLDNN_CACHE_NUM
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=-1)</code></li>
      <li>Flag to set num of elements that MKLDNN cache can hold. Default is -1 which means cache size is unbounded. Should only be set if your model has variable input shapes, as cache size may grow unbounded. The number represents the number of items in the cache and is proportional to the number of layers that use MKLDNN and different input shape.</li>
    </ul>
  </li>
  <li>MXNET_ENFORCE_DETERMINISM
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If set to true, MXNet will only use deterministic algorithms in forward and backward computation.
If no such algorithm exists given other constraints, MXNet will error out. This variable affects the choice
of CUDNN convolution algorithms. Please see <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html">CUDNN developer guide</a> for more details.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PARALLEL_SIZE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=200000)</code></li>
      <li>The minimum size to call parallel operations by OpenMP for CPU context.</li>
      <li>When the array size is bigger than or equal to this threshold, the operation implemented by OpenMP is executed with the Recommended OMP Thread Count.</li>
      <li>When the array size is less than this threshold, the operation is implemented naively in single thread.</li>
    </ul>
  </li>
  <li>MXNET_OPTIMIZER_AGGREGATION_SIZE
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>Maximum value is 60.</li>
      <li>This variable controls how many weights will be updated in a single call to optimizer (for optimizers that support aggregation, currently limited to SGD).</li>
    </ul>
  </li>
  <li>MXNET_CPU_TEMP_COPY
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>This variable controls how many temporary memory resources to create for all CPU context for use in operator.</li>
    </ul>
  </li>
  <li>MXNET_GPU_TEMP_COPY
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1)</code></li>
      <li>This variable controls how many temporary memory resources to create for each GPU context for use in operator.</li>
    </ul>
  </li>
  <li>MXNET_CPU_PARALLEL_RAND_COPY
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=1)</code></li>
      <li>This variable controls how many parallel random number generator resources to create for all CPU context for use in operator.</li>
    </ul>
  </li>
  <li>MXNET_GPU_PARALLEL_RAND_COPY
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>This variable controls how many parallel random number generator resources to create for each GPU context for use in operator.</li>
    </ul>
  </li>
  <li>MXNET_GPU_CUDNN_DROPOUT_STATE_COPY
    <ul>
      <li>Values: Int <code class="highlighter-rouge">(default=4)</code></li>
      <li>This variable controls how many CuDNN dropout state resources to create for each GPU context for use in operator.</li>
    </ul>
  </li>
  <li>MXNET_SUBGRAPH_BACKEND
    <ul>
      <li>Values: String <code class="highlighter-rouge">(default="MKLDNN")</code> if MKLDNN is avaliable, otherwise <code class="highlighter-rouge">(default="")</code></li>
      <li>This variable controls the subgraph partitioning in MXNet.</li>
      <li>This variable is used to perform MKL-DNN FP32 operator fusion and quantization. Please refer to the <a href="https://github.com/apache/incubator-mxnet/blob/v1.5.x/docs/tutorials/mkldnn/operator_list.md">MKL-DNN operator list</a> for how this variable is used and the list of fusion passes.</li>
      <li>Set <code class="highlighter-rouge">MXNET_SUBGRAPH_BACKEND=NONE</code> to disable subgraph backend.</li>
    </ul>
  </li>
  <li>MXNET_SAFE_ACCUMULATION
    <ul>
      <li>Values: Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If this variable is set, the accumulation will enter the safe mode, meaning accumulation is done in a data type of higher precision than
the input data type, leading to more accurate accumulation results with a possible performance loss and backward compatibility loss.
For example, when the variable is set to 1(true), if the input data type is float16, then the accumulation will be done
with float32.</li>
      <li>Model accuracies do not necessarily improve with this environment variable turned on.</li>
    </ul>
  </li>
  <li>MXNET_USE_FUSION
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If this variable is set, MXNet will try fusing some of the operations (pointwise operations only for now).</li>
      <li>It works in Symbolic execution as well as in Gluon models hybridized with <code class="highlighter-rouge">static_alloc=True</code> option.</li>
      <li>Only applies to MXNet that has been compiled with CUDA (<code class="highlighter-rouge">pip install mxnet-cuXX</code> or built from source with <code class="highlighter-rouge">USE_CUDA=1</code>) and running on GPU.</li>
    </ul>
  </li>
  <li>MXNET_RTC_VERBOSE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>Only applies to MXNet that has been compiled with CUDA.</li>
      <li>If this variable is set, MXNet will print the code for operators compiled at runtime.</li>
    </ul>
  </li>
  <li>MXNET_ELIMINATE_COMMON_EXPR
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>If this variable is set, MXNet will simplify the computation graph, eliminating duplicated operations on the same inputs.</li>
    </ul>
  </li>
  <li>MXNET_USE_MKLDNN_RNN
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=1)</code></li>
      <li>This variable controls whether to use the MKL-DNN backend in fused RNN operator for CPU context. There are two fusion implementations of RNN operator in MXNet. The MKL-DNN implementation has a better performance than the naive one, but the latter is more stable in the backward operation currently.</li>
    </ul>
  </li>
  <li>MXNET_FC_TRUE_FP16
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If this variable is set to true, MXNet will perform fp16 accumulation when using cuBLAS and input datatype is set to float16. This could increase the speed of the computation, but might result in loss of accuracy. This makes this setting useful mainly for inference usecases.</li>
    </ul>
  </li>
  <li>MXNET_RNN_USE_WEIGHT_CACHE
    <ul>
      <li>Values: 0(false) or 1(true) <code class="highlighter-rouge">(default=0)</code></li>
      <li>If this variable is set, MXNet will ignore the altering of the version of NDArray which is the input parameter of the RNN operator. In Gluon API, there is a <code class="highlighter-rouge">_rnn_param_concat</code> operator concatenating the weights and bias of RNN into a single parameter tensor that changes the version number. Since the values of the parameters are invariant in inference pass, the RNN operator could ignore the altering of the version to escape much overhead from re-initializing the parameters.</li>
    </ul>
  </li>
</ul>

<h2 id="settings-for-minimum-memory-usage">Settings for Minimum Memory Usage</h2>
<ul>
  <li>Make sure <code class="highlighter-rouge">min(MXNET_EXEC_NUM_TEMP, MXNET_GPU_WORKER_NTHREADS) = 1</code>
    <ul>
      <li>The default setting satisfies this.</li>
    </ul>
  </li>
</ul>

<h2 id="settings-for-more-gpu-parallelism">Settings for More GPU Parallelism</h2>
<ul>
  <li>Set <code class="highlighter-rouge">MXNET_GPU_WORKER_NTHREADS</code> to a larger number (e.g., 2)
    <ul>
      <li>To reduce memory usage, consider setting <code class="highlighter-rouge">MXNET_EXEC_NUM_TEMP</code>.</li>
      <li>This might not speed things up, especially for image applications, because GPU is usually fully utilized even with serialized jobs.</li>
    </ul>
  </li>
</ul>

<h2 id="settings-for-controlling-omp-tuning">Settings for controlling OMP tuning</h2>
<ul>
  <li>Set <code class="highlighter-rouge">MXNET_USE_OPERATOR_TUNING=0</code> to disable Operator tuning code which decides whether to use OMP or not for operator
    <ul>
      <li>Values: String representation of MXNET_ENABLE_OPERATOR_TUNING environment variable</li>
      <li>0=disable all</li>
      <li>1=enable all</li>
      <li>float32, float16, float32=list of types to enable, and disable those not listed</li>
      <li>refer : https://github.com/apache/incubator-mxnet/blob/master/src/operator/operator_tune-inl.h#L444</li>
    </ul>
  </li>
  <li>Set <code class="highlighter-rouge">MXNET_USE_NUM_CORES_OPERATOR_TUNING</code> to define num_cores to be used by operator tuning code.
    <ul>
      <li>This reduces operator tuning overhead when there are multiple instances of mxnet running in the system and we know that
each mxnet will take only partial num_cores available with system.</li>
      <li>refer: https://github.com/apache/incubator-mxnet/pull/13602</li>
    </ul>
  </li>
</ul>

    </div>
</div>
