<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->
---
layout: page
---
<div class="row">
    <div class="col-3 docs-side-bar">
        <h3 style="text-transform: capitalize; padding-left:10px">faq</h3>
        <ul>
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/add_op_in_backend">A Beginner's Guide to Implementing Operators in MXNet Backend</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/cloud">MXNet on the Cloud</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/distributed_training">Distributed Training in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/env_var">Environment Variables</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/float16">Float16</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/large_tensor_support">Using MXNet with Large Tensor Support</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/model_parallel_lstm">Model Parallel</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/new_op">Create New Operators</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/perf">Some Tips for Improving MXNet Performance</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/recordio">Create a Dataset Using RecordIO</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/s3_integration">Use data from S3 for training</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/security">MXNet Security Best Practices</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/tensor_inspector_tutorial">Use TensorInspector to Help Debug Operators</a></li>
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/using_rtc">Using runtime compilation (RTC) to write CUDA kernels in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/versions/master/api/faq/why_mxnet">Why MXNet came to be?</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
               <!-- resource-p -->
        </ul>
    </div>
    <div class="col-9">
        <!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="mxnet-on-the-cloud">MXNet on the Cloud</h1>

<p>Deep learning can require extremely powerful hardware, often for unpredictable durations of time.
Moreover, <em>MXNet</em> can benefit from both multiple GPUs and multiple machines.
Accordingly, cloud computing, as offered by AWS and others,
is especially well suited to training deep learning models.
Using AWS, we can rapidly fire up multiple machines with multiple GPUs each at will
and maintain the resources for precisely the amount of time needed.</p>

<h2 id="set-up-an-aws-gpu-cluster-from-scratch">Set Up an AWS GPU Cluster from Scratch</h2>

<p>In this document, we provide a step-by-step guide that will teach you
how to set up an AWS cluster with <em>MXNet</em>. We show how to:</p>

<ul>
  <li><a href="#use-pre-installed-ec2-gpu-instance">Use Pre-installed EC2 GPU Instance</a></li>
  <li><a href="#build-and-run-mxnet-on-a-gpu-instance">Build and run MXNet on a single computer</a></li>
  <li><a href="#set-up-an-ec2-gpu-cluster-for-distributed-training">Set up an EC2 GPU cluster for distributed training</a></li>
</ul>

<h3 id="use-pre-installed-ec2-gpu-instance">Use Pre-installed EC2 GPU Instance</h3>
<p>The <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=Deep+Learning+AMI">Deep Learning AMIs</a>
are a series of images supported and maintained by Amazon Web Services for use
on Amazon Elastic Compute Cloud (Amazon EC2) and contain the latest MXNet release.</p>

<p>Now you can launch <em>MXNet</em> directly on an EC2 GPU instance.
You can also use <a href="https://jupyter.org">Jupyter</a> notebook on EC2 machine.
Here is a <a href="https://github.com/dmlc/mxnet-notebooks">good tutorial</a>
on how to connect to a Jupyter notebook running on an EC2 instance.</p>

<h3 id="set-up-an-ec2-gpu-instance-from-scratch">Set Up an EC2 GPU Instance from Scratch</h3>

<p><a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=Deep+Learning+Base+AMI">Deep Learning Base AMIs</a>
provide a foundational image with NVIDIA CUDA, cuDNN, GPU drivers, Intel
MKL-DNN, Docker and Nvidia-Docker, etc. for deploying your own custom deep
learning environment. You may follow the <a href="https://mxnet.apache.org/get_started/build_from_source">MXNet Build From Source
instructions</a> easily on
the Deep Learning Base AMIs.</p>

<h3 id="set-up-an-ec2-gpu-cluster-for-distributed-training">Set Up an EC2 GPU Cluster for Distributed Training</h3>

<p>A cluster consists of multiple computers.
You can use one computer with <em>MXNet</em> installed as the root computer for submitting jobs,and then launch several
slave computers to run the jobs. For example, launch multiple instances using an
AMI with dependencies installed. There are two options:</p>

<ul>
  <li>
    <p>Make all slaves’ ports accessible (same for the root) by setting type: All TCP,
 Source: Anywhere in Configure Security Group.</p>
  </li>
  <li>
    <p>Use the same <code class="highlighter-rouge">pem</code> as the root computer to access all slave computers, and
 then copy the <code class="highlighter-rouge">pem</code> file into the root computer’s <code class="highlighter-rouge">~/.ssh/id_rsa</code>. If you do this, all slave computers can be accessed with SSH from the root.</p>
  </li>
</ul>

<p>Now, run the CNN on multiple computers. Assume that we are on a working
directory of the root computer, such as <code class="highlighter-rouge">~/train</code>, and MXNet is built as <code class="highlighter-rouge">~/mxnet</code>.</p>

<ol>
  <li>Pack the <em>MXNet</em> Python library into this working directory for easy
  synchronization:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">cp</span> <span class="nt">-r</span> ~/mxnet/python/mxnet <span class="nb">.</span>
  <span class="nb">cp</span> ~/mxnet/lib/libmxnet.so mxnet/
</code></pre></div></div>

<p>And then copy the training program:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">cp</span> ~/mxnet/example/image-classification/<span class="k">*</span>.py <span class="nb">.</span>
  <span class="nb">cp</span> <span class="nt">-r</span> ~/mxnet/example/image-classification/common <span class="nb">.</span>
</code></pre></div></div>

<ol>
  <li>Prepare a host file with all slaves private IPs. For example, <code class="highlighter-rouge">cat hosts</code>:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  172.30.0.172
  172.30.0.171
</code></pre></div></div>

<ol>
  <li>Assuming that there are two computers, train the CNN using two workers:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ../../tools/launch.py <span class="nt">-n</span> 2 <span class="nt">-H</span> hosts <span class="nt">--sync-dir</span> /tmp/mxnet python train_mnist.py <span class="nt">--kv-store</span> dist_sync
</code></pre></div></div>

<p><strong><em>Note:</em></strong> Sometimes the jobs linger at the slave computers even though you’ve pressed <code class="highlighter-rouge">Ctrl-c</code>
at the root node. To terminate them, use the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>hosts | xargs <span class="nt">-I</span><span class="o">{}</span> ssh <span class="nt">-o</span> <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no <span class="o">{}</span> <span class="s1">'uname -a; pgrep python | xargs kill -9'</span>
</code></pre></div></div>

<p><strong><em>Note:</em></strong> The preceding example is very simple to train and therefore isn’t a good
benchmark for distributed training. Consider using other <a href="https://github.com/dmlc/mxnet/tree/master/example/image-classification">examples</a>.</p>

<h3 id="more-options">More Options</h3>
<h4 id="use-multiple-data-shards">Use Multiple Data Shards</h4>
<p>It is common to pack a dataset into multiple files, especially when working in a distributed environment.
<em>MXNet</em> supports direct loading from multiple data shards.
Put all of the record files into a folder, and point the data path to the folder.</p>

<h4 id="use-yarn-and-sge">Use YARN and SGE</h4>
<p>Although using SSH can be simple when you don’t have a cluster scheduling framework,
<em>MXNet</em> is designed to be portable to various platforms.
We provide scripts available in <a href="https://github.com/dmlc/dmlc-core/tree/master/tracker">tracker</a>
to allow running on other cluster frameworks, including Hadoop (YARN) and SGE.
We welcome contributions from the community of examples of running <em>MXNet</em> on your favorite distributed platform.</p>

    </div>
</div>
