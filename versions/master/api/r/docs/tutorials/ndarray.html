<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->
---
layout: page
---
<div class="row">
    <div class="col-3 docs-side-bar">

        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
        
        
        
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
          <!-- page -->
        </ul>
    </div>
    <div class="col-9">
        <!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="ndarray-vectorized-tensor-computations-on-cpus-and-gpus">NDArray: Vectorized Tensor Computations on CPUs and GPUs</h1>

<p><code class="highlighter-rouge">NDArray</code> is the basic vectorized operation unit in MXNet for matrix and tensor computations.
Users can perform usual calculations as on an R”s array, but with two additional features:</p>

<ul>
  <li>
    <p>Multiple devices: All operations can be run on various devices including
CPUs and GPUs.</p>
  </li>
  <li>
    <p>Automatic parallelization: All operations are automatically executed in
 parallel with each other.</p>
  </li>
</ul>

<h2 id="create-and-initialize">Create and Initialize</h2>

<p>Let”s create <code class="highlighter-rouge">NDArray</code> on either a GPU or a CPU:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">require</span><span class="p">(</span><span class="n">mxnet</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Loading required package: mxnet
## Loading required package: methods
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.zeros</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w"> </span><span class="c1"># create a 2-by-3 matrix on cpu</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.zeros</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="n">mx.cpu</span><span class="p">())</span><span class="w"> </span><span class="c1"># create a 2-by-3 matrix on cpu</span><span class="w">
</span><span class="c1"># c &lt;- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUDA enabled.</span><span class="w">
</span></code></pre></div></div>

<p>Typically for CUDA-enabled devices, the device id of a GPU starts from 0.
That’s why we passed in 0 to the GPU id.</p>

<p>We can initialize an <code class="highlighter-rouge">NDArray</code> object in various ways:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">))</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.rnorm</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span><span class="w">
</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>To check the numbers in an <code class="highlighter-rouge">NDArray</code>, we can simply run:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.array</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="w">
</span><span class="nf">class</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] "matrix"
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    1    1    1
</code></pre></div></div>

<h2 id="performing-basic-operations">Performing Basic Operations</h2>

<h3 id="elemental-wise-operations">Elemental-wise Operations</h3>

<p>You can perform elemental-wise operations on <code class="highlighter-rouge">NDArray</code> objects, as follows:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">8</span><span class="w">
</span><span class="n">as.array</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##      [,1] [,2] [,3] [,4]
## [1,]    2    2    2    2
## [2,]    2    2    2    2
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">as.array</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##       [,1]  [,2]  [,3]  [,4]
## [1,] 0.125 0.125 0.125 0.125
## [2,] 0.125 0.125 0.125 0.125
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="w">
</span><span class="n">as.array</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##       [,1]  [,2]  [,3]  [,4]
## [1,] 2.125 2.125 2.125 2.125
## [2,] 2.125 2.125 2.125 2.125
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="n">as.array</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##         [,1]    [,2]    [,3]    [,4]
## [1,] -3.9375 -3.9375 -3.9375 -3.9375
## [2,] -3.9375 -3.9375 -3.9375 -3.9375
</code></pre></div></div>

<p>If two <code class="highlighter-rouge">NDArray</code>s are located on different devices, we need to explicitly move them to the same one. For instance:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="n">mx.gpu</span><span class="p">())</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">8</span><span class="w">
</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.copyto</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">mx.gpu</span><span class="p">())</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w">
</span><span class="n">as.array</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="loading-and-saving">Loading and Saving</h3>

<p>You can save a list of <code class="highlighter-rouge">NDArray</code> object to your disk with <code class="highlighter-rouge">mx.nd.save</code>:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w">
</span><span class="n">mx.nd.save</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">a</span><span class="p">),</span><span class="w"> </span><span class="s2">"temp.ndarray"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>You can load it back easily:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.load</span><span class="p">(</span><span class="s2">"temp.ndarray"</span><span class="p">)</span><span class="w">
</span><span class="n">as.array</span><span class="p">(</span><span class="n">a</span><span class="p">[[</span><span class="m">1</span><span class="p">]])</span><span class="w">
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    1    1    1
</code></pre></div></div>

<p>We can directly save data to and load it from a distributed file system, such as Amazon S3 and HDFS:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mx.nd.save</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">a</span><span class="p">),</span><span class="w"> </span><span class="s2">"s3://mybucket/mydata.bin"</span><span class="p">)</span><span class="w">
</span><span class="n">mx.nd.save</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">a</span><span class="p">),</span><span class="w"> </span><span class="s2">"hdfs///users/myname/mydata.bin"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h2 id="automatic-parallelization">Automatic Parallelization</h2>

<p><code class="highlighter-rouge">NDArray</code> can automatically execute operations in parallel. Automatic parallelization is useful when
using multiple resources, such as CPU cards, GPU cards, and CPU-to-GPU memory bandwidth.</p>

<p>For example, if we write <code class="highlighter-rouge">a &lt;- a + 1</code> followed by <code class="highlighter-rouge">b &lt;- b + 1</code>, and <code class="highlighter-rouge">a</code> is on a CPU and
<code class="highlighter-rouge">b</code> is on a GPU, executing them in parallel improves
efficiency. Furthermore, because copying data between CPUs and GPUs are also expensive, running in parallel with other computations further increases efficiency.</p>

<p>It’s hard to find the code that can be executed in parallel by eye. In the
following example, <code class="highlighter-rouge">a &lt;- a + 1</code> and <code class="highlighter-rouge">c &lt;- c * 3</code> can be executed in parallel, but <code class="highlighter-rouge">a &lt;- a + 1</code> and
<code class="highlighter-rouge">b &lt;- b * 3</code> should be in sequential.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.ones</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">))</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">a</span><span class="w">
</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mx.nd.copyto</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">mx.cpu</span><span class="p">())</span><span class="w">
</span><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="n">b</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">3</span><span class="w">
</span></code></pre></div></div>

<p>Luckily, MXNet can automatically resolve the dependencies and
execute operations in parallel accurately. This allows us to write our program assuming there is only a single thread. MXNet will
automatically dispatch the program to multiple devices.</p>

<p>MXNet achieves this with lazy evaluation. Each operation is issued to an
internal engine, and then returned. For example, if we run <code class="highlighter-rouge">a &lt;- a + 1</code>, it
returns immediately after pushing the plus operator to the engine. This
asynchronous processing allows us to push more operators to the engine. It determines
the read and write dependencies and the best way to execute them in
parallel.</p>

<p>The actual computations are finished, allowing us to copy the results someplace else, such as <code class="highlighter-rouge">as.array(a)</code> or <code class="highlighter-rouge">mx.nd.save(a, "temp.dat")</code>. To write highly parallelized codes, we only need to postpone when we need
the results.</p>

<h2 id="next-steps">Next Steps</h2>
<ul>
  <li><a href="/api/r/docs/tutorials/symbol">Symbol</a></li>
  <li><a href="/api/r/docs/tutorials/classify_real_image_with_pretrained_model">Classify Real-World Images with Pre-trained Model</a></li>
  <li><a href="/api/r/docs/tutorials/char_rnn_model">Character Language Model using RNN</a></li>
</ul>
</div>
</div>
