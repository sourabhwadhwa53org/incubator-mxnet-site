{"nbformat": 4, "cells": [{"source": "# Saving and Loading Gluon Models\n\nTraining large models take a lot of time and it is a good idea to save the trained models to files to avoid training them again and again. There are a number of reasons to do this. For example, you might want to do inference on a machine that is different from the one where the model was trained. Sometimes model's performance on validation set decreases towards the end of the training because of overfitting. If you saved your model parameters after every epoch, at the end you can decide to use the model that performs best on the validation set. Another reason would be to train your model using one language (like Python that has a lot of tools for training) and run inference using a different language (like Scala probably because your application is built on Scala).\n\nIn this tutorial, we will learn ways to save and load Gluon models. There are two ways to save/load Gluon models:\n\n**1. Save/load model parameters only**\n\nParameters of any Gluon model can be saved using the `save_parameters` and `load_parameters` method. This does not save model architecture. This method is used to save parameters of dynamic (non-hybrid) models. Model architecture cannot be saved for dynamic models because model architecture changes during execution.\n\n**2. Save/load model parameters AND architecture**\n\nThe Model architecture of `Hybrid` models stays static and don't change during execution. Therefore both model parameters AND architecture can be saved and loaded using `export`, `imports` methods.\n\nLet's look at the above methods in more detail. Let's start by importing the modules we'll need.", "cell_type": "markdown", "metadata": {}}, {"source": "from __future__ import print_function\n\nimport mxnet as mx\nimport mxnet.ndarray as nd\nfrom mxnet import nd, autograd, gluon\nfrom mxnet.gluon.data.vision import transforms\n\nimport numpy as np", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Setup: build and train a simple model\n\nWe need a trained model before we can save it to a file. So let's go ahead and build a very simple convolutional network and train it on MNIST data.\n\nLet's define a helper function to build a LeNet model and another helper to train LeNet with MNIST.", "cell_type": "markdown", "metadata": {}}, {"source": "# Use GPU if one exists, else use CPU\nctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n\n# MNIST images are 28x28. Total pixels in input layer is 28x28 = 784\nnum_inputs = 784\n# Clasify the images into one of the 10 digits\nnum_outputs = 10\n# 64 images in a batch\nbatch_size = 64\n\n# Load the training data\ntrain_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True).transform_first(transforms.ToTensor()),\n                                   batch_size, shuffle=True)\n\n# Build a simple convolutional network\ndef build_lenet(net):    \n    with net.name_scope():\n        # First convolution\n        net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n        # Second convolution\n        net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n        # Flatten the output before the fully connected layers\n        net.add(gluon.nn.Flatten())\n        # First fully connected layers with 512 neurons\n        net.add(gluon.nn.Dense(512, activation=\"relu\"))\n        # Second fully connected layer with as many neurons as the number of classes\n        net.add(gluon.nn.Dense(num_outputs))\n\n        return net\n\n# Train a given model using MNIST data\ndef train_model(model):\n    # Initialize the parameters with Xavier initializer\n    model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n    # Use cross entropy loss\n    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n    # Use Adam optimizer\n    trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': .001})\n\n    # Train for one epoch\n    for epoch in range(1):\n        # Iterate through the images and labels in the training data\n        for batch_num, (data, label) in enumerate(train_data):\n            # get the images and labels\n            data = data.as_in_context(ctx)\n            label = label.as_in_context(ctx)\n            # Ask autograd to record the forward pass\n            with autograd.record():\n                # Run the forward pass\n                output = model(data)\n                # Compute the loss\n                loss = softmax_cross_entropy(output, label)\n            # Compute gradients\n            loss.backward()\n            # Update parameters\n            trainer.step(data.shape[0])\n\n            # Print loss once in a while\n            if batch_num % 50 == 0:\n                curr_loss = nd.mean(loss).asscalar()\n                print(\"Epoch: %d; Batch %d; Loss %f\" % (epoch, batch_num, curr_loss))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Let's build a model and train it. After training, we will save and restore this model from a file.", "cell_type": "markdown", "metadata": {}}, {"source": "net = build_lenet(gluon.nn.Sequential())\ntrain_model(net)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Saving model parameters to file\n\nOkay, we now have a model (`net`) that we can save to a file. Let's save the parameters of this model to a file using the `save_parameters` function.", "cell_type": "markdown", "metadata": {}}, {"source": "file_name = \"net.params\"\nnet.save_parameters(file_name)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We have successfully saved the parameters of the model into a file.\n\nNote: `Block.collect_params().save()` is not a recommended way to save parameters of a Gluon network if you plan to load the parameters back into a Gluon network using `Block.load_parameters()`.\n\n## Loading model parameters from file\n\nLet's now create a network with the parameters we saved into the file. We build the network again using the helper first and then load the weights from the file we saved using the `load_parameters` function.", "cell_type": "markdown", "metadata": {}}, {"source": "new_net = build_lenet(gluon.nn.Sequential())\nnew_net.load_parameters(file_name, ctx=ctx)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Note that to do this, we need the definition of the network as Python code. If we want to recreate this network on a different machine using the saved weights, we need the same Python code (`build_lenet`) that created the network to create the `new_net` object shown above. This means Python code needs to be copied over to any machine where we want to run this network.\n\nIf our network is [Hybrid](https://mxnet.incubator.apache.org/tutorials/gluon/hybrid.html), we can even save the network architecture into files and we won't need the network definition in a Python file to load the network. We'll see how to do it in the next section.\n\nLet's test the model we just loaded from file.", "cell_type": "markdown", "metadata": {}}, {"source": "import matplotlib.pyplot as plt\n\ndef verify_loaded_model(net):\n    \"\"\"Run inference using ten random images.\n    Print both input and output of the model\"\"\"\n\n    def transform(data, label):\n        return data.astype(np.float32)/255, label.astype(np.float32)\n\n    # Load ten random images from the test dataset\n    sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n                                  10, shuffle=True)\n\n    for data, label in sample_data:\n\n        # Display the images\n        img = nd.transpose(data, (1,0,2,3))\n        img = nd.reshape(img, (28,10*28,1))\n        imtiles = nd.tile(img, (1,1,3))\n        plt.imshow(imtiles.asnumpy())\n        plt.show()\n\n        # Display the predictions\n        data = nd.transpose(data, (0, 3, 1, 2))\n        out = net(data.as_in_context(ctx))\n        predictions = nd.argmax(out, axis=1)\n        print('Model predictions: ', predictions.asnumpy())\n\n        break\n\nverify_loaded_model(new_net)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Saving model parameters AND architecture to file\n\n[Hybrid](https://mxnet.incubator.apache.org/tutorials/gluon/hybrid.html) models can be serialized as JSON files using the `export` function. Once serialized, these models can be loaded from other language bindings like C++ or Scala for faster inference or inference in different environments.\n\nNote that the network we created above is not a Hybrid network and therefore cannot be serialized into a JSON file. So, let's create a Hybrid version of the same network and train it.", "cell_type": "markdown", "metadata": {}}, {"source": "net = build_lenet(gluon.nn.HybridSequential())\nnet.hybridize()\ntrain_model(net)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We now have a trained hybrid network. This can be exported into files using the `export` function. The `export` function will export the model architecture into a `.json` file and model parameters into a `.params` file.", "cell_type": "markdown", "metadata": {}}, {"source": "net.export(\"lenet\", epoch=1)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "`export` in this case creates `lenet-symbol.json` and `lenet-0001.params` in the current directory.\n\n## Loading model parameters AND architecture from file\n\n### From a different frontend\n\nOne of the main reasons to serialize model architecture into a JSON file is to load it from a different frontend like C, C++ or Scala. Here is a couple of examples:\n1. [Loading serialized Hybrid networks from C](https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/predict-cpp/image-classification-predict.cc)\n2. [Loading serialized Hybrid networks from Scala](https://github.com/apache/incubator-mxnet/blob/master/scala-package/infer/src/main/scala/org/apache/mxnet/infer/ImageClassifier.scala)\n\n### From Python\n\nSerialized Hybrid networks (saved as .JSON and .params file) can be loaded and used inside Python frontend using `gluon.nn.SymbolBlock`. To demonstrate that, let's load the network we serialized above.", "cell_type": "markdown", "metadata": {}}, {"source": "deserialized_net = gluon.nn.SymbolBlock.imports(\"lenet-symbol.json\", ['data'], \"lenet-0001.params\")", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "`deserialized_net` now contains the network we deserialized from files. Let's test the deserialized network to make sure it works.", "cell_type": "markdown", "metadata": {}}, {"source": "verify_loaded_model(deserialized_net)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\n\n\nThat's all! We learned how to save and load Gluon networks from files. Parameters of any Gluon network can be persisted into files. For hybrid networks, both the architecture of the network and the parameters can be saved to and loaded from files.\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n\n", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "python"}, "nbformat_minor": 2}