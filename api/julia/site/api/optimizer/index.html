
<!DOCTYPE doctype html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ie=edge" http-equiv="x-ua-compatible"/>
<meta content="Copy to clipboard" name="lang:clipboard.copy"/>
<meta content="Copied to clipboard" name="lang:clipboard.copied"/>
<meta content="en" name="lang:search.language"/>
<meta content="True" name="lang:search.pipeline.stopwords"/>
<meta content="True" name="lang:search.pipeline.trimmer"/>
<meta content="No matching documents" name="lang:search.result.none"/>
<meta content="1 matching document" name="lang:search.result.one"/>
<meta content="# matching documents" name="lang:search.result.other"/>
<meta content="[\s\-]+" name="lang:search.tokenizer"/>
<link href="../../assets/images/favicon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.0.4, mkdocs-material-4.4.2" name="generator"/>
<title>Optimizers - MXNet.jl</title>
<link href="../../assets/stylesheets/application.30686662.css" rel="stylesheet"/>
<script src="../../assets/javascripts/modernizr.74668098.js"></script>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
<link href="../../assets/fonts/material-icons.css" rel="stylesheet"/>
<link href="../../assets/Documenter.css" rel="stylesheet"/>
</head>
<body dir="ltr">
<svg class="md-svg">
<defs>
<svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg"><path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor"></path></svg>
</defs>
</svg>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<a class="md-skip" href="#optimizers" tabindex="1">
        Skip to content
      </a>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a class="md-header-nav__button md-logo" href="../.." title="MXNet.jl">
<i class="md-icon"></i>
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
              MXNet.jl
            </span>
<span class="md-header-nav__topic">
              
                Optimizers
              
            </span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="query" data-md-state="active" name="query" placeholder="Search" spellcheck="false" type="text"/>
<label class="md-icon md-search__icon" for="__search"></label>
<button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
        
      </button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
            Type to start searching
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/dmlc/MXNet.jl/" title="Go to repository">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container">
<main class="md-main" role="main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title md-nav__title--site" for="__drawer">
<a class="md-nav__button md-logo" href="../.." title="MXNet.jl">
<i class="md-icon"></i>
</a>
    MXNet.jl
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/dmlc/MXNet.jl/" title="Go to repository">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../.." title="Home">
      Home
    </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" id="nav-2" type="checkbox"/>
<label class="md-nav__link" for="nav-2">
      Tutorial
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-2">
        Tutorial
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../tutorial/mnist/" title="Digit Recognition on MNIST">
      Digit Recognition on MNIST
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../tutorial/char-lstm/" title="Generating Random Sentence with LSTM RNN">
      Generating Random Sentence with LSTM RNN
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      User Guide
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-3">
        User Guide
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../user-guide/install/" title="Installation Guide">
      Installation Guide
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../user-guide/overview/" title="Overview">
      Overview
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../user-guide/faq/" title="FAQ">
      FAQ
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-toggle md-nav__toggle" data-md-toggle="nav-4" id="nav-4" type="checkbox"/>
<label class="md-nav__link" for="nav-4">
      API Documentation
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-4">
        API Documentation
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../context/" title="Context">
      Context
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../model/" title="Models">
      Models
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../initializer/" title="Initializers">
      Initializers
    </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        Optimizers
      </label>
<a class="md-nav__link md-nav__link--active" href="./" title="Optimizers">
      Optimizers
    </a>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#built-in-optimizers">
    Built-in optimizers
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#stochastic-gradient-descent">
    Stochastic Gradient Descent
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam">
    ADAM
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
    AdaGrad
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adadelta">
    AdaDelta
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamax">
    AdaMax
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
    RMSProp
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nadam">
    Nadam
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../callback/" title="Callbacks in training">
      Callbacks in training
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../metric/" title="Evaluation Metrics">
      Evaluation Metrics
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../io/" title="Data Providers">
      Data Providers
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../ndarray/" title="NDArray API">
      NDArray API
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../symbolic-node/" title="Symbolic API">
      Symbolic API
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../nn-factory/" title="Neural Networks Factory">
      Neural Networks Factory
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../executor/" title="Executor">
      Executor
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../visualize/" title="Network Visualization">
      Network Visualization
    </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#built-in-optimizers">
    Built-in optimizers
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#stochastic-gradient-descent">
    Stochastic Gradient Descent
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adam">
    ADAM
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adagrad">
    AdaGrad
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adadelta">
    AdaDelta
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adamax">
    AdaMax
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#rmsprop">
    RMSProp
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#nadam">
    Nadam
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<a class="md-icon md-content__icon" href="https://github.com/dmlc/MXNet.jl/edit/master/docs/api/optimizer.md" title="Edit this page"></a>
<p><a id="Optimizers-1"></a></p>
<h1 id="optimizers">Optimizers</h1>
<p>Says, you have the parameter <code>W</code> inited for your model and got its gradient stored as <code>∇</code> (perhaps from AutoGrad APIs). Here is minimal snippet of getting your parameter <code>W</code> baked by <code>SGD</code>.</p>
<pre><code class="julia-repl">julia&gt; using MXNet

julia&gt; opt = SGD(η = 10)
SGD(10, 0.0, 0, 0, 0.0001, MXNet.mx.LearningRate.Fixed(10.0), MXNet.mx.Momentum.Null())

julia&gt; decend! = getupdater(opt)
(::getfield(MXNet.mx, Symbol("#updater#5792")){SGD,Dict{Int64,Any}}) (generic function with 1 method)

julia&gt; W = NDArray(Float32[1, 2, 3, 4]);

julia&gt; ∇ = NDArray(Float32[.1, .2, .3, .4]);

julia&gt; decend!(1, ∇, W)
4-element NDArray{Float32,1} @ CPU0:
 -0.0010000467f0
 -0.0020000935f0
 -0.003000021f0
 -0.004000187f0
</code></pre>
<p><a href="#MXNet.mx.AbstractOptimizer" id="MXNet.mx.AbstractOptimizer">#</a>
<strong><code>MXNet.mx.AbstractOptimizer</code></strong> — <em>Type</em>.</p>
<pre><code>AbstractOptimizer
</code></pre>
<p>Base type for all optimizers.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L22-L26" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.getupdater-Tuple{AbstractOptimizer}" id="MXNet.mx.getupdater-Tuple{AbstractOptimizer}">#</a>
<strong><code>MXNet.mx.getupdater</code></strong> — <em>Method</em>.</p>
<pre><code>getupdater(optimizer)
</code></pre>
<p>A utility function to create an updater function of <code>KVStore</code>, that uses its closure to store all the states needed for each weights.</p>
<p>Ther returned function has following signature:</p>
<pre><code class="julia">decend!(index::Int, ∇::NDArray, x::NDArray)
</code></pre>
<p>If the optimizer is stateful and need access/store states during updating, <code>index</code> will be the key to access/store states.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L252-L266" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.normgrad!-Tuple{AbstractOptimizer,NDArray,NDArray}" id="MXNet.mx.normgrad!-Tuple{AbstractOptimizer,NDArray,NDArray}">#</a>
<strong><code>MXNet.mx.normgrad!</code></strong> — <em>Method</em>.</p>
<pre><code>normgrad(optimizer, W, ∇)
</code></pre>
<p>Get the properly normalized gradient (re-scaled and clipped if necessary).</p>
<ul>
<li><code>optimizer</code>: the optimizer, should contain the field <code>scale</code>, <code>clip</code> and <code>λ</code>.</li>
<li><code>W::NDArray</code>: the trainable weights.</li>
<li><code>∇::NDArray</code>: the original gradient of the weights.</li>
</ul>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L278-L287" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.AbstractLearningRateScheduler" id="MXNet.mx.AbstractLearningRateScheduler">#</a>
<strong><code>MXNet.mx.AbstractLearningRateScheduler</code></strong> — <em>Type</em>.</p>
<pre><code>AbstractLearningRateScheduler
</code></pre>
<p>Base type for all learning rate scheduler.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L29-L33" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.AbstractMomentumScheduler" id="MXNet.mx.AbstractMomentumScheduler">#</a>
<strong><code>MXNet.mx.AbstractMomentumScheduler</code></strong> — <em>Type</em>.</p>
<pre><code>AbstractMomentumScheduler
</code></pre>
<p>Base type for all momentum scheduler.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L36-L40" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.OptimizationState" id="MXNet.mx.OptimizationState">#</a>
<strong><code>MXNet.mx.OptimizationState</code></strong> — <em>Type</em>.</p>
<pre><code>OptimizationState
</code></pre>
<p><strong>Attributes</strong></p>
<ul>
<li><code>batch_size</code>: The size of the mini-batch used in stochastic training.</li>
<li><code>curr_epoch</code>: The current epoch count. Epoch 0 means no training yet, during the first pass through the data, the epoch will be 1; during the second pass, the epoch count will be 1, and so on.</li>
<li><code>curr_batch</code>: The current mini-batch count. The batch count is reset during every epoch. The batch count 0 means the beginning of each epoch, with no mini-batch seen yet. During the first mini-batch, the mini-batch count will be 1.</li>
<li><code>curr_iter</code>: The current iteration count. One iteration corresponds to one mini-batch, but unlike the mini-batch count, the iteration count does <strong>not</strong> reset in each epoch. So it track the <em>total</em> number of mini-batches seen so far.</li>
</ul>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L43-L60" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.LearningRate.Exp" id="MXNet.mx.LearningRate.Exp">#</a>
<strong><code>MXNet.mx.LearningRate.Exp</code></strong> — <em>Type</em>.</p>
<pre><code>LearningRate.Exp(η₀; γ = 0.9)
</code></pre>
<p>
<script type="math/tex; mode=display">
\eta_t = \eta_0\gamma^t
</script>
</p>
<p>Where <code>t</code> is the epoch count, or the iteration count.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L105" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.LearningRate.Fixed" id="MXNet.mx.LearningRate.Fixed">#</a>
<strong><code>MXNet.mx.LearningRate.Fixed</code></strong> — <em>Type</em>.</p>
<pre><code>LearningRate.Fixed(η)
</code></pre>
<p>Fixed learning rate scheduler always return the same learning rate.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L94-L98" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.LearningRate.Inv" id="MXNet.mx.LearningRate.Inv">#</a>
<strong><code>MXNet.mx.LearningRate.Inv</code></strong> — <em>Type</em>.</p>
<pre><code>LearningRate.Inv(η₀; γ = 0.9, p = 0.5)
</code></pre>
<p>
<script type="math/tex; mode=display">
\eta_t = \eta_0 (1 + \gamma t)^{-p}
</script>
</p>
<p>Where <code>t</code> is the epoch count, or the iteration count.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L127" target="_blank">source</a><br/></p>
<p><a href="#Base.get-Tuple{MXNet.mx.AbstractLearningRateScheduler}" id="Base.get-Tuple{MXNet.mx.AbstractLearningRateScheduler}">#</a>
<strong><code>Base.get</code></strong> — <em>Method</em>.</p>
<pre><code>get(sched::AbstractLearningRateScheduler)
</code></pre>
<p>Returns the current learning rate.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L87-L91" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.Momentum.Fixed" id="MXNet.mx.Momentum.Fixed">#</a>
<strong><code>MXNet.mx.Momentum.Fixed</code></strong> — <em>Type</em>.</p>
<pre><code>Momentum.Fixed
</code></pre>
<p>Fixed momentum scheduler always returns the same value.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L190-L194" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.Momentum.NadamScheduler" id="MXNet.mx.Momentum.NadamScheduler">#</a>
<strong><code>MXNet.mx.Momentum.NadamScheduler</code></strong> — <em>Type</em>.</p>
<pre><code>NadamScheduler(; μ = 0.99, δ = 0.004, γ = 0.5, α = 0.96)
</code></pre>
<p>Nesterov-accelerated adaptive momentum scheduler.</p>
<p>Description in <a href="http://cs229.stanford.edu/proj2015/054_report.pdf">Incorporating Nesterov Momentum into Adam</a>.</p>
<p>
<script type="math/tex; mode=display">
\mu_t = \mu_0 * (1 - \gamma * \alpha^{t * \delta})
</script>
</p>
<p>Where</p>
<ul>
<li><code>t</code>: iteration count</li>
<li><code>μ</code>: default <code>0.99</code>, μ₀</li>
<li><code>δ</code>: default <code>0.004</code> is scheduler decay.</li>
<li><code>γ</code>: default <code>0.5</code></li>
<li><code>α</code>: default <code>0.96</code></li>
</ul>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L201" target="_blank">source</a><br/></p>
<p><a href="#MXNet.mx.Momentum.Null" id="MXNet.mx.Momentum.Null">#</a>
<strong><code>MXNet.mx.Momentum.Null</code></strong> — <em>Type</em>.</p>
<pre><code>Momentum.Null
</code></pre>
<p>The null momentum scheduler always returns 0 for momentum. It is also used to explicitly indicate momentum should not be used.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L179-L184" target="_blank">source</a><br/></p>
<p><a href="#Base.get-Tuple{MXNet.mx.Momentum.NadamScheduler,Any}" id="Base.get-Tuple{MXNet.mx.Momentum.NadamScheduler,Any}">#</a>
<strong><code>Base.get</code></strong> — <em>Method</em>.</p>
<pre><code>get(n::NadamScheduler, t)
</code></pre>
<p>Where <code>t</code> is the iteration count.</p>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizer.jl#L235-L239" target="_blank">source</a><br/></p>
<p><a id="Built-in-optimizers-1"></a></p>
<h2 id="built-in-optimizers">Built-in optimizers</h2>
<p><a id="Stochastic-Gradient-Descent-1"></a></p>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p><a href="#MXNet.mx.SGD" id="MXNet.mx.SGD">#</a>
<strong><code>MXNet.mx.SGD</code></strong> — <em>Type</em>.</p>
<pre><code>SGD(; kwargs...)
</code></pre>
<p>Stochastic gradient descent optimizer.</p>
<p>Vanilla SGD:</p>
<p>
<script type="math/tex; mode=display">
\theta \leftarrow \theta - \eta \nabla
</script>
</p>
<p>SGD with momentum::</p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
  \nu    & \leftarrow \mu \nu_{t-1} - \eta \nabla \\
  \theta & \leftarrow \theta + \nu_t
\end{align*}
</script>
</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.01</code>, learning rate.</li>
<li><code>μ</code>: default <code>0</code>, the momentum, usually set to <code>0.9</code> in this implementation.</li>
<li><code>λ</code>: default <code>0.0001</code>, weight decay is equivalent to adding a global l2 regularizer to the parameters.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the bounded range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>μ_sched::AbstractMomentumScheduler</code>: default <code>Momentum.Null()</code>, a dynamic momentum scheduler. If set, will overwrite the <code>momentum</code> parameter.</li>
<li><code>η_sched::AbstractLearningRateScheduler</code>: default <code>LearningRate.Fixed(η)</code>, a dynamic learning rate scheduler. If set, will overwrite the <code>η</code> parameter.</li>
</ul>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/sgd.jl#L18" target="_blank">source</a><br/></p>
<p><a id="ADAM-1"></a></p>
<h3 id="adam">ADAM</h3>
<p><a href="#MXNet.mx.ADAM" id="MXNet.mx.ADAM">#</a>
<strong><code>MXNet.mx.ADAM</code></strong> — <em>Type</em>.</p>
<pre><code> ADAM
</code></pre>
<p>The solver described in Diederik Kingma, Jimmy Ba: <em>Adam: A Method for Stochastic Optimization</em>. arXiv:1412.6980 [cs.LG].</p>
<pre><code>ADAM(; kwargs...)
</code></pre>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.001</code>, learning rate.</li>
<li><code>β1</code>: default <code>0.9</code>.</li>
<li><code>β2</code>: default <code>0.999</code>.</li>
<li><code>ϵ</code>: default <code>1e-8</code>.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
<li><code>η_sched::AbstractLearningRateScheduler</code>: default <code>LearningRate.Fixed(η)</code>, a dynamic learning rate scheduler. If set, will overwrite the <code>η</code> parameter.</li>
</ul>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/adam.jl#L18-L42" target="_blank">source</a><br/></p>
<p><a id="AdaGrad-1"></a></p>
<h3 id="adagrad">AdaGrad</h3>
<p><a href="#MXNet.mx.AdaGrad" id="MXNet.mx.AdaGrad">#</a>
<strong><code>MXNet.mx.AdaGrad</code></strong> — <em>Type</em>.</p>
<pre><code>AdaGrad(; kwargs...)
</code></pre>
<p>Scale learning rates by dividing with the square root of accumulated squared gradients. See [1] for further description.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.1</code>, learning rate.</li>
<li><code>ϵ</code>: default <code>1e-6</code>, small value added for numerical stability.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
</ul>
<p><strong>Notes</strong></p>
<p>Using step size <code>η</code> AdaGrad calculates the learning rate for feature <code>i</code> at time step t as:</p>
<p>
<script type="math/tex; mode=display">
η_{t,i} = \frac{lr}{\sqrt{\sum^t_{t^\prime} g^2_{t^\prime,i} + ϵ}} g_{t,i}
</script>
</p>
<p>as such the learning rate is monotonically decreasing. Epsilon is not included in the typical formula, see [2].</p>
<p><strong>References</strong></p>
<ol>
<li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011): Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121-2159.</li>
<li>Chris Dyer: Notes on AdaGrad. <a href="http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf">http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf</a></li>
</ol>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/adagrad.jl#L18" target="_blank">source</a><br/></p>
<p><a id="AdaDelta-1"></a></p>
<h3 id="adadelta">AdaDelta</h3>
<p><a href="#MXNet.mx.AdaDelta" id="MXNet.mx.AdaDelta">#</a>
<strong><code>MXNet.mx.AdaDelta</code></strong> — <em>Type</em>.</p>
<pre><code>AdaDelta(; kwargs...)
</code></pre>
<p>Scale learning rates by the ratio of accumulated gradients to accumulated updates, see [1] and notes for further description.</p>
<p><strong>Attributes</strong></p>
<ul>
<li><code>η</code>: default <code>1.0</code>, learning rate.</li>
<li><code>ρ</code>: default <code>0.95</code>, squared gradient moving average decay factor.</li>
<li><code>ϵ</code>: default <code>1e-6</code>, small value added for numerical stability.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
</ul>
<p><strong>Notes</strong></p>
<p><code>ρ</code> should be between 0 and 1. A value of <code>ρ</code> close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.</p>
<p><code>ρ = 0.95</code> and <code>ϵ = 1e-6</code> are suggested in the paper and reported to work for multiple datasets (MNIST, speech). In the paper, no learning rate is considered (so <code>η = 1.0</code>). Probably best to keep it at this value.</p>
<p><code>ϵ</code> is important for the very first update (so the numerator does not become 0).</p>
<p>Using the step size <code>η</code> and a decay factor <code>ρ</code> the learning rate is calculated as:</p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
  r_t &= ρ r_{t-1} + (1 - ρ) g^2 \\
  η_t &= η \frac{\sqrt{s_{t-1} + ϵ}} {\sqrt{r_t + ϵ}} \\
  s_t &= ρ s_{t-1} + (1 - ρ) (η_t \times g)^2
\end{align*}
</script>
</p>
<p><strong>References</strong></p>
<ol>
<li>Zeiler, M. D. (2012): ADADELTA: An Adaptive Learning Rate Method. arXiv Preprint arXiv:1212.5701.</li>
</ol>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/adadelta.jl#L18" target="_blank">source</a><br/></p>
<p><a id="AdaMax-1"></a></p>
<h3 id="adamax">AdaMax</h3>
<p><a href="#MXNet.mx.AdaMax" id="MXNet.mx.AdaMax">#</a>
<strong><code>MXNet.mx.AdaMax</code></strong> — <em>Type</em>.</p>
<pre><code>AdaMax(; kwargs...)
</code></pre>
<p>This is a variant of of the Adam algorithm based on the infinity norm. See [1] for further description.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.002</code>, learning rate.</li>
<li><code>β1</code>: default <code>0.9</code>, exponential decay rate for the first moment estimates.</li>
<li><code>β2</code>: default <code>0.999</code>, exponential decay rate for the weighted infinity norm estimates.</li>
<li><code>ϵ</code>: default <code>1e-8</code>, small value added for numerical stability.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
</ul>
<p><strong>References</strong></p>
<ol>
<li>Kingma, Diederik, and Jimmy Ba (2014): Adam: A Method for Stochastic Optimization. Section 7. <a href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>.</li>
</ol>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/adamax.jl#L18-L45" target="_blank">source</a><br/></p>
<p><a id="RMSProp-1"></a></p>
<h3 id="rmsprop">RMSProp</h3>
<p><a href="#MXNet.mx.RMSProp" id="MXNet.mx.RMSProp">#</a>
<strong><code>MXNet.mx.RMSProp</code></strong> — <em>Type</em>.</p>
<pre><code>RMSProp(; kwargs...)
</code></pre>
<p>Scale learning rates by dividing with the moving average of the root mean squared (RMS) gradients. See [1] for further description.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.1</code>, learning rate.</li>
<li><code>ρ</code>: default <code>0.9</code>, gradient moving average decay factor.</li>
<li><code>ϵ</code>: default <code>1e-8</code>, small value added for numerical stability.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
</ul>
<p><strong>Notes</strong></p>
<p><code>ρ</code> should be between 0 and 1. A value of <code>ρ</code> close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast.</p>
<p>Using the step size <code>η</code> and a decay factor <code>ρ the learning rate</code>ηₜ` is calculated as:</p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
  r_t &= ρ r_{t-1} + (1 - ρ)g^2 \\
  η_t &= \frac{η}{\sqrt{r_t + ϵ}}
\end{align*}
</script>
</p>
<p><strong>References</strong></p>
<ol>
<li>Tieleman, T. and Hinton, G. (2012): Neural Networks for Machine Learning, Lecture 6.5 - rmsprop. Coursera. <a href="http://www.youtube.com/watch?v=O3sxAc4hxZU">http://www.youtube.com/watch?v=O3sxAc4hxZU</a> (formula @5:20)</li>
</ol>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/rmsprop.jl#L18" target="_blank">source</a><br/></p>
<p><a id="Nadam-1"></a></p>
<h3 id="nadam">Nadam</h3>
<p><a href="#MXNet.mx.Nadam" id="MXNet.mx.Nadam">#</a>
<strong><code>MXNet.mx.Nadam</code></strong> — <em>Type</em>.</p>
<pre><code>Nadam(; kwargs...)
</code></pre>
<p>Nesterov Adam optimizer: Adam RMSprop with Nesterov momentum, see [1] and notes for further description.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>η</code>: default <code>0.001</code>, learning rate.</li>
<li><code>β1</code>: default <code>0.99</code>.</li>
<li><code>β2</code>: default <code>0.999</code>.</li>
<li><code>ϵ</code>: default <code>1e-8</code>, small value added for numerical stability.</li>
<li><code>clip</code>: default <code>0</code>, gradient clipping. If positive, will clip the gradient into the range <code>[-clip, clip]</code>.</li>
<li><code>scale</code>: default <code>0</code>, gradient rescaling. If != 0, multiply the gradient with <code>scale</code> before updating. Often choose to be <code>1.0 / batch_size</code>. If leave it default, high-level API like <code>fit!</code> will set it to <code>1.0 / batch_size</code>, since <code>fit!</code> knows the <code>batch_size</code>.</li>
<li><code>λ</code>: default <code>0.00001</code>, weight decay is equivalent to adding a global l2 regularizer for all the parameters.</li>
<li><code>η_sched::AbstractLearningRateScheduler</code>: default <code>nothing</code>, a dynamic learning rate scheduler. If set, will overwrite the <code>η</code> parameter.</li>
<li>
<p><code>μ_sched::NadamScheduler</code> default <code>NadamScheduler()</code> of the form.</p>
<p>
<script type="math/tex; mode=display">
\mu_t = β_1 (1 - 0.5 \times 0.96^{t \times 0.004})
</script>
</p>
</li>
</ul>
<p><strong>Notes</strong></p>
<p>Default parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.</p>
<p><strong>References</strong></p>
<ol>
<li><a href="http://cs229.stanford.edu/proj2015/054_report.pdf">Incorporating Nesterov Momentum into Adam</a>.</li>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a>.</li>
</ol>
<p><a class="documenter-source" href="https://github.com/apache/incubator-mxnet/blob/c9818480680f84daa6e281a974ab263691302ba8/julia/src/optimizers/nadam.jl#L18" target="_blank">source</a><br/></p>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="../initializer/" rel="prev" title="Initializers">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  Previous
                </span>
                Initializers
              </span>
</div>
</a>
<a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="../callback/" rel="next" title="Callbacks in training">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  Next
                </span>
                Callbacks in training
              </span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
</div>
</div>
</div>
</footer>
</div>
<script src="../../assets/javascripts/application.c648116f.js"></script>
<script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<script src="../../assets/mathjaxhelper.js"></script>
</body>
</html>