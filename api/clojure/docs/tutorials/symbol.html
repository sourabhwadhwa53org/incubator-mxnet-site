<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->
---
layout: page
---
<div class="row">
    <div class="col-3 docs-side-bar">

        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
        
        
        
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
        
           <!-- resource-p -->
          <!-- page -->
        </ul>
    </div>
    <div class="col-9">
        <!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="mxnet-clojure-symbolic-api">MXNet Clojure Symbolic API</h1>

<p>Topics:</p>

<ul>
  <li><a href="#how-to-compose-symbols">How to Compose Symbols</a></li>
  <li><a href="#more-complicated-compositions">More Complicated Compositions</a></li>
  <li><a href="#group-multiple-symbols">Group Multiple Symbols</a></li>
  <li><a href="#serialization">Serialization</a></li>
  <li><a href="#executing-symbols">Executing Symbols</a></li>
  <li><a href="/versions/master/api/clojure/docs/api/org.apache.clojure-mxnet.symbol">Symbol API Reference</a></li>
</ul>

<p>We also highly encourage you to read <a href="symbol_in_pictures">Symbolic Configuration and Execution in Pictures</a>.</p>

<p>To follow along with this documentation, you can use this namespace to with the following requirements:</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">ns</span><span class="w"> </span><span class="n">docs.symbol</span><span class="w">
  </span><span class="p">(</span><span class="no">:require</span><span class="w"> </span><span class="p">[</span><span class="n">org.apache.clojure-mxnet.executor</span><span class="w"> </span><span class="no">:as</span><span class="w"> </span><span class="n">executor</span><span class="p">]</span><span class="w">
            </span><span class="p">[</span><span class="n">org.apache.clojure-mxnet.ndarray</span><span class="w"> </span><span class="no">:as</span><span class="w"> </span><span class="n">ndarray</span><span class="p">]</span><span class="w">
            </span><span class="p">[</span><span class="n">org.apache.clojure-mxnet.symbol</span><span class="w"> </span><span class="no">:as</span><span class="w"> </span><span class="n">sym</span><span class="p">]</span><span class="w">
            </span><span class="p">[</span><span class="n">org.apache.clojure-mxnet.context</span><span class="w"> </span><span class="no">:as</span><span class="w"> </span><span class="n">context</span><span class="p">]))</span><span class="w">
</span></code></pre></div></div>

<h2 id="how-to-compose-symbols">How to Compose Symbols</h2>

<p>The Symbolic API provides a way to configure computation graphs.
You can configure the graphs either at the level of neural network layer operations or as fine-grained operations.</p>

<p>The following example configures a two-layer neural network.</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"data"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">fc1</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="s">"fc1"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">128</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">act1</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/activation</span><span class="w"> </span><span class="s">"act1"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">fc1</span><span class="w"> </span><span class="no">:act-type</span><span class="w"> </span><span class="s">"relu"</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">fc2</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="s">"fc2"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">act1</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">64</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/softmax-output</span><span class="w"> </span><span class="s">"out"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">fc2</span><span class="p">}))</span><span class="w">
</span></code></pre></div></div>

<p>This can also be combined more dynamically with the <code class="highlighter-rouge">as-&gt;</code> Clojure threading form.</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="nf">as-&gt;</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"data"</span><span class="p">)</span><span class="w"> </span><span class="n">data</span><span class="w">
  </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="s">"fc1"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">128</span><span class="p">})</span><span class="w">
  </span><span class="p">(</span><span class="nf">sym/activation</span><span class="w"> </span><span class="s">"act1"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="no">:act-type</span><span class="w"> </span><span class="s">"relu"</span><span class="p">})</span><span class="w">
  </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="s">"fc2"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">64</span><span class="p">})</span><span class="w">
  </span><span class="p">(</span><span class="nf">sym/softmax-output</span><span class="w"> </span><span class="s">"out"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">data</span><span class="p">}))</span><span class="w">

</span><span class="n">net</span><span class="w"> </span><span class="c1">;=&gt; #object[org.apache.mxnet.Symbol 0x5c78c8c2 "org.apache.mxnet.Symbol@5c78c8c2"]</span><span class="w">
</span></code></pre></div></div>

<p>The basic arithmetic operators (plus, minus, div, multiplication) work as expected.</p>

<p>The following example creates a computation graph that adds two inputs together.</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"a"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"b"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/+</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">b</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h2 id="more-complicated-compositions">More Complicated Compositions</h2>

<p>MXNet provides well-optimized symbols for layers commonly used in deep learning (see src/operator). We can also define new operators in Python. The following example first performs an element-wise add between two symbols, then feeds them to the fully connected operator:</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">lhs</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"data1"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">rhs</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"data2"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="s">"fc1"</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/+</span><span class="w"> </span><span class="n">lhs</span><span class="w"> </span><span class="n">rhs</span><span class="p">)</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">128</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="nf">sym/list-arguments</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="c1">;=&gt; ["data1" "data2" "fc1_weight" "fc1_bias"]</span><span class="w">
</span></code></pre></div></div>

<h2 id="group-multiple-symbols">Group Multiple Symbols</h2>

<p>To construct neural networks with multiple loss layers, we can use <code class="highlighter-rouge">group</code> to group multiple symbols together. The following example groups two outputs:</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"data"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">fc1</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/fully-connected</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="no">:num-hidden</span><span class="w"> </span><span class="mi">128</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">net2</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/activation</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">fc1</span><span class="w"> </span><span class="no">:act-type</span><span class="w"> </span><span class="s">"relu"</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">out1</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/softmax-output</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">net2</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">out2</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/linear-regression-output</span><span class="w"> </span><span class="p">{</span><span class="no">:data</span><span class="w"> </span><span class="n">net2</span><span class="p">}))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">group</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/group</span><span class="w"> </span><span class="p">[</span><span class="n">out1</span><span class="w"> </span><span class="n">out2</span><span class="p">]))</span><span class="w">
</span><span class="p">(</span><span class="nf">sym/list-outputs</span><span class="w"> </span><span class="n">group</span><span class="p">)</span><span class="w">
</span><span class="c1">;=&gt; ["softmaxoutput0_output" "linearregressionoutput0_output"]</span><span class="w">
</span></code></pre></div></div>

<h2 id="serialization">Serialization</h2>
<p>You can use the <a href="/versions/master/api/clojure/docs/api/org.apache.clojure-mxnet.symbol.html#var-save"><code class="highlighter-rouge">save</code></a> and <a href="/versions/master/api/clojure/docs/api/org.apache.clojure-mxnet.symbol.html#var-load"><code class="highlighter-rouge">load</code></a> functions to serialize the Symbol objects. The advantage of using save and load functions is that it is language agnostic and cloud friendly. The symbol is saved in JSON format. You can also get a JSON string directly using mxnet.Symbol.toJson. Refer to API documentation for more details.</p>

<p>The following example shows how to save a symbol to a file, load it back, and compare two symbols using a JSON string. You can also save to S3 as well</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"a"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"b"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/+</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">b</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="nf">sym/save</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="s">"symbol-c.json"</span><span class="p">)</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">c2</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/load</span><span class="w"> </span><span class="s">"symbol-c.json"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="nb">=</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/to-json</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/to-json</span><span class="w"> </span><span class="n">c2</span><span class="p">))</span><span class="w"> </span><span class="c1">;=&gt;true</span><span class="w">
</span></code></pre></div></div>

<h2 id="executing-symbols">Executing Symbols</h2>

<p>To execute symbols, first we need to define the data that they should run on. We can do it by using the bind method, which accepts device context and a dict mapping free variable names to NDArrays as arguments and returns an executor. The executor provides forward method for evaluation and an attribute outputs to get all the results.</p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"a"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/variable</span><span class="w"> </span><span class="s">"b"</span><span class="p">))</span><span class="w">
</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/+</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">b</span><span class="p">))</span><span class="w">

</span><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">ex</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/bind</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">{</span><span class="s">"a"</span><span class="w"> </span><span class="p">(</span><span class="nf">ndarray/ones</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span><span class="p">])</span><span class="w"> </span><span class="s">"b"</span><span class="w"> </span><span class="p">(</span><span class="nf">ndarray/ones</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span><span class="p">])}))</span><span class="w">
</span><span class="p">(</span><span class="nb">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="nf">executor/forward</span><span class="w"> </span><span class="n">ex</span><span class="p">)</span><span class="w">
    </span><span class="p">(</span><span class="nf">executor/outputs</span><span class="p">)</span><span class="w">
    </span><span class="p">(</span><span class="nb">first</span><span class="p">)</span><span class="w">
    </span><span class="p">(</span><span class="nf">ndarray/-&gt;vec</span><span class="p">))</span><span class="c1">;=&gt;  [2.0 2.0 2.0 2.0]</span><span class="w">
</span></code></pre></div></div>

<p>We can evaluate the same symbol on GPU with different data.
<em>To do this you must have the correct native library jar defined as a dependency</em></p>

<p><strong>Note In order to execute the following section on a cpu set gpu_device to (cpu)</strong></p>

<div class="language-clojure highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="k">def</span><span class="w"> </span><span class="n">ex</span><span class="w"> </span><span class="p">(</span><span class="nf">sym/bind</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="p">(</span><span class="nf">context/gpu</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="s">"a"</span><span class="w"> </span><span class="p">(</span><span class="nf">ndarray/ones</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span><span class="p">])</span><span class="w"> </span><span class="s">"b"</span><span class="w"> </span><span class="p">(</span><span class="nf">ndarray/ones</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span><span class="p">])}))</span><span class="w">
</span></code></pre></div></div>

<h2 id="next-steps">Next Steps</h2>
<ul>
  <li>See <a href="ndarray">NDArray API</a> for vector/matrix/tensor operations.</li>
  <li>See <a href="kvstore">KVStore API</a> for multi-GPU and multi-host distributed training.</li>
</ul>
</div>
</div>
