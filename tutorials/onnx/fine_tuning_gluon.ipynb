{"nbformat": 4, "cells": [{"source": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Fine-tuning an ONNX model with MXNet/Gluon\n\nFine-tuning is a common practice in Transfer Learning. One can take advantage of the pre-trained weights of a network, and use them as an initializer for their own task. Indeed, quite often it is difficult to gather a dataset large enough that it would allow training from scratch deep and complex networks such as ResNet152 or VGG16. For example in an image classification task, using a network trained on a large dataset like ImageNet gives a good base from which the weights can be slightly updated, or fine-tuned, to predict accurately the new classes. We will see in this tutorial that this can be achieved even with a relatively small number of new training examples.\n\n\n[Open Neural Network Exchange (ONNX)](https://github.com/onnx/onnx) provides an open source format for AI models. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types.\n\nIn this tutorial we will:\n\n- learn how to pick a specific layer from a pre-trained .onnx model file\n- learn how to load this model in Gluon and fine-tune it on a different dataset\n\n## Pre-requisite\n\nTo run the tutorial you will need to have installed the following python modules:\n- [MXNet > 1.1.0](http://mxnet.incubator.apache.org/install/index.html)\n- [onnx](https://github.com/onnx/onnx)\n- matplotlib\n\nWe recommend that you have first followed this tutorial:\n- [Inference using an ONNX model on MXNet Gluon](https://mxnet.incubator.apache.org/tutorials/onnx/inference_on_onnx_model.html)", "cell_type": "markdown", "metadata": {}}, {"source": "import json\nimport logging\nimport multiprocessing\nimport os\nimport tarfile\n\nlogging.basicConfig(level=logging.INFO)\n\nimport matplotlib.pyplot as plt\nimport mxnet as mx\nfrom mxnet import gluon, nd, autograd\nfrom mxnet.gluon.data.vision.datasets import ImageFolderDataset\nfrom mxnet.gluon.data import DataLoader\nimport mxnet.contrib.onnx as onnx_mxnet\nimport numpy as np\n\n%matplotlib inline", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Downloading supporting files\nThese are images and a vizualisation script:", "cell_type": "markdown", "metadata": {}}, {"source": "image_folder = \"images\"\nutils_file = \"utils.py\" # contain utils function to plot nice visualization\nimages = ['wrench.jpg', 'dolphin.jpg', 'lotus.jpg']\nbase_url = \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/doc/tutorials/onnx/{}?raw=true\"\n\n\nfor image in images:\n    mx.test_utils.download(base_url.format(\"{}/{}\".format(image_folder, image)), fname=image,dirname=image_folder)\nmx.test_utils.download(base_url.format(utils_file), fname=utils_file)\n\nfrom utils import *", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Downloading a model from the ONNX model zoo\n\nWe download a pre-trained model, in our case the [GoogleNet](https://arxiv.org/abs/1409.4842) model, trained on [ImageNet](http://www.image-net.org/) from the [ONNX model zoo](https://github.com/onnx/models). The model comes packaged in an archive `tar.gz` file containing an `model.onnx` model file.", "cell_type": "markdown", "metadata": {}}, {"source": "base_url = \"https://s3.amazonaws.com/download.onnx/models/opset_3/\"\ncurrent_model = \"bvlc_googlenet\"\nmodel_folder = \"model\"\narchive_file = \"{}.tar.gz\".format(current_model)\narchive_path = os.path.join(model_folder, archive_file)\nurl = \"{}{}\".format(base_url, archive_file)\nonnx_path = os.path.join(model_folder, current_model, 'model.onnx')\n\n# Download the zipped model\nmx.test_utils.download(url, dirname = model_folder)\n\n# Extract the model\nif not os.path.isdir(os.path.join(model_folder, current_model)):\n    print('Extracting {} in {}...'.format(archive_path, model_folder))\n    tar = tarfile.open(archive_path, \"r:gz\")\n    tar.extractall(model_folder)\n    tar.close()\n    print('Model extracted.')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Downloading the Caltech101 dataset\n\nThe [Caltech101 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) is made of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images.\n\n*L. Fei-Fei, R. Fergus and P. Perona. Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. IEEE. CVPR 2004, Workshop on Generative-Model\nBased Vision. 2004*", "cell_type": "markdown", "metadata": {}}, {"source": "data_folder = \"data\"\ndataset_name = \"101_ObjectCategories\"\narchive_file = \"{}.tar.gz\".format(dataset_name)\narchive_path = os.path.join(data_folder, archive_file)\ndata_url = \"https://s3.us-east-2.amazonaws.com/mxnet-public/\"\n\nif not os.path.isfile(archive_path):\n    mx.test_utils.download(\"{}{}\".format(data_url, archive_file), dirname = data_folder)\n    print('Extracting {} in {}...'.format(archive_file, data_folder))\n    tar = tarfile.open(archive_path, \"r:gz\")\n    tar.extractall(data_folder)\n    tar.close()\n    print('Data extracted.')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "training_path = os.path.join(data_folder, dataset_name)\ntesting_path = os.path.join(data_folder, \"{}_test\".format(dataset_name))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Load the data using an ImageFolderDataset and a DataLoader\n\nWe need to transform the images to a format accepted by the network", "cell_type": "markdown", "metadata": {}}, {"source": "EDGE = 224\nSIZE = (EDGE, EDGE)\nBATCH_SIZE = 32\nNUM_WORKERS = 6", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We transform the dataset images using the following operations:\n- resize the shorter edge to 224, the longer edge will be greater or equal to 224\n- center and crop an area of size (224,224)\n- transpose the channels to be (3,224,224)", "cell_type": "markdown", "metadata": {}}, {"source": "def transform(image, label):\n    resized = mx.image.resize_short(image, EDGE)\n    cropped, crop_info = mx.image.center_crop(resized, SIZE)\n    transposed = nd.transpose(cropped, (2,0,1))\n    return transposed, label", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "The train and test dataset are created automatically by passing the root of each folder. The labels are built using the sub-folders names as label.\n```\ntrain_root\n__label1\n____image1\n____image2\n__label2\n____image3\n____image4\n```", "cell_type": "markdown", "metadata": {}}, {"source": "dataset_train = ImageFolderDataset(root=training_path)\ndataset_test = ImageFolderDataset(root=testing_path)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We use several worker processes, which means the dataloading and pre-processing is going to be distributed across multiple processes. This will help preventing our GPU from starving and waiting for the data to be copied across", "cell_type": "markdown", "metadata": {}}, {"source": "dataloader_train = DataLoader(dataset_train.transform(transform, lazy=False), batch_size=BATCH_SIZE, last_batch='rollover',\n                              shuffle=True, num_workers=NUM_WORKERS)\ndataloader_test = DataLoader(dataset_test.transform(transform, lazy=False), batch_size=BATCH_SIZE, last_batch='rollover',\n                             shuffle=False, num_workers=NUM_WORKERS)\nprint(\"Train dataset: {} images, Test dataset: {} images\".format(len(dataset_train), len(dataset_test)))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "categories = dataset_train.synsets\nNUM_CLASSES = len(categories)\nBATCH_SIZE = 32", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Let's plot the 1000th image to test the dataset", "cell_type": "markdown", "metadata": {}}, {"source": "N = 1000\nplt.imshow((transform(dataset_train[N][0], 0)[0].asnumpy().transpose((1,2,0))))\nplt.axis('off')\nprint(categories[dataset_train[N][1]])", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Fine-Tuning the ONNX model\n\n### Getting the last layer\n\nLoad the ONNX model", "cell_type": "markdown", "metadata": {}}, {"source": "sym, arg_params, aux_params = onnx_mxnet.import_model(onnx_path)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "This function get the output of a given layer", "cell_type": "markdown", "metadata": {}}, {"source": "def get_layer_output(symbol, arg_params, aux_params, layer_name):\n    all_layers = symbol.get_internals()\n    net = all_layers[layer_name+'_output']\n    net = mx.symbol.Flatten(data=net)\n    new_args = dict({k:arg_params[k] for k in arg_params if k in net.list_arguments()})\n    new_aux = dict({k:aux_params[k] for k in aux_params if k in net.list_arguments()})\n    return (net, new_args, new_aux)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Here we print the different layers of the network to make it easier to pick the right one", "cell_type": "markdown", "metadata": {}}, {"source": "sym.get_internals()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We get the network until the output of the `flatten0` layer", "cell_type": "markdown", "metadata": {}}, {"source": "new_sym, new_arg_params, new_aux_params = get_layer_output(sym, arg_params, aux_params, 'flatten0')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Fine-tuning in gluon\n\n\nWe can now take advantage of the features and pattern detection knowledge that our network learnt training on ImageNet, and apply that to the new Caltech101 dataset.\n\n\nWe pick a context, fine-tuning on CPU will be **WAY** slower.", "cell_type": "markdown", "metadata": {}}, {"source": "ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We create a symbol block that is going to hold all our pre-trained layers, and assign the weights of the different pre-trained layers to the newly created SymbolBlock", "cell_type": "markdown", "metadata": {}}, {"source": "pre_trained = gluon.nn.SymbolBlock(outputs=new_sym, inputs=mx.sym.var('data_0'))\nnet_params = pre_trained.collect_params()\nfor param in new_arg_params:\n    if param in net_params:\n        net_params[param]._load_init(new_arg_params[param], ctx=ctx)\nfor param in new_aux_params:\n    if param in net_params:\n        net_params[param]._load_init(new_aux_params[param], ctx=ctx)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We create the new dense layer with the right new number of classes (101) and initialize the weights", "cell_type": "markdown", "metadata": {}}, {"source": "dense_layer = gluon.nn.Dense(NUM_CLASSES)\ndense_layer.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "We add the SymbolBlock and the new dense layer to a HybridSequential network", "cell_type": "markdown", "metadata": {}}, {"source": "net = gluon.nn.HybridSequential()\nwith net.name_scope():\n    net.add(pre_trained)\n    net.add(dense_layer)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Loss\nSoftmax cross entropy for multi-class classification", "cell_type": "markdown", "metadata": {}}, {"source": "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Trainer\nInitialize trainer with common training parameters", "cell_type": "markdown", "metadata": {}}, {"source": "LEARNING_RATE = 0.0005\nWDECAY = 0.00001\nMOMENTUM = 0.9", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "The trainer will retrain and fine-tune the entire network. If we use `dense_layer` instead of `net` in the cell below, the gradient updates would only be applied to the new last dense layer. Essentially we would be using the pre-trained network as a featurizer.", "cell_type": "markdown", "metadata": {}}, {"source": "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n                        {'learning_rate': LEARNING_RATE,\n                         'wd':WDECAY,\n                         'momentum':MOMENTUM})", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Evaluation loop\n\nWe measure the accuracy in a non-blocking way, using `nd.array` to take care of the parallelisation that MXNet and Gluon offers.", "cell_type": "markdown", "metadata": {}}, {"source": " def evaluate_accuracy_gluon(data_iterator, net):\n    num_instance = 0\n    sum_metric = nd.zeros(1,ctx=ctx, dtype=np.int32)\n    for i, (data, label) in enumerate(data_iterator):\n        data = data.astype(np.float32).as_in_context(ctx)\n        label = label.astype(np.int32).as_in_context(ctx)\n        output = net(data)\n        prediction = nd.argmax(output, axis=1).astype(np.int32)\n        num_instance += len(prediction)\n        sum_metric += (prediction==label).sum()\n    accuracy = (sum_metric.astype(np.float32)/num_instance)\n    return accuracy.asscalar()", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "%%time\nprint(\"Untrained network Test Accuracy: {0:.4f}\".format(evaluate_accuracy_gluon(dataloader_test, net)))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "### Training loop", "cell_type": "markdown", "metadata": {}}, {"source": "val_accuracy = 0\nfor epoch in range(5):\n    for i, (data, label) in enumerate(dataloader_train):\n        data = data.astype(np.float32).as_in_context(ctx)\n        label = label.as_in_context(ctx)\n\n        if i%20==0 and i >0:\n            print('Batch [{0}] loss: {1:.4f}'.format(i, loss.mean().asscalar()))\n\n        with autograd.record():\n            output = net(data)\n            loss = softmax_cross_entropy(output, label)\n        loss.backward()\n        trainer.step(data.shape[0])\n\n    nd.waitall() # wait at the end of the epoch    \n    new_val_accuracy = evaluate_accuracy_gluon(dataloader_test, net)    \n    print(\"Epoch [{0}] Test Accuracy {1:.4f} \".format(epoch, new_val_accuracy))\n\n    # We perform early-stopping regularization, to prevent the model from overfitting\n    if val_accuracy > new_val_accuracy:\n        print('Validation accuracy is decreasing, stopping training')\n        break\n    val_accuracy = new_val_accuracy              ", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Testing\nIn the previous tutorial, we saw that the network trained on ImageNet couldn't classify correctly `wrench`, `dolphin`, `lotus` because these are not categories of the ImageNet dataset.\n\nLet's see if our network fine-tuned on Caltech101 is up for the task:", "cell_type": "markdown", "metadata": {}}, {"source": "# Number of predictions to show\nTOP_P = 3", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "# Convert img to format expected by the network\ndef transform(img):\n    return nd.array(np.expand_dims(np.transpose(img, (2,0,1)),axis=0).astype(np.float32), ctx=ctx)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "# Load and transform the test images\ncaltech101_images_test = [plt.imread(os.path.join(image_folder, \"{}\".format(img))) for img in images]\ncaltech101_images_transformed = [transform(img) for img in caltech101_images_test]", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Helper function to run batches of data", "cell_type": "markdown", "metadata": {}}, {"source": "def run_batch(net, data):\n    results = []\n    for batch in data:\n        outputs = net(batch)\n        results.extend([o for o in outputs.asnumpy()])\n    return np.array(results)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "result = run_batch(net, caltech101_images_transformed)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "plot_predictions(caltech101_images_test, result, categories, TOP_P)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\n\n\n\n**Great!** The network classified these images correctly after being fine-tuned on a dataset that contains images of `wrench`, `dolphin` and `lotus`\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "python"}, "nbformat_minor": 2}